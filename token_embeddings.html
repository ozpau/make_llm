<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="In this blog post I’ll explore the structure of token embeddings in large language models.">

<title>LLM token embeddings – make_llm</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-b53751a350365c71b6c909e95f209ed1.css" rel="stylesheet" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-6496619755334375754ec9f180086959.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark-c7dbc3b26946799cb48197aed96bf0eb.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="LLM token embeddings – make_llm">
<meta property="og:description" content="In this blog post I’ll explore the structure of token embeddings in large language models.">
<meta property="og:image" content="https://ozpau.github.io/make_llm/02_token_embeddings_files/figure-html/cell-11-output-1.png">
<meta property="og:site_name" content="make_llm">
<meta property="og:image:height" content="432">
<meta property="og:image:width" content="548">
<meta name="twitter:title" content="LLM token embeddings – make_llm">
<meta name="twitter:description" content="In this blog post I’ll explore the structure of token embeddings in large language models.">
<meta name="twitter:image" content="https://ozpau.github.io/make_llm/02_token_embeddings_files/figure-html/cell-11-output-1.png">
<meta name="twitter:image-height" content="432">
<meta name="twitter:image-width" content="548">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">make_llm</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./token_embeddings.html">LLM token embeddings</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">make_llm</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./core.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">core</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./token_embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">LLM token embeddings</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">discussions</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discussions/attention.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Attention</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">transformers</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./transformers/transformers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transformers</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#generate-text" id="toc-generate-text" class="nav-link active" data-scroll-target="#generate-text">Generate text</a></li>
  <li><a href="#llm-parameters" id="toc-llm-parameters" class="nav-link" data-scroll-target="#llm-parameters">LLM Parameters</a></li>
  <li><a href="#pca" id="toc-pca" class="nav-link" data-scroll-target="#pca">PCA</a></li>
  <li><a href="#understanding-token-embeddings" id="toc-understanding-token-embeddings" class="nav-link" data-scroll-target="#understanding-token-embeddings">Understanding token embeddings</a></li>
  <li><a href="#kdtree" id="toc-kdtree" class="nav-link" data-scroll-target="#kdtree">KDTree</a></li>
  <li><a href="#singular-values" id="toc-singular-values" class="nav-link" data-scroll-target="#singular-values">Singular values</a></li>
  <li><a href="#adding-noise" id="toc-adding-noise" class="nav-link" data-scroll-target="#adding-noise">Adding noise</a></li>
  <li><a href="#why-does-random-projection-work" id="toc-why-does-random-projection-work" class="nav-link" data-scroll-target="#why-does-random-projection-work">Why does random projection work?</a></li>
  <li><a href="#fixing-projection-scale" id="toc-fixing-projection-scale" class="nav-link" data-scroll-target="#fixing-projection-scale">Fixing projection scale</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/ozpau/make_llm/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="token_embeddings.html.md"><i class="bi bi-file-code"></i>CommonMark</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">LLM token embeddings</h1>
</div>

<div>
  <div class="description">
    In this blog post I’ll explore the structure of token embeddings in large language models.
  </div>
</div>


<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<div id="e1a35cf4-e714-4d9e-a13d-ed34d9c1eee0" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#from transformers import pipeline</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c5a4686c-6205-4a9f-b0dd-0a12e3dba2f4" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Doesn't produce any content (probably using incorrectly)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co">#pipe = pipeline("text-generation", model="nroggendorff/smallama-it")</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#pipe = pipeline("text-generation", model="KingNish/Qwen2.5-0.5b-Test-ft")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9bb4b8f8-a5da-40cf-acca-e175122c7c92" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_memory():</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        torch.cuda.empty_cache()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    gc.collect()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>For our experiments let’s grab a small language model. In this case, I will use Qwen2.5-0.5B. It fits in just about 1GB of VRAM, so it’s great for experimentation!</p>
<div id="1165c67b-4074-4b5d-b91a-6f05b3ca7f64" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>model_name <span class="op">=</span> <span class="st">"Qwen/Qwen2.5-0.5B-Instruct"</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    model_name,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    torch_dtype<span class="op">=</span><span class="st">"auto"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    device_map<span class="op">=</span><span class="st">"auto"</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(model_name)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>2025-02-25 01:22:48.553363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1740475368.575449   32363 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1740475368.582244   32363 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered</code></pre>
</div>
</div>
<section id="generate-text" class="level2">
<h2 class="anchored" data-anchor-id="generate-text">Generate text</h2>
<p>Even though this model is small, it can still generate coherent text:</p>
<div id="1886a78c-acbf-450b-9045-58ec4e3597fb" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> demo(max_new_tokens<span class="op">=</span><span class="dv">2048</span>,prompt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> prompt:</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> <span class="st">"Write an introduction for a blog post about large language model token embeddings. Tags: PCA, layer sizes, nearest neighbours, KDTree, singular values."</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    messages <span class="op">=</span> [</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: <span class="st">"You are an autocorrelating model. You use previous outputs for reasoning. Pay attention to tags."</span>},</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        {<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: prompt}</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> tokenizer.apply_chat_template(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        messages,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        tokenize<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        add_generation_prompt<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    model_inputs <span class="op">=</span> tokenizer([text], return_tensors<span class="op">=</span><span class="st">"pt"</span>).to(model.device)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    generated_ids <span class="op">=</span> model.generate(</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="op">**</span>model_inputs,</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        max_new_tokens<span class="op">=</span>max_new_tokens</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    generated_ids <span class="op">=</span> [</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        output_ids[<span class="bu">len</span>(input_ids):] <span class="cf">for</span> input_ids, output_ids <span class="kw">in</span> <span class="bu">zip</span>(model_inputs.input_ids, generated_ids)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> tokenizer.batch_decode(generated_ids, skip_special_tokens<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    display_markdown(response, raw<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f45388dc-7127-40df-9f51-11940f1167f4" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>demo()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our blog where we explore the fascinating world of large language models and their intricate relationship with neural networks, specifically focusing on token embeddings. As you delve deeper into this topic, you’ll uncover how these mathematical constructs play a crucial role in understanding complex natural language processing tasks.</p>
<p>In today’s era of artificial intelligence, large language models have emerged as indispensable tools capable of generating human-like text. However, they often struggle with handling high-dimensional data due to their computational demands. One solution that has proven effective is token embedding, which converts words into numerical representations using vectors. This technique significantly enhances the performance of language models by reducing the dimensionality of input data.</p>
<p>One method used to achieve this is through the application of Principal Component Analysis (PCA) to reduce the complexity of the data while preserving most of its variance. By analyzing the first few principal components, we can effectively represent the underlying structure of the data. This approach is particularly useful when dealing with large datasets or when the number of features exceeds the capacity of traditional methods like k-nearest neighbors or KD-trees.</p>
<p>Another powerful technique is utilizing the concept of layer sizes in deep learning. Understanding the architecture of a network helps us comprehend how information flows between layers and what kind of patterns emerge at each stage. By examining the learned weights and activations from different layers, we gain insights into the function of each component in the neural network.</p>
<p>Lastly, we delve into the realm of Singular Value Decomposition (SVD), which is employed to extract the essential features of the dataset. SVD decomposes the input matrix into three parts: the left singular vectors, the right singular vectors, and the diagonal matrix containing the singular values. These components allow us to visualize the importance of various features in the context of the entire dataset.</p>
<p>Understanding these techniques and their interplay within the framework of large language models will provide you with valuable insights into the complexities involved in training and deploying such models. Whether you’re a seasoned AI researcher or just starting out, exploring these concepts can lead to significant advancements in your work. Stay tuned for more updates and stay curious!</p>
</div>
</div>
<div id="f2c7b262-0d6c-463f-92d9-66a1ec9e89f6" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>demo()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our blog post exploring the fascinating world of large language models and their groundbreaking advancements in text generation! In this article, we delve into the complex world of token embeddings, which play a crucial role in understanding how these models process and generate human-like language. We’ll dive deep into the intricacies of PCA (Principal Component Analysis), layer sizes, nearest neighbors, KDTree (K-D Tree) implementation, and singular values, providing a comprehensive overview of the topic.</p>
<p>From understanding the importance of token embeddings in natural language processing to mastering advanced techniques like nearest neighbors and KDTree implementations, you’ll discover not only the technical aspects but also the practical applications that these methods have revolutionized. Whether you’re a seasoned data scientist or just starting out in the field, this blog post promises to be an enlightening journey through the depths of machine learning and natural language processing. So grab your notebook and get ready to explore the next frontier in AI technology!</p>
</div>
</div>
<div id="38eb034b-e4ce-4c4d-9d4e-c632831197aa" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>demo()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>In today’s digital age, the role of large language models (LLMs) in various fields has become increasingly prominent. These models, particularly those trained on large datasets, have generated significant advancements across several domains such as natural language processing (NLP), speech recognition, and more recently, text generation. However, one aspect that has not been fully explored is the representation of input data within these LLMs—a critical yet often overlooked step in their training process.</p>
<p>Large language models rely heavily on understanding the context of inputs they encounter during their training phase. To achieve this, they employ embedding layers to convert textual data into numerical vectors. This conversion involves capturing both the meaning of the words themselves as well as contextual information that might be present. The goal is to map the raw text data into a higher-dimensional space where it can be effectively analyzed by the model. This process is known as tokenization or vectorization.</p>
<p>PCA (Principal Component Analysis) plays a pivotal role in this process because it helps identify the most important features in the dataset. By analyzing the variance captured by each principal component, we can understand which aspects of the input data are most relevant for the model’s performance. Layer sizes, specifically the number of hidden units in the neural networks, significantly influence the complexity and dimensionality of the embedding space. The choice between dense and sparse layers depends on the nature of the task at hand; dense layers are used when there is ample context available, whereas sparse layers are preferred when the context is sparse.</p>
<p>Nearest Neighbors (NN) techniques are essential for learning embeddings based on proximity to other tokens. KDTree (K-d Tree) algorithms efficiently store and query point sets, making them ideal for spatially distributed data like text documents. The selection of optimal parameters for these algorithms, including tree depth and leaf size, is crucial for achieving effective token embeddings.</p>
<p>Singular Value Decomposition (SVD) is another powerful tool for understanding the structure of the input data. It decomposes the matrix formed by the embedding vectors into three matrices—U, Σ, and V^T—and provides insights into how much of the total variance is explained by each feature. This decomposition allows us to understand the relative importance of different dimensions in the embedding space.</p>
<p>Understanding these components—PCA, layer sizes, nearest neighbors, KDTree, and SVD—can greatly enhance our comprehension of how LLMs handle and represent input data. This knowledge will be invaluable in optimizing the training process, improving model performance, and leveraging the full potential of these powerful tools.</p>
</div>
</div>
</section>
<section id="llm-parameters" class="level2">
<h2 class="anchored" data-anchor-id="llm-parameters">LLM Parameters</h2>
<p>An LLM is made from an embedding layer, followed by transformer layers. Let’s look at their sizes:</p>
<div id="599ada1c-9543-4af3-8ef6-fbb100fea498" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="kw">lambda</span> x: (np.prod(x.shape), x.shape), model.model.parameters()))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sizes[:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[(136134656, torch.Size([151936, 896])),
 (802816, torch.Size([896, 896])),
 (896, torch.Size([896])),
 (114688, torch.Size([128, 896])),
 (128, torch.Size([128])),
 (114688, torch.Size([128, 896])),
 (128, torch.Size([128])),
 (802816, torch.Size([896, 896])),
 (4358144, torch.Size([4864, 896])),
 (4358144, torch.Size([4864, 896]))]</code></pre>
</div>
</div>
<p>Size of the embedding layer seems to dominate model size for this particular model:</p>
<div id="5f59f3aa-b3c1-4931-9475-507090446904" class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(sizes, columns<span class="op">=</span>[<span class="st">'size'</span>, <span class="st">'shape'</span>])</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>df.iloc[:<span class="dv">40</span>].plot(y <span class="op">=</span> <span class="st">'size'</span>, kind<span class="op">=</span><span class="st">'bar'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In fact, let’s compare size of embeddings with the rest of the model parameters:</p>
<div id="f22b0c18-1f43-459c-a5c4-e8d376dcae6f" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df.loc[<span class="dv">0</span>,<span class="st">'size'</span>]<span class="op">/</span>df.loc[:, <span class="st">'size'</span>].<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.275557948415276</code></pre>
</div>
</div>
<p>So, 28% of the model is in the embeddings.</p>
</section>
<section id="pca" class="level2">
<h2 class="anchored" data-anchor-id="pca">PCA</h2>
<p>Now let’s look at embeddings in more detail</p>
<div id="11c7a2bb-1dbb-4a52-881b-d4176f29df7c" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> model.get_input_embeddings()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>emb_w <span class="op">=</span> emb.weight</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>emb_w.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 896])</code></pre>
</div>
</div>
<p>We have about 150k tokens, and we use 896 dimensions to encode them. Let’s see if there is any visible structure among them.</p>
<div id="dc6ed05f-c5e0-4a5e-a317-147fc3b4777d" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> emb_w.cpu().detach()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> w.<span class="bu">type</span>(torch.Tensor)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Numbers, characters, symbols seem to be nicely grouped together:</p>
<div id="29f06806-d25b-4bb3-a874-b173217d0642" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>w_pca <span class="op">=</span> w.pca(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="f176897c-96fc-4074-8a05-c1bb4896460a" class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>w_pca.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 3])</code></pre>
</div>
</div>
<div id="97f6bd41-64f2-4638-9ebd-470a5ad46633" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>fac0,fac1,fac2 <span class="op">=</span> w_pca.t()[:<span class="dv">3</span>]</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(p,p <span class="op">+</span> <span class="dv">100</span>))</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> fac0[idxs]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> fac2[idxs]</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">12</span>))</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, Y)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>token_labels <span class="op">=</span> [tokenizer.decode([i]) <span class="cf">for</span> i <span class="kw">in</span> idxs]</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, x, y <span class="kw">in</span> <span class="bu">zip</span>(token_labels, X, Y):</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    plt.text(x,y,i, color<span class="op">=</span>np.random.rand(<span class="dv">3</span>)<span class="op">*</span><span class="fl">0.7</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.show()</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">#plt.savefig('token_encodings.png')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="understanding-token-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="understanding-token-embeddings">Understanding token embeddings</h2>
<p>Now let’s check if there are any tokens that have very similar encodings.</p>
<div id="4d8069f2-7901-45d8-9ee6-0dd09c73b091" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>token_labels <span class="op">=</span> L([tokenizer.decode([i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(w.shape[<span class="dv">0</span>])])</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>token_labels</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#151936) ['!','"','#','$','%','&amp;',"'",'(',')','*','+',',','-','.','/','0','1','2','3','4'...]</code></pre>
</div>
</div>
<div id="374346b5-8476-4c8f-88af-79441dafad50" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>w.T.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([896, 151936])</code></pre>
</div>
</div>
<div id="af379b23-d6f4-459f-a508-060a54ef66d7" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(w.T, columns<span class="op">=</span>token_labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here is a correlation matrix of first 500 tokens. Notice that there is a suspicious cluster of tokens with large correlation.</p>
<div id="f1b86ac0-20e6-4ffa-8d6c-e1ca444cabcc" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>plt.matshow(df.iloc[:,:<span class="dv">500</span>].corr())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-21-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="5e5350fa-1af8-4a22-8519-b9fbbcb46759" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>df.iloc[:,<span class="dv">124</span>:<span class="dv">130</span>].corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.036561</td>
<td>0.949049</td>
<td>0.999370</td>
<td>0.613330</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.036553</td>
<td>0.949040</td>
<td>0.999370</td>
<td>0.613357</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>0.036561</td>
<td>0.036553</td>
<td>1.000000</td>
<td>0.039240</td>
<td>0.036404</td>
<td>0.066037</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>0.949049</td>
<td>0.949040</td>
<td>0.039240</td>
<td>1.000000</td>
<td>0.948284</td>
<td>0.574783</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>0.999370</td>
<td>0.999370</td>
<td>0.036404</td>
<td>0.948284</td>
<td>1.000000</td>
<td>0.615100</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>0.613330</td>
<td>0.613357</td>
<td>0.066037</td>
<td>0.574783</td>
<td>0.615100</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div id="59a2a844-8bf0-45b3-a80e-22ad2d92b474" class="cell">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>plt.matshow(df.iloc[:,<span class="dv">124</span>:<span class="dv">225</span>].corr())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="8ac80ec0-e248-4d6d-80c3-280bcb2e461f" class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>plt.matshow(df.iloc[:,<span class="dv">177</span>:<span class="dv">190</span>].corr())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-24-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f7e42bfd-970c-49bf-a595-be3ca840ac4b" class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>df.iloc[:,<span class="dv">177</span>:<span class="dv">190</span>].corr()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th">�</th>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286441</td>
<td>0.324009</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.999999</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286423</td>
<td>0.324003</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>0.999999</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286415</td>
<td>0.323999</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286428</td>
<td>0.324013</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286434</td>
<td>0.324009</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286445</td>
<td>0.324002</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286447</td>
<td>0.324016</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286421</td>
<td>0.323967</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286428</td>
<td>0.323993</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286425</td>
<td>0.324003</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">�</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>1.000000</td>
<td>0.286441</td>
<td>0.324025</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th"></td>
<td>0.286441</td>
<td>0.286423</td>
<td>0.286415</td>
<td>0.286428</td>
<td>0.286434</td>
<td>0.286445</td>
<td>0.286447</td>
<td>0.286421</td>
<td>0.286428</td>
<td>0.286425</td>
<td>0.286441</td>
<td>1.000000</td>
<td>0.196765</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th"></td>
<td>0.324009</td>
<td>0.324003</td>
<td>0.323999</td>
<td>0.324013</td>
<td>0.324009</td>
<td>0.324002</td>
<td>0.324016</td>
<td>0.323967</td>
<td>0.323993</td>
<td>0.324003</td>
<td>0.324025</td>
<td>0.196765</td>
<td>1.000000</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>These have almost perfect correlation. But these seem to correspond to some special byte sequences. Perhaps this has something to do with unicode?</p>
<p>We can also put token embeddings into a dataframe, and join it together with token labels sequence for convenience:</p>
<div id="a6934ead-4672-4ae9-a5c7-0cf3cfef1e6c" class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(w)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'token'</span>] <span class="op">=</span> token_labels</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df.reindex(columns<span class="op">=</span>pd.Index([df.columns[<span class="op">-</span><span class="dv">1</span>]] <span class="op">+</span> <span class="bu">list</span>(df.columns[:<span class="op">-</span><span class="dv">1</span>])))</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">token</th>
<th data-quarto-table-cell-role="th">0</th>
<th data-quarto-table-cell-role="th">1</th>
<th data-quarto-table-cell-role="th">2</th>
<th data-quarto-table-cell-role="th">3</th>
<th data-quarto-table-cell-role="th">4</th>
<th data-quarto-table-cell-role="th">5</th>
<th data-quarto-table-cell-role="th">6</th>
<th data-quarto-table-cell-role="th">7</th>
<th data-quarto-table-cell-role="th">8</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">886</th>
<th data-quarto-table-cell-role="th">887</th>
<th data-quarto-table-cell-role="th">888</th>
<th data-quarto-table-cell-role="th">889</th>
<th data-quarto-table-cell-role="th">890</th>
<th data-quarto-table-cell-role="th">891</th>
<th data-quarto-table-cell-role="th">892</th>
<th data-quarto-table-cell-role="th">893</th>
<th data-quarto-table-cell-role="th">894</th>
<th data-quarto-table-cell-role="th">895</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>!</td>
<td>-0.010376</td>
<td>0.040771</td>
<td>0.009705</td>
<td>0.000070</td>
<td>-0.027100</td>
<td>-0.002975</td>
<td>-0.001160</td>
<td>-0.019531</td>
<td>0.028442</td>
<td>...</td>
<td>-0.008179</td>
<td>0.016724</td>
<td>0.022339</td>
<td>-0.027222</td>
<td>-0.029541</td>
<td>-0.015381</td>
<td>-0.007477</td>
<td>0.009827</td>
<td>0.013611</td>
<td>-0.006683</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>"</td>
<td>-0.014587</td>
<td>-0.001366</td>
<td>-0.017700</td>
<td>-0.002670</td>
<td>0.003708</td>
<td>-0.001495</td>
<td>0.005402</td>
<td>-0.010620</td>
<td>0.017700</td>
<td>...</td>
<td>-0.013672</td>
<td>-0.038330</td>
<td>0.003433</td>
<td>-0.007355</td>
<td>-0.007172</td>
<td>0.001152</td>
<td>-0.005798</td>
<td>-0.002441</td>
<td>0.002441</td>
<td>-0.008118</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>#</td>
<td>-0.036621</td>
<td>-0.010193</td>
<td>0.007812</td>
<td>-0.010925</td>
<td>0.008057</td>
<td>0.016724</td>
<td>0.023438</td>
<td>-0.030273</td>
<td>-0.009399</td>
<td>...</td>
<td>-0.024048</td>
<td>-0.002716</td>
<td>0.006134</td>
<td>-0.007538</td>
<td>-0.008545</td>
<td>0.008789</td>
<td>0.011292</td>
<td>-0.007355</td>
<td>-0.017700</td>
<td>-0.000675</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>$</td>
<td>-0.009338</td>
<td>-0.012085</td>
<td>-0.015381</td>
<td>0.010864</td>
<td>0.003967</td>
<td>-0.005981</td>
<td>0.005707</td>
<td>-0.006866</td>
<td>0.014343</td>
<td>...</td>
<td>-0.047119</td>
<td>0.007324</td>
<td>0.007935</td>
<td>-0.006531</td>
<td>-0.019897</td>
<td>-0.007812</td>
<td>0.020874</td>
<td>-0.002655</td>
<td>0.015137</td>
<td>-0.017822</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>%</td>
<td>-0.009521</td>
<td>0.004211</td>
<td>0.006012</td>
<td>-0.018433</td>
<td>0.006409</td>
<td>0.013428</td>
<td>0.016846</td>
<td>0.003372</td>
<td>-0.000965</td>
<td>...</td>
<td>0.005798</td>
<td>-0.010742</td>
<td>-0.000957</td>
<td>-0.024170</td>
<td>-0.000463</td>
<td>0.010620</td>
<td>-0.003967</td>
<td>0.003937</td>
<td>-0.006195</td>
<td>-0.007874</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">151931</td>
<td></td>
<td>0.005951</td>
<td>-0.005310</td>
<td>0.003326</td>
<td>0.002472</td>
<td>-0.003067</td>
<td>0.002563</td>
<td>-0.009583</td>
<td>0.015747</td>
<td>-0.000580</td>
<td>...</td>
<td>0.003845</td>
<td>0.016113</td>
<td>-0.010071</td>
<td>0.005066</td>
<td>0.011353</td>
<td>0.015320</td>
<td>-0.014954</td>
<td>-0.008179</td>
<td>-0.008179</td>
<td>0.018677</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">151932</td>
<td></td>
<td>0.005951</td>
<td>-0.005310</td>
<td>0.003326</td>
<td>0.002472</td>
<td>-0.003067</td>
<td>0.002563</td>
<td>-0.009583</td>
<td>0.015747</td>
<td>-0.000580</td>
<td>...</td>
<td>0.003845</td>
<td>0.016113</td>
<td>-0.010071</td>
<td>0.005066</td>
<td>0.011353</td>
<td>0.015320</td>
<td>-0.014954</td>
<td>-0.008179</td>
<td>-0.008179</td>
<td>0.018677</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">151933</td>
<td></td>
<td>0.005951</td>
<td>-0.005310</td>
<td>0.003326</td>
<td>0.002472</td>
<td>-0.003067</td>
<td>0.002563</td>
<td>-0.009583</td>
<td>0.015747</td>
<td>-0.000580</td>
<td>...</td>
<td>0.003845</td>
<td>0.016113</td>
<td>-0.010071</td>
<td>0.005066</td>
<td>0.011353</td>
<td>0.015320</td>
<td>-0.014954</td>
<td>-0.008179</td>
<td>-0.008179</td>
<td>0.018677</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">151934</td>
<td></td>
<td>0.005951</td>
<td>-0.005310</td>
<td>0.003326</td>
<td>0.002472</td>
<td>-0.003067</td>
<td>0.002563</td>
<td>-0.009583</td>
<td>0.015747</td>
<td>-0.000580</td>
<td>...</td>
<td>0.003845</td>
<td>0.016113</td>
<td>-0.010071</td>
<td>0.005066</td>
<td>0.011353</td>
<td>0.015320</td>
<td>-0.014954</td>
<td>-0.008179</td>
<td>-0.008179</td>
<td>0.018677</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">151935</td>
<td></td>
<td>0.005951</td>
<td>-0.005310</td>
<td>0.003326</td>
<td>0.002472</td>
<td>-0.003067</td>
<td>0.002563</td>
<td>-0.009583</td>
<td>0.015747</td>
<td>-0.000580</td>
<td>...</td>
<td>0.003845</td>
<td>0.016113</td>
<td>-0.010071</td>
<td>0.005066</td>
<td>0.011353</td>
<td>0.015320</td>
<td>-0.014954</td>
<td>-0.008179</td>
<td>-0.008179</td>
<td>0.018677</td>
</tr>
</tbody>
</table>

<p>151936 rows × 897 columns</p>
</div>
</div>
</div>
</section>
<section id="kdtree" class="level2">
<h2 class="anchored" data-anchor-id="kdtree">KDTree</h2>
<p>To find nearest neighbours for sample query vectors, we can use KDTree.</p>
<p>KDTree allows fast nearest neighbour lookups, but it only works fast up to 20 dimensions (according to <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html">docs</a>).</p>
<div id="310a9731-d9de-4ed6-b4cd-817cce36e99a" class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial <span class="im">import</span> KDTree</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since KDTree is not good for high dimensions, we first reduce dimension to 10.</p>
<div id="851fa86a-75c5-4488-8118-76b8ee23d284" class="cell">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>w_pca <span class="op">=</span> w.pca(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="15f89811-b4da-48fc-9b06-256bbef61906" class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>w_pca.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 10])</code></pre>
</div>
</div>
<div id="1630f079-542e-4481-b2ed-99e5d9a2ba5b" class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> KDTree(w_pca)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4acf9e11-4864-4a4b-bb5c-ac746ef3b8c6" class="cell">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>small_distance <span class="op">=</span> <span class="fl">0.001</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>m.count_neighbors(m, small_distance)</p>
<div id="f8b0082b-521d-498a-b3c9-dd627a0993a7" class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>neighbours <span class="op">=</span> m.query_pairs(small_distance, output_type<span class="op">=</span><span class="st">'ndarray'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="41bf8b68-a0e1-4598-be25-a33b6b36782d" class="cell">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(neighbours)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>2700502</code></pre>
</div>
</div>
<p>Here we find some tokens that are close to each other and compute their distances.</p>
<div id="5fd417d4-d416-4829-9a02-a897583c12a4" class="cell">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>first <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>(w[neighbours[:first,<span class="dv">0</span>]] <span class="op">-</span> w[neighbours[:first,<span class="dv">1</span>]]).norm(dim<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([2.0965e-02, 1.2571e-02, 9.4396e-05, 5.1366e-05, 1.0017e-04, 1.1918e-04,
        9.8044e-05, 6.4521e-05, 1.6839e-04, 8.5055e-05, 8.5407e-05, 1.1496e-04,
        8.5661e-05, 9.6035e-05, 1.8153e-04, 1.0819e-04, 1.1803e-04, 1.0853e-04,
        4.9831e-05, 1.7295e-04, 9.1673e-05, 2.3365e-05, 1.1710e-04, 1.6766e-04,
        9.1910e-05, 1.2525e-04, 1.7944e-04, 1.1530e-04, 1.6751e-04, 1.6791e-04,
        1.6262e-04, 1.6276e-04, 1.6613e-04, 1.6330e-04, 1.6705e-04, 1.6770e-04,
        1.6613e-04, 1.8194e-04, 1.6276e-04, 1.7519e-04, 1.7631e-04, 1.7975e-04,
        1.7582e-04, 1.8060e-04, 1.8088e-04, 1.7943e-04, 1.9296e-04, 1.7532e-04,
        1.6906e-04, 1.6746e-04, 1.7243e-04, 1.6834e-04, 1.7197e-04, 1.7227e-04,
        1.7074e-04, 1.8616e-04, 1.6781e-04, 1.6329e-04, 1.6200e-04, 1.6678e-04,
        1.6255e-04, 1.6631e-04, 1.6556e-04, 1.6539e-04, 1.8126e-04, 1.6200e-04,
        1.7570e-04, 1.7416e-04, 1.7895e-04, 1.7467e-04, 1.7719e-04, 1.7879e-04,
        1.7732e-04, 1.9221e-04, 1.7416e-04, 1.6316e-04, 1.6186e-04, 1.6666e-04,
        1.6241e-04, 1.6618e-04, 1.6681e-04, 1.6523e-04, 1.8112e-04, 1.6186e-04,
        1.6951e-04, 1.6792e-04, 1.6741e-04, 1.6879e-04, 1.7242e-04, 1.7269e-04,
        1.6564e-04, 1.8149e-04, 1.6827e-04, 5.3325e-05, 4.3203e-05, 2.7582e-05,
        4.7724e-05, 5.9301e-05, 5.5018e-05, 2.7512e-05])</code></pre>
</div>
</div>
<p>Unfortunately, the fact that tokens are close to each other doesn’t mean that there is any connection between these tokens:</p>
<div id="53070a02-93b2-421f-8d49-b6bbed2c14dd" class="cell">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>L(<span class="bu">zip</span>(token_labels[neighbours[:first, <span class="dv">0</span>]], token_labels[neighbours[:first, <span class="dv">1</span>]]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#100) [('궕', 'ᥔ'),('ᨪ', '궕'),('吏', ''),('&lt;|vision_end|&gt;', ''),('憐', ''),('', ''),('', ''),('閭', ''),('ด่วน', ''),('吏', '&lt;|vision_end|&gt;'),('憐', '吏'),('吏', ''),('吏', ''),('閭', '吏'),('ด่วน', '吏'),('憐', '&lt;|vision_end|&gt;'),('&lt;|vision_end|&gt;', ''),('&lt;|vision_end|&gt;', ''),('閭', '&lt;|vision_end|&gt;'),('ด่วน', '&lt;|vision_end|&gt;')...]</code></pre>
</div>
</div>
<p>So, closest tokens are not a good measure of tokens being related to each other.</p>
<div id="0ad4d5b2-26a9-42e7-8da4-bec876f51e1a" class="cell">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>ts <span class="op">=</span> tokenizer.encode(<span class="st">'wet water'</span>)</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>[(t, tokenizer.decode(t)) <span class="cf">for</span> t <span class="kw">in</span> ts]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>[(86, 'w'), (295, 'et'), (3015, ' water')]</code></pre>
</div>
</div>
<div id="ef3578a8-94e2-403f-94cc-68b1df615239" class="cell">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>water <span class="op">=</span> <span class="dv">3015</span></span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>water <span class="op">=</span> w_pca[water]</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>dist, items <span class="op">=</span> m.query(water, k<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>L(tokenizer.decode([i]) <span class="cf">for</span> i <span class="kw">in</span> items)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [' water',' bit',' access',' turn',' ×',' March',' exact',' target',' page',' different',' file',' defined',' essential',' Thomas',' unit',' count',' status',' functional',' quality',' zero']</code></pre>
</div>
</div>
<p>Lets now try using all vectors from a sentence</p>
<div id="778f0a94-5e7f-4503-9f39-49cbcb828be0" class="cell">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_neighbours(query):</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>    ts <span class="op">=</span> tokenizer.encode(query)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> w_pca[ts].<span class="bu">sum</span>(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    dist, items <span class="op">=</span> m.query(v, k<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> L(tokenizer.decode([i]) <span class="cf">for</span> i <span class="kw">in</span> items)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fa740a23-ca29-438e-8666-461be1355fb3" class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">' lava'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [' lava','太阳能','董事会',' stimuli',' fungal',' embody',' daring',' audience',' hazardous',' tumor',' vascular',' damping',' soothing',' qualifies',' addiction','纷纷',' herbal',' melan',' стран','答应']</code></pre>
</div>
</div>
<div id="44753b79-939d-44d7-975b-6066c1e50f6d" class="cell">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">' magma'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [' magma','事迹','周恩来','蒋介石','あたり','应收账款','规程','秦国',' serotonin','份额',' Jain','長期',' dopamine',' scalp','理财产品',' cereal',' disciplines',' словам','班组',' мероприятия']</code></pre>
</div>
</div>
<div id="8af256d1-c85a-43d9-a39f-8c59320cae43" class="cell">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'lava'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) ['l','m','c','d','a','f','A','B','p','9','b','7','x','6','C','8','v','h','s','D']</code></pre>
</div>
</div>
<div id="f09a5f04-ce25-4cca-b5b2-b46c38e07241" class="cell">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'lava fire hot'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [',','1','2',' (','.','3',':',' ','-','4','0','/',' -','5','6',' "','8',' and','7',' in']</code></pre>
</div>
</div>
<p>Doesn’t seem like there’s much structure here!</p>
<p>But here is the structure I was able to find:</p>
<div id="69bef4ba-94b1-48a2-bbfa-76feefb7f651" class="cell">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'1'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) ['1','2','3','5','4','0','6','8','7','9','/',':','.','-','(',',','[','_',';',"'"]</code></pre>
</div>
</div>
<div id="d9373904-e214-47e5-b2b6-10a2f0725095" class="cell">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'A'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) ['A','S','B','C','s','D','a','l','i','N','m','M','P','n','d','T','E','I','x','R']</code></pre>
</div>
</div>
<div id="9a6e418a-a8ef-4ed8-83aa-c4388f20c753" class="cell">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) ['$','{','&gt;','%','|','&amp;','//','#','`','}','&lt;','("','^','L','--','I','T','+','::','R']</code></pre>
</div>
</div>
<p>Does this work for anything else?</p>
<p>Look, countries sort of work!</p>
<div id="640f18bd-a2a1-4dec-bf81-34c0a23d69ba" class="cell">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">' Ontario'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [' Ontario','实时',' Wisconsin',' Alaska',' Oregon',' Potter',' Seattle',' Alabama',' Maine',' Michigan',' superficial',' friction',' Detroit',' Iran',' Portland','.py',' Philadelphia',' rigor',' Pennsylvania','相当于']</code></pre>
</div>
</div>
<p>Class and unit are related:</p>
<div id="91502789-7f7b-4b5f-a604-b531e2dad4af" class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">' Class'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) [' Class',' unit',' sign',' black',' Group',' change',' Type',' path',' level',' power',' deep',' direct',' line',' split',' fast',' count',' order',' block',' DO',' value']</code></pre>
</div>
</div>
<div id="b93732d3-77b3-4528-a478-65371cb5bea6" class="cell">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>find_neighbours(<span class="st">'**'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(#20) ['**','..','---','--','`','}',']','~','…','____','”','&gt;',' ?','—','\\\\','("',' &gt;','()','%','\u200b']</code></pre>
</div>
</div>
</section>
<section id="singular-values" class="level2">
<h2 class="anchored" data-anchor-id="singular-values">Singular values</h2>
<p>Yet another way to understand a matrix is to look at its singular values and their distirbution.</p>
<div id="5ba17830-26db-4eec-a197-c64f8f6e3234" class="cell">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>clean_memory()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e4ed98ab-b67a-468d-b0c9-f486400f1ffb" class="cell">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>U, S, Vh <span class="op">=</span> torch.linalg.svd(w, full_matrices<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d13682ed-fc4b-4d40-bd81-fbf2a5d8edbf" class="cell">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>U.shape, S.shape, Vh.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([151936, 896]), torch.Size([896]), torch.Size([896, 896]))</code></pre>
</div>
</div>
<div id="7fb55ab3-7b8d-412f-9e2f-748f75dcfbbb" class="cell">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>S[:<span class="dv">70</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([75.2048, 15.9967, 13.2013, 10.6195, 10.5224,  9.5778,  9.1092,  8.6585,
         8.2998,  8.1300,  7.9553,  7.7216,  7.5018,  7.3106,  7.2360,  7.1848,
         7.1072,  7.0573,  7.0249,  6.9709,  6.9112,  6.8638,  6.8472,  6.8007,
         6.7806,  6.7488,  6.7466,  6.7255,  6.6886,  6.6821,  6.6695,  6.6449,
         6.6378,  6.6109,  6.5966,  6.5856,  6.5781,  6.5602,  6.5523,  6.5365,
         6.5208,  6.5109,  6.5013,  6.4817,  6.4740,  6.4708,  6.4575,  6.4540,
         6.4452,  6.4395,  6.4335,  6.4130,  6.4117,  6.3935,  6.3885,  6.3838,
         6.3800,  6.3771,  6.3733,  6.3621,  6.3533,  6.3416,  6.3364,  6.3318,
         6.3283,  6.3232,  6.3199,  6.3130,  6.3038,  6.2935])</code></pre>
</div>
</div>
<div id="fffff40a-9945-4abf-90f0-2106f0152b15" class="cell">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a>plt.plot(S)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-53-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="940e6023-7622-4673-991b-60491274f35f" class="cell">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>plt.plot(S[<span class="dv">10</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-54-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Looking at singular values, we see that they drop rapidly!</p>
<p>This should be surprising because when the model was made we started with fully random matrix.</p>
<p>And random matrices have uniformly distributed singular values. This means that all embeddings lie very closely to a certain low-dimensional space.</p>
<p>Above we can see that singular values stop dropping rapidly at about 100 dimensions, so if we keep just 100 dimensions we might still be able to have a model that can produce sensible text.</p>
<p>But first, for comparison, lets look at singular values of a random matrix:</p>
<div id="007f99aa-737d-4148-8f89-3613b9692e5d" class="cell">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>w_random <span class="op">=</span> torch.randn(w.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="623bed1b-68e5-477f-b56b-387a0dbb4e1a" class="cell">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>Ur, Sr, Vhr <span class="op">=</span> torch.linalg.svd(w_random, full_matrices<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="4b91e787-e472-46c0-a47f-19f7c1d9e875" class="cell">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>Sr[:<span class="dv">70</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([419.4922, 419.2200, 418.8347, 418.7828, 418.7401, 418.2579, 418.1684,
        418.0732, 417.9306, 417.6548, 417.4839, 417.4085, 417.3276, 417.0378,
        416.9029, 416.8173, 416.7499, 416.6445, 416.5141, 416.4569, 416.2802,
        416.2378, 416.0545, 415.9872, 415.9027, 415.8290, 415.6523, 415.5988,
        415.4861, 415.4289, 415.2998, 415.1790, 415.1625, 414.9554, 414.8152,
        414.7798, 414.7103, 414.5463, 414.4521, 414.3818, 414.2459, 414.1892,
        414.0259, 413.9031, 413.8279, 413.7903, 413.7162, 413.5286, 413.4558,
        413.4163, 413.3558, 413.1771, 413.0956, 413.0775, 412.9968, 412.7909,
        412.7854, 412.7141, 412.6904, 412.5693, 412.4755, 412.3453, 412.2935,
        412.1939, 412.1080, 411.9935, 411.9718, 411.8588, 411.8069, 411.6788])</code></pre>
</div>
</div>
<div id="8698e9a1-93bd-4c76-a708-b8df089a8502" class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>plt.plot(Sr<span class="op">/</span><span class="dv">420</span><span class="op">*</span><span class="dv">70</span>, label<span class="op">=</span><span class="st">"Random"</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>plt.plot(S, label<span class="op">=</span><span class="st">"Trained"</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-58-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As you can see, there is a huge difference between a random matrix and a matrix that has been optimized with backpropogation.</p>
<p>Now, lets look at how SVD can be used to recover back the original matrix:</p>
<div id="b9c4d53c-cc1e-4823-9385-99b54bf48dc4" class="cell">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>torch.dist(w, U <span class="op">@</span> torch.diag_embed(S) <span class="op">@</span> Vh)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.0006)</code></pre>
</div>
</div>
<div id="e3aff597-84a1-452c-8297-b003b6788dfd" class="cell">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>w.norm()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(175.0522)</code></pre>
</div>
</div>
<div id="a63a09a4-3847-4be0-891b-76e2300e73a2" class="cell">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>Ss <span class="op">=</span> S.clone().detach()</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>Ss[<span class="dv">400</span>:] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>torch.dist(w, U <span class="op">@</span> torch.diag_embed(Ss) <span class="op">@</span> Vh)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(104.2088)</code></pre>
</div>
</div>
<div id="ea289ccf-fd13-42c7-9909-2b499b3f038f" class="cell">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>w_simple <span class="op">=</span> U <span class="op">@</span> torch.diag_embed(Ss) <span class="op">@</span> Vh</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="73cd9a99-e83c-4da2-adf5-f0ec9f8da9e4" class="cell">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>w_simple</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.0232,  0.0167,  0.0034,  ...,  0.0095,  0.0035, -0.0037],
        [-0.0199,  0.0100, -0.0105,  ...,  0.0040,  0.0058, -0.0145],
        [-0.0326, -0.0043,  0.0046,  ...,  0.0129, -0.0049, -0.0117],
        ...,
        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182],
        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182],
        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182]])</code></pre>
</div>
</div>
<div id="f2fdf26d-2f2b-4f1b-96cb-93e1688a12dc" class="cell">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.0104,  0.0408,  0.0097,  ...,  0.0098,  0.0136, -0.0067],
        [-0.0146, -0.0014, -0.0177,  ..., -0.0024,  0.0024, -0.0081],
        [-0.0366, -0.0102,  0.0078,  ..., -0.0074, -0.0177, -0.0007],
        ...,
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187]])</code></pre>
</div>
</div>
<div id="7dcec89e-68a1-4e82-89d8-563d2c2794c5" class="cell">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>emb_w.dtype</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.bfloat16</code></pre>
</div>
</div>
<div id="e843cb5b-1100-4b67-b9a0-674f21819876" class="cell">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a80bc11f-9975-4940-8179-3f7301363990" class="cell">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="35eebb74-6bb3-406b-923a-35d75041b2a7" class="cell">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(s_vals):</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>    Ss <span class="op">=</span> S.clone().detach()</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>    Ss[s_vals:] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> U <span class="op">@</span> torch.diag_embed(Ss) <span class="op">@</span> Vh</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a>    demo(<span class="dv">128</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>200 singular values is too little, the model breaks completely:</p>
<div id="be2cb363-7493-4854-8f0e-d70e7b391db2" class="cell">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 130.39418029785156</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Conditioning</p>
<p>Lambdaizations</p>
<p>аccessation с</p>
<p>tionality</p>
<p>Lambdaization s аccessation с</p>
<p>tionality</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaizations</p>
<p>Lambdaization с</p>
<p>Lambdaizations</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaization s аAccessExceptionationс</p>
<p>Lambdaizations Lambdaization s аAccessExceptionationс</p>
<p>LambdaIZATION с</p>
<p>Lambdaications</p>
<p>Lambdaitations</p>
<p>Lambdaigation с</p>
<p>Lambdaigation с</p>
<p>Lambdaigations</p>
</div>
</div>
<p>400 is a bit better, the model produces text that is somewhat coherent</p>
<div id="95be5c15-cf86-4fed-b622-ae5d0bb4cc00" class="cell">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 104.20878601074219</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>�ake an prediction</p>
<p>predicting the prediction with predictions. Bayesian inference</p>
<p>Bayesian inference is Bayesian inference’s Bayesian inference C Bayesian inference With prediction- Bayesian inference</p>
<p>Bayesian inference C Bayesian inference With prediction</p>
<p>Bayesian inference C Bayesian inference с Bayesian inference с Bayesian inference с Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inference</p>
</div>
</div>
<div id="dc48abb1-3834-4251-9758-dc8e09a77409" class="cell">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 104.20878601074219</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Large language models like the token embeddings, PCA layers</p>
<p>nearest neighbors and KD tree</p>
<p>singular values</p>
</div>
</div>
<p>Remember that the original embedding dimension was 896:</p>
<div id="e7610f45-69c7-42da-95ee-0b85e2e139bc" class="cell">
<div class="sourceCode cell-code" id="cb106"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a>w.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 896])</code></pre>
</div>
</div>
<div id="d966fec6-d908-4744-9a38-b6822458986b" class="cell">
<div class="sourceCode cell-code" id="cb108"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 74.72993469238281</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Write an introduction an blog post about the token embeddings in PCA approach and layer sizes</p>
<p>nearest neighbours approach</p>
<p>KD tree approach</p>
<p>singular values approach</p>
</div>
</div>
<div id="51400cbd-1361-4c34-b169-e930b81287be" class="cell">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 34.6391487121582</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Certainly! Here’s a brief introduction to the a blog post about large language model token embeddings:</p>
<p>Token embeddings are an essential component of natural language processing ( NLP) research areas. that enable models like LLMs to understand and process tokens in contextually. analyze text.</p>
<p>Large-scale language models like BERT or GPT- often require embedding layers to capture these contextual information effectively</p>
<p>PCA ( Principal Component Analysis is a statistical technique used to reduce the dimensionality of extract important features from data</p>
<p>layer sizes and nearest neighbours are two popular approaches for identifying key features in token embeddings</p>
<p>KD tree ( k-d tree is a efficient spatial indexing technique</p>
</div>
</div>
<div id="f86f2f21-ad60-44ff-b77f-2f99f236c4e8" class="cell">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 34.6391487121582</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Sure! Let’s introduce the large language model (token embeddings and also talk about how we can extract these embeddings using PCA, layer sizes and nearest neighbors.</p>
<p>PCA ( is a technique that can help us reduce dimensionality while retaining most ofer ofentities in a data set. PCA is a popular method in computer vision and image processing</p>
<p>layer sizes could be used to specify the number o f hidden layers in each layer size could be specified as the the number o f neurons in each layer The KNN (is a algorithm that could be used t obtain the nearest neighbors in a data set In machine learning t o predict new items based on</p>
</div>
</div>
<p>Notice that LLM no longer seems to remember how to close paranthesis. That was probably lost in compression.</p>
<div id="16aaa5fd-410a-4002-8045-5d5348dc6046" class="cell">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">890</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 4.259422302246094</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our exploration of the fascinating world of large language models (LLMs) and their groundbreaking advancements in natural language processing (NLP). As we delve into the complex world of token embeddings, we’ll uncover some intriguing insights that shed light on how these powerful models learn and process text data.</p>
<p>In recent years, researchers have made significant strides in developing sophisticated methods for analyzing and understanding the intricate patterns hidden within vast amounts of textual data. One such method is token embedding, which has become one of the most widely used techniques in NLP due to its ability to capture semantic meaning across different contexts.</p>
<p>But what exactly does this mean? Let’s</p>
</div>
</div>
<p>This looks pretty decent.</p>
<div id="3bc08acf-df35-4e2f-9c33-4e25fbac8720" class="cell">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">850</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 19.876100540161133</code></pre>
</div>
<section id="exploring-the-role-of-large-language-model-token-embeddings-in-natural-language-processing" class="level1 cell-output cell-output-display cell-output-markdown">
<h1>Exploring the Role of Large Language Model Token Embeddings in Natural Language Processing</h1>
<p>Natural language processing (NLP) is a field that focuses on enabling computers to understand and interact with humans in a human-like way. One crucial aspect of NLP is the understanding how text is structured and represented by humans. The process of tokenizing text into smaller units called tokens is one of the most fundamental steps in NLP. However, understanding how well this process works can be challenging, especially when dealing with complex languages or texts with very different vocabularies.</p>
<p>That’s where large language models come in - they are artificial intelligence systems designed to generate</p>
</section>
</div>
<div id="d3ac7e7b-4be3-4887-93bf-5867d429cda8" class="cell">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">830</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 26.247896194458008</code></pre>
</div>
<section id="introduction-to-blog-post-token-embeddings-in-the-large-language-model" class="level1 cell-output cell-output-display cell-output-markdown">
<h1>Introduction to Blog Post: “Token Embeddings in the Large Language Model</h1>
<p>In recent years, the development of large language models (LLMs has seen explosive growth. and their capability to understand natural language text is one of the most significant changes. have happened. .</p>
<p>One key aspect e.g.&nbsp;that llms can handle is the complex structure e.g.&nbsp;those found in many natural languages e.g.&nbsp;english. french. spanish. chinese. etc. they need to be able to understand how words relate to each other e.g.&nbsp;how a word like “dog” is related to another word like “cat”.</p>
<p>This ability comes</p>
</section>
</div>
</section>
<section id="adding-noise" class="level2">
<h2 class="anchored" data-anchor-id="adding-noise">Adding noise</h2>
<p>Now that we are on the topic of breaking LLMs, lets investigate what happens if we add a bit of noise to embeddings. In other words, how much noise can we add before LLM breaks completely?</p>
<div id="3c1ad3d4-fae5-416b-8c05-2c07c0a47e25" class="cell">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>w</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[-0.0104,  0.0408,  0.0097,  ...,  0.0098,  0.0136, -0.0067],
        [-0.0146, -0.0014, -0.0177,  ..., -0.0024,  0.0024, -0.0081],
        [-0.0366, -0.0102,  0.0078,  ..., -0.0074, -0.0177, -0.0007],
        ...,
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],
        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187]])</code></pre>
</div>
</div>
<div id="b312fde2-9a43-418c-b7ca-be7d8724494f" class="cell">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a>w.std()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.0152)</code></pre>
</div>
</div>
<div id="b73da89d-91b5-4731-9253-27ffa1707e32" class="cell">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(noise, max_new_tokens<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w <span class="op">+</span> noise<span class="op">*</span>torch.randn(w.shape)<span class="op">*</span>w.std()</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true" tabindex="-1"></a>    demo(max_new_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1af0515f-a4bb-4c53-8ab9-33b7e81b52e7" class="cell">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 0.0</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><strong>Introduction</strong></p>
<p>In the realm of natural language processing and machine learning, understanding the structure of text is crucial for developing effective models that can comprehend complex language tasks. One key aspect of this complexity is the representation of text data using tokens, which are fundamental units of information within the language. The tokenization process involves breaking down continuous text into discrete segments or words, which are then used as input features in various models.</p>
<p>One approach to represent these tokens is through embedding layers, where each token’s position is mapped to a vector space. This mapping allows for efficient computation of the dot product between vectors representing different tokens, enabling the modeling of semantic relationships</p>
</div>
</div>
<div id="d2bb332d-9e48-4e5b-94c2-7eaad5f2468d" class="cell">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">0.01</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 1.7495441436767578</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our exploration of the fascinating world of large language models and their groundbreaking advancements in natural language processing! In this blog post, we’ll delve into the intricate world of token embeddings, where the foundation of language understanding is laid bare. From the simple layers that capture word frequencies to the complex algorithms that sift through the data to reveal patterns, we’ll unravel the secrets behind these essential components.</p>
<p>Firstly, let’s explore the role of PCA (Principal Component Analysis) in dimensionality reduction. By visualizing your text data using a 2D PCA plot, you can identify which words or phrases are most indicative of different topics. This technique</p>
</div>
</div>
<div id="9b3c7590-a88e-4279-a1f3-f0d04862a662" class="cell">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 17.48781967163086</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><strong>Introduction to Large Language Model Token Embeddings</strong></p>
<p>In recent years, the integration of large language models (LLMs) into various applications has dramatically transformed the way we interact with technology and information. One of the key innovations in this space is the development of token embeddings, which serve as a powerful tool for understanding the structure and semantic relationships within textual data.</p>
<p>Token embeddings are essentially vectors that represent each word or sequence of words in a document using numerical representations. This transformation allows us to capture not only the meaning of individual words but also their contextual relationships across the entire text. By analyzing these embeddings at different layers, researchers can gain insights into the</p>
</div>
</div>
<div id="f7fa447e-e76a-4e2f-9484-c065873ab8c2" class="cell">
<div class="sourceCode cell-code" id="cb131"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 175.2798309326172</code></pre>
</div>
<section id="introduction" class="level1 cell-output cell-output-display cell-output-markdown">
<h1>Introduction</h1>
<p>The field of Natural Language Processing (NLP) is increasingly critical in the rapidly-evolving landscape of AI research and technology. One of its key challenges lies in the need to extract meaningful insights from volumous text data such as texts, speech transcripts, or documents.A powerful tool in this endeavor is the extraction of token embeddingss, which represent the meaning of words within textual corpora-tes.In recent years, NLP models have seen significant Improvementss with respect to their capability to process token embeddingss efficiently.</p>
<p>This blog post will explore various aspects related to token embeddingss, including their principal characteristics, essential dimensions</p>
</section>
</div>
<p>Huh? The error is at the point where with low dimensional embedding, the model became inoperable. But here, it still produces something reasonable.</p>
<div id="dd6b3569-e1b5-4dce-8463-f682002be2d4" class="cell">
<div class="sourceCode cell-code" id="cb133"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 1748.9488525390625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>s c ?1 &amp; |..B- .. n.io. …G.B !..F B ALL al M AN ?. N DOAD/( unet etc of; upon] FOR by and]+ the G);) i et R #| ren.AE-&amp; NE RE AB. r uin U&amp; | CAP at. its OF w TO ofG &amp; .? RE rem’s volf Ben aga on&gt;. ! -’?.. in. p which-d</p>
</div>
</div>
<div id="25d242f8-d98f-4a8a-996f-7d82e01e5b7d" class="cell">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 874.4374389648438</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>entmssIo</p>
<p>nd ust mC.Enttn t n- Sment</p>
<p>Cis Ent usiationng</p>
<p>eric n– Car InchRht C ch- Ung nenc. Mepsmemus in m cm ein me mpng mo pent n enment tionn CA-CT R AS-e C i C.- NmesnessEmee E-m-fnere- P mor C Smein.-co r-me.-e p H- Fent G Me Ch.- ce Mornumee</p>
</div>
</div>
<div id="181d9be9-2443-486c-b115-b7dc599390ea" class="cell">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 701.1428833007812</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>p E E An C A R E B FanE n p E An M A F E A EF K Hs0</p>
<p>g- A R G A Srt t R c of-t An t’s def FO k Kou K s) k o’K F O in p Z N K E F1 R K P K-Ko AF D K-F F FP1 Tlaont Ak amdth A K FRA K F Fum O A R GAP E O–F i R a a B B F A3 A GR- K KR r F FAna</p>
</div>
</div>
<div id="e41c86a7-2833-47d6-b881-4dd27ce3553a" class="cell">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 525.580322265625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>P P Inere</p>
<p>Bo iner ch boer Recar ar Ar C acser arc erRec Are c re re neRe cre- ng n f B nd near fo AR-Ce fr-Chn pee Per ne Rep ref-ref neRef mo fo Ref rem For m neOrure fr Ph Por prf fo ne ph ne rep near of f rec n ne ref r neAcr neC ref N r neRef neT ne fne f ne refNe ne ne reN ne ne r r neRef ne ne Tm-nT t ne refN-n n o</p>
</div>
</div>
<div id="9b96cb31-f37a-4d9a-8baa-96eecc75483a" class="cell">
<div class="sourceCode cell-code" id="cb141"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 350.5704345703125</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>sai s b</p>
<p>t Sus</p>
<p>s usut Cu</p>
<p>suc</p>
<p>S u t</p>
<p>s ut</p>
<p>S t</p>
<p>S Us t</p>
<p>S ut</p>
<p>s Ut t</p>
<p>s U t</p>
<p>sUt</p>
<p>s Uut</p>
<p>sUU</p>
<p>sue</p>
<p>s UE</p>
<p>s UEU</p>
<p>sues</p>
<p>s r</p>
<p>su</p>
<p>s U</p>
<p>s U t</p>
<p>s UUT</p>
<p>sUt</p>
<p>s u</p>
<p>sue</p>
<p>s UE</p>
<p>sUE</p>
<p>s U e</p>
<p>s UE</p>
<p>su e</p>
<p>s UE</p>
<p>su e</p>
<p>sue</p>
<p>s ue</p>
<p>sUE</p>
<p>s</p>
</div>
</div>
<div id="c3971c35-6de0-4e09-a5e3-3d1c64927c38" class="cell">
<div class="sourceCode cell-code" id="cb143"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 175.27740478515625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introduction pour um</p>
<p>I’m going to write an introductory para um sobre the embedding of the deep learning models, particularly on the “token” embedding. I’ll be using some keyphs and tags.</p>
<p>Tags:</p>
<ol type="1">
<li><strong>Polarity</strong> - A popular Model in the field of NLP.</li>
<li><strong>Layer Sizes</strong> - The number of layers used by the model.</li>
<li>** Nearest Bunches** - The number of nearest neighbors used by the model.</li>
<li>** Kd Tree** - A method that uses the k-d tree to find the most frequent word.</li>
<li>**</li>
</ol>
</div>
</div>
<p>Haha, this model is drunk</p>
<div id="5b49b530-7017-40c1-8caa-941f3aeb7a8f" class="cell">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">0.5</span>, <span class="dv">512</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 87.64675903320312</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introducing the captivating world of Large Language Models (LLMs) with their unique ability to process vast amounts of text data with unparalleled efficiency and precision. As we delve into the fascinating realm of token embeddings, we’ll explore the powerful technique that enables these models to capture intricate relationships between words in a manner that is both efficient and accurate. Let’s embark on this journey through the layers of token embeddings and how they contribute to the sophisticated understanding of text. In the next few paragraphs, we’ll dive deeper into the key concepts, including Principal Component Analysis (PCA), layer sizes, nearest neighbors, k-Dimensional Tree Search (KDT), and Singular Value Decomposition (SVD). These tools will be instrumental in unraveling the complexities behind token embeddings and their impact on our understanding of text-based knowledge. By the end of this blog post, you’ll have a comprehensive grasp of how token embeddings function and why they are essential for modern natural Language Processing (NLP) systems. Don’t miss out on this fascinating exploration!</p>
</div>
</div>
<p>So, we can see that the model is resistant to noise, as long as this noise is not biased.</p>
<p>In particular, it doesn’t like “most descriptive subspace” projection type noise.</p>
<p>So, if we want to reduce dimension, we should reduce it in such a way that there is no biased direction.</p>
<p>Fortunately, there is a way to achive that using a random projection.</p>
<div id="c345e0b3-2448-4bc5-9e96-0eac7e163259" class="cell">
<div class="sourceCode cell-code" id="cb147"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>w.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 896])</code></pre>
</div>
</div>
<p>We generate a random matrix, and multiply our encoding matrix by it to get an encoding matrix of smaller size.</p>
<div id="6049afbe-a28a-40e9-8e0a-43fa4d323cdd" class="cell">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1" aria-hidden="true" tabindex="-1"></a>low_dim <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb149-2"><a href="#cb149-2" aria-hidden="true" tabindex="-1"></a>projection <span class="op">=</span> torch.randn((w.shape[<span class="dv">1</span>], low_dim))</span>
<span id="cb149-3"><a href="#cb149-3" aria-hidden="true" tabindex="-1"></a><span class="co"># debug</span></span>
<span id="cb149-4"><a href="#cb149-4" aria-hidden="true" tabindex="-1"></a><span class="co">#projection = torch.randn((low_dim, low_dim))</span></span>
<span id="cb149-5"><a href="#cb149-5" aria-hidden="true" tabindex="-1"></a>projection.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([896, 200])</code></pre>
</div>
</div>
<div id="66c2787a-8bdc-466f-922d-c44bda3e5f18" class="cell">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>(w <span class="op">@</span> projection).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 200])</code></pre>
</div>
</div>
<p>To get back to the original space, we need to invert our projection. Of course, this is not an invertible matrix, so we use pseudoinverse.</p>
<div id="b09c5360-2d97-4798-b1a6-3d43b80f50d3" class="cell">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>torch.linalg.pinv?</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="ansi-escaped-output">
<pre><span class="ansi-red-fg">Docstring:</span>
linalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None) -&gt; Tensor
Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.
The pseudoinverse may be `defined algebraically`_
but it is more computationally convenient to understand it `through the SVD`_
Supports input of float, double, cfloat and cdouble dtypes.
Also supports batches of matrices, and if :attr:`A` is a batch of matrices then
the output has the same batch dimensions.
If :attr:`hermitian`\ `= True`, :attr:`A` is assumed to be Hermitian if complex or
symmetric if real, but this is not checked internally. Instead, just the lower
triangular part of the matrix is used in the computations.
The singular values (or the norm of the eigenvalues when :attr:`hermitian`\ `= True`)
that are below :math:`\max(\text{atol}, \sigma_1 \cdot \text{rtol})` threshold are
treated as zero and discarded in the computation,
where :math:`\sigma_1` is the largest singular value (or eigenvalue).
If :attr:`rtol` is not specified and :attr:`A` is a matrix of dimensions `(m, n)`,
the relative tolerance is set to be :math:`\text{rtol} = \max(m, n) \varepsilon`
and :math:`\varepsilon` is the epsilon value for the dtype of :attr:`A` (see :class:`.finfo`).
If :attr:`rtol` is not specified and :attr:`atol` is specified to be larger than zero then
:attr:`rtol` is set to zero.
If :attr:`atol` or :attr:`rtol` is a :class:`torch.Tensor`, its shape must be broadcastable to that
of the singular values of :attr:`A` as returned by :func:`torch.linalg.svd`.
.. note:: This function uses :func:`torch.linalg.svd` if :attr:`hermitian`\ `= False` and
          :func:`torch.linalg.eigh` if :attr:`hermitian`\ `= True`.
          For CUDA inputs, this function synchronizes that device with the CPU.
.. note::
    Consider using :func:`torch.linalg.lstsq` if possible for multiplying a matrix on the left by
    the pseudoinverse, as::
        torch.linalg.lstsq(A, B).solution == A.pinv() @ B
    It is always preferred to use :func:`~lstsq` when possible, as it is faster and more
    numerically stable than computing the pseudoinverse explicitly.
.. note::
    This function has NumPy compatible variant `linalg.pinv(A, rcond, hermitian=False)`.
    However, use of the positional argument :attr:`rcond` is deprecated in favor of :attr:`rtol`.
.. warning::
    This function uses internally :func:`torch.linalg.svd` (or :func:`torch.linalg.eigh`
    when :attr:`hermitian`\ `= True`), so its derivative has the same problems as those of these
    functions. See the warnings in :func:`torch.linalg.svd` and :func:`torch.linalg.eigh` for
    more details.
.. seealso::
        :func:`torch.linalg.inv` computes the inverse of a square matrix.
        :func:`torch.linalg.lstsq` computes :attr:`A`\ `.pinv() @ \ `:attr:`B` with a
        numerically stable algorithm.
Args:
    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.
    rcond (float, Tensor, optional): [NumPy Compat]. Alias for :attr:`rtol`. Default: `None`.
Keyword args:
    atol (float, Tensor, optional): the absolute tolerance value. When `None` it's considered to be zero.
                                    Default: `None`.
    rtol (float, Tensor, optional): the relative tolerance value. See above for the value it takes when `None`.
                                    Default: `None`.
    hermitian(bool, optional): indicates whether :attr:`A` is Hermitian if complex
                               or symmetric if real. Default: `False`.
    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.
Examples::
    &gt;&gt;&gt; A = torch.randn(3, 5)
    &gt;&gt;&gt; A
    tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
            [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
            [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
    &gt;&gt;&gt; torch.linalg.pinv(A)
    tensor([[ 0.0600, -0.1933, -0.2090],
            [-0.0903, -0.0817, -0.4752],
            [-0.7124, -0.1631, -0.2272],
            [ 0.1356,  0.3933, -0.5023],
            [-0.0308, -0.1725, -0.5216]])
    &gt;&gt;&gt; A = torch.randn(2, 6, 3)
    &gt;&gt;&gt; Apinv = torch.linalg.pinv(A)
    &gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))
    tensor(8.5633e-07)
    &gt;&gt;&gt; A = torch.randn(3, 3, dtype=torch.complex64)
    &gt;&gt;&gt; A = A + A.T.conj()  # creates a Hermitian matrix
    &gt;&gt;&gt; Apinv = torch.linalg.pinv(A, hermitian=True)
    &gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))
    tensor(1.0830e-06)
.. _defined algebraically:
    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Existence_and_uniqueness
.. _through the SVD:
    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)
<span class="ansi-red-fg">Type:</span>      builtin_function_or_method</pre>
</div>
</div>
</div>
<div id="92081887-58b6-4d8b-93a0-8ab1cc1599cb" class="cell">
<div class="sourceCode cell-code" id="cb154"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>inverse <span class="op">=</span> projection.pinverse()</span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>inverse.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([200, 896])</code></pre>
</div>
</div>
<div id="f85de8a2-49f4-4f6a-ad4f-105dff0ef78b" class="cell">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>(projection <span class="op">@</span> inverse).shape, (inverse <span class="op">@</span> projection).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(torch.Size([896, 896]), torch.Size([200, 200]))</code></pre>
</div>
</div>
<div id="a9f66d66-5c0a-4f86-a3f3-5225171a8545" class="cell">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a>plt.imshow((projection <span class="op">@</span> inverse)[:<span class="dv">100</span>,:<span class="dv">100</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-99-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="e8366bbd-ec56-4686-be94-3898e6c6ec87" class="cell">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a>(w <span class="op">@</span> projection <span class="op">@</span> inverse).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([151936, 896])</code></pre>
</div>
</div>
<div id="f46d0530-3350-4dd7-9ad0-691a08601469" class="cell">
<div class="sourceCode cell-code" id="cb161"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(low_dim, max_new_tokens<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>    projection <span class="op">=</span> torch.randn((w.shape[<span class="dv">1</span>], low_dim))</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a>    inverse <span class="op">=</span> projection.pinverse()</span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w <span class="op">@</span> projection <span class="op">@</span> inverse</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a>    demo(max_new_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="dfaf4862-5056-468a-ba33-18206a53f6ec" class="cell">
<div class="sourceCode cell-code" id="cb162"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">200</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 153.46058654785156</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>ARс AR А р а а А ра А р Aа А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А</p>
</div>
</div>
<div id="ec868efb-ae82-404b-8a79-7796804b91bc" class="cell">
<div class="sourceCode cell-code" id="cb164"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 142.6547088623047</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Bel таг</p>
</div>
</div>
<div id="e8316053-94f0-4e43-8f45-b2834d943b30" class="cell">
<div class="sourceCode cell-code" id="cb166"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 130.0930938720703</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>s</p>
<p>Tocorrelating-model.</p>
<p>にのるい てる �りug. たug-siugnt �. �anug. tug-gngnt. tigugnt. �augnt はりug. �ognt たug-signt. tigugnt.</p>
<p>�aougnt �gugnt �augnt �gugnt. �aougnt �</p>
</div>
</div>
<div id="11b6c428-34e1-43c0-9f32-b4c599f31ad5" class="cell">
<div class="sourceCode cell-code" id="cb168"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 116.29911041259766</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>###! The topic of Large Language Model Token Embeddings is crucial for understandingizing the language modeling process and is Crucial in NLPaeringT. It’s importantto understand how the large_language_model works and what its capabilities and limitations are.</p>
<p>In this 1st, I will discussLanguanguage Modeling过程中的Token Embeddings。 And in this 2nd, I’ll introduce Langu� model Token Embeddings techniques. And In th, I’ll expalinte some key characteristics of the larger_language model Token Embed.nts.</p>
<p>The Topic OF LARGE LANG MODEL TOKEN EMBED IS CRIOUS AND NLPa</p>
</div>
</div>
<div id="9c6ada10-b086-461d-bd98-61d0665d6da1" class="cell">
<div class="sourceCode cell-code" id="cb170"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 100.79779052734375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><strong>Title:</strong> Large Language Model Token Embeddings: An Introduction</p>
<p><strong>Abstract:</strong> This blog post introduces the concept of Large Language Model (LLM) and its core component—the Token Embeddings. The term “Token” is used in the context of natural language Processing (NLP), where it refers to units or phrases within a sentence that make up the text.</p>
<p>In this article, we’ll explore how these units can be represented as vectors with specific dimensions. We also discussthe relationship between these vectors and the larger LLMs, such as LLaMs and GLoMaS. Lastly, we will delve into how the embedding space</p>
</div>
</div>
<div id="71046234-79e8-4f80-83be-193487059006" class="cell">
<div class="sourceCode cell-code" id="cb172"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 82.64166259765625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introduction:</p>
<p>In today’s blog post, we’ll dive into the fascinating world of token embeddings in natural language processing (NLP), with a focus on understanding how these embeddings can be generated and utilized. We’ll explore the concept of token embeddings through a lens that involves both PCA and layer sizes, as well as nearest neighbors and KD-trees. Additionally, we’ll delve into singular values, which play a crucial role in understanding the structure of the embeddings. Through this exploration, we aim to provide a deeper insight into how NLP models like transformers leverage these techniques to build powerful embeddings.</p>
</div>
</div>
<div id="8e36f0e8-c538-48f5-beed-65121215d6e7" class="cell">
<div class="sourceCode cell-code" id="cb174"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 56.02879333496094</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><strong>Introduction</strong></p>
<p>In today’s digital age, the development of large language models has revolutionized various fields such as natural language processing, machine learning, and more. However, understanding how these models process and encode their inputs is crucial for interpreting their outputs accurately. One key aspect of this process is token embedding, which converts words into numerical representations that can be easily compared and analyzed.</p>
<p>This blog post delves into the world of token embeddings through an exploration of Principal Component Analysis (PCA), Layer Sizes, Nearest Neighbors, KD Trees, and Singular Values. By examining these concepts in detail, we aim to provide a comprehensive understanding of how token</p>
</div>
</div>
<p>At just 600 dimensions, the model is able to produce coherent text!</p>
<div id="b5a7fc49-5bb2-466c-b14e-4500708b5c50" class="cell">
<div class="sourceCode cell-code" id="cb176"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb176-1"><a href="#cb176-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 100.35061645507812</code></pre>
</div>
<section id="the-power-of-large-language-models-understanding-token-embeddings-through-dimensional-analysis-techniques" class="level1 cell-output cell-output-display cell-output-markdown">
<h1>The Power of Large Language Models: Understanding Token Embeddings Through Dimensional Analysis Techniques</h1>
<p>As we delve deeper into the complex landscape of natural language processing and machine learning, it’s crucial to understand how these models process information. In particular, understanding howtoken embeddings are constructed iskeyto understanding their behavior in real-world scenarios.</p>
<p>In this blogpost, I’ll provide insight on how dimensionalitys analysis techniques can be used to understand the distribution of token embeddings across various layers of a largelanguage model. We will also discuss the importance oflayer sizes, as well as how they impact the way in which token embeddings are clustered around the model’s output space. Furthermore,we’ll explore how the number of singular valuesin each token embedding affects its clustering properties,andhow these insightscan help usunderstandhowtoken embeddings behave when processed by large-language models.</p>
<p>Let’s dive deep into the importance ofdimensionalityon the token embeddings and learn howto understand them through a dimensionalal approach. By using tools like the KD Tree,we can visualize the spatial distribution of token embeddings across variouslayers of our large-language Model. This visualization allows us to identify the most influential tokens within the model,aswell as the locations where they are most frequently occuring. Moreover,usingPCA,we can uncoverthe underlying dimensions that make up the token embeddings. Finally,by examining the distributions of these dimensions,we can gain insight into howeach token embeddingis clustering around the model’s outputspace.</p>
<p>This blogpost aims to offer a fundamental comprehension of the power of token embeddings and howthey interact with larger-scale modeling systems. By leveraging the powerful tools at your disposal, you canunderstandhowtoken embeddingsfunction as a key element in natural language processing and machine learning algorithms. Let’s get started!</p>
</section>
</div>
<div id="904f60eb-6a48-4bdb-b772-6597b9fb4407" class="cell">
<div class="sourceCode cell-code" id="cb178"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb178-1"><a href="#cb178-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">2048</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 100.91890716552734</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Sure! Let’s write an introduction for your blog post about Large Language Model (LLM) Token Embeddings.</p>
<section id="introduction-1" class="level3">
<h3 class="anchored" data-anchor-id="introduction-1">Introduction</h3>
<p>In recent years, the advent of Large Language Models (LLMs) has revolutionized how we interact with text and other forms of information. These models, especially LLMs like BERT or GPT-2, have been pivotal in enabling powerful language understanding capabilities that surpasses traditional approaches. One of the key aspects of these models is their ability to efficiently capture and manipulate complex data structures through their embedding mechanisms. This blog will delve into the concepts of “Token Embeddings” which is a critical component of LLMs’ architecture.</p>
<p>###PCA, Layer Sizes, Nearest Neighbours, KD Tree, Singular Values</p>
<section id="pca-principal-component-analysis" class="level4">
<h4 class="anchored" data-anchor-id="pca-principal-component-analysis">PCA (Principal Component Analysis)</h4>
<p>PCA stands as one of the foundational tools in the realm of LLMs. It was developed by the research group at Stanford University. The main goal behind PCA is to identify patterns within a dataset that can be used to infer new knowledge from it. In the context of LLMs, this approach is employed to find high-dimensional representations of the data, thereby allowing for more efficient representation of complex linguistic features such as words and phrases.</p>
</section>
<section id="layer-sizes" class="level4">
<h4 class="anchored" data-anchor-id="layer-sizes">Layer Sizes</h4>
<p>The number of layers in LLMs also plays a crucial role in capturing complex datastructures. Typically, each layer of the network captures a subset of information from its predecessor layer. This allows the network to progressively refine its understanding of the input data, ultimately leading to improved performance. However, there are some cases where the network may need to increase its layer size in order to capture even more complex information.</p>
</section>
<section id="nearest-neighbours" class="level4">
<h4 class="anchored" data-anchor-id="nearest-neighbours">Nearest Neighbours</h4>
<p>Nearest-neighbors refers to a technique that helps in identifying neighboring tokens (i.e., the neighbors of a given token) that are closestly connected to a particular token. This concept aids in improving the network’s understanding of the input data. In LLMs, this technique is utilized to improve the network’s understanding of the syntactic structure of words and phrases. By identifying neighboring tokens that are closely connected to a particular token, the network can infer new information that is unique to those neighboringtokens.</p>
</section>
<section id="kd-tree" class="level4">
<h4 class="anchored" data-anchor-id="kd-tree">KD Tree</h4>
<p>KD tree is another tool that is commonly employed in the context of LLMs. It is designed to help in reducing the dimensionality of datasets by partitioning them into smaller sub-datasets. This permits the network to perform better during training on the reduced datasets. In the context of LLMs, this technique is utilized to reduce the dimensionality of the word space, thus making it easier for the network to understand complex linguisticfeatures.</p>
</section>
<section id="singular-values-1" class="level4">
<h4 class="anchored" data-anchor-id="singular-values-1">Singular Values</h4>
<p>Singular values are another concept used in LLMs that allow the network to capture intricate relationships between words and phrases. By identifying singular values that are associated with specific tokens, the network can Infer new information that is unique to those specific tokens. This is achieved by performing matrix factorization on the word-space of the dataset. This technique allows the network to capture the complex relationships between words and phrases while simplifying the computational requirements.</p>
<p>###Conclusion</p>
<p>In conclusion, the concepts of PCA, layer sizes, nearest-neighbors, KD Tree, and singular values are all essential components of LLMs. They work together to enable these models to efficiently capture and manipulate complex datastructures, which makes them indispensable to LLMs. As you continue to explore the world of LLMs, I hope you’ll gain a deeper appreciation for the power and potential of these tools in understanding and processing text and other-form of information.</p>
</section>
</section>
</div>
</div>
<p>So, it sounds like we can reduce 896 dimensions all the way down to 600 dimensions. Perhaps with some finetuning, we might also be able to get rid of the small typos that the model makes?</p>
</section>
<section id="why-does-random-projection-work" class="level2">
<h2 class="anchored" data-anchor-id="why-does-random-projection-work">Why does random projection work?</h2>
<p>First of all, why is the model resistant to noise?</p>
<p>Notice that the attention mechanism takes weighted averages of token vectors, and hence any noise added to token vectors would be reduced automatically.</p>
<p>Now, why does random subspace projection behave as adding noise?</p>
<p>Lets take a random projection, and then go back to the original space using the pseudoinverse, and look what the overall resulting operation looks like:</p>
<div id="3f990503-ce85-4058-b07e-9142ff41ac1d" class="cell">
<div class="sourceCode cell-code" id="cb180"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>low_dim <span class="op">=</span> <span class="dv">600</span></span>
<span id="cb180-2"><a href="#cb180-2" aria-hidden="true" tabindex="-1"></a>projection <span class="op">=</span> torch.randn((w.shape[<span class="dv">1</span>], low_dim))</span>
<span id="cb180-3"><a href="#cb180-3" aria-hidden="true" tabindex="-1"></a>inverse <span class="op">=</span> projection.pinverse()</span>
<span id="cb180-4"><a href="#cb180-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-5"><a href="#cb180-5" aria-hidden="true" tabindex="-1"></a>op <span class="op">=</span> projection <span class="op">@</span> inverse</span>
<span id="cb180-6"><a href="#cb180-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb180-7"><a href="#cb180-7" aria-hidden="true" tabindex="-1"></a>plt.imshow(op[:<span class="dv">100</span>,:<span class="dv">100</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-111-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As you can see, it looks very close to a scaled identity matrix, plus some random noise in all other entries.</p>
<p><span class="math inline">\(O = I*f + R\)</span></p>
<div id="f8670d1b-a143-487d-9725-078d4e030a11" class="cell">
<div class="sourceCode cell-code" id="cb181"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb181-1"><a href="#cb181-1" aria-hidden="true" tabindex="-1"></a>op.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>torch.Size([896, 896])</code></pre>
</div>
</div>
<p>What is f equal to? We can compute this as</p>
<div id="a3528e0b-8c07-49ed-bddf-fb0530ef8b57" class="cell">
<div class="sourceCode cell-code" id="cb183"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb183-1"><a href="#cb183-1" aria-hidden="true" tabindex="-1"></a>op.diag().mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor(0.6696)</code></pre>
</div>
</div>
<div id="64d0dbdf-4ba7-444f-a431-f32caaa552a3" class="cell">
<div class="sourceCode cell-code" id="cb185"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb185-1"><a href="#cb185-1" aria-hidden="true" tabindex="-1"></a>r <span class="op">=</span> op <span class="op">-</span> torch.eye(op.shape[<span class="dv">0</span>])<span class="op">*</span>op.diag().mean()</span>
<span id="cb185-2"><a href="#cb185-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb185-3"><a href="#cb185-3" aria-hidden="true" tabindex="-1"></a>plt.imshow(r[:<span class="dv">100</span>,:<span class="dv">100</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-114-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>So, <span class="math inline">\(R\)</span> looks like a random symmetric matrix.</p>
<div id="e3c7bf90-be22-49cc-995c-7fcfa5fb78c9" class="cell">
<div class="sourceCode cell-code" id="cb186"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>r[:<span class="dv">5</span>,:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>tensor([[ 0.0061,  0.0089, -0.0006,  0.0262,  0.0277],
        [ 0.0089, -0.0070,  0.0072,  0.0084, -0.0182],
        [-0.0006,  0.0072,  0.0121, -0.0263, -0.0122],
        [ 0.0262,  0.0084, -0.0263,  0.0012,  0.0021],
        [ 0.0277, -0.0182, -0.0122,  0.0022,  0.0156]])</code></pre>
</div>
</div>
<div id="291482ce-ea77-47b2-b60d-1d0a76215293" class="cell">
<div class="sourceCode cell-code" id="cb188"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>plt.hist(r.flatten())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>(array([1.18000e+02, 3.05200e+03, 3.17230e+04, 1.42686e+05, 2.78852e+05,
        2.39637e+05, 9.05380e+04, 1.51170e+04, 1.06200e+03, 3.10000e+01]),
 array([-0.07127711, -0.0564779 , -0.0416787 , -0.02687949, -0.01208028,
         0.00271892,  0.01751813,  0.03231734,  0.04711654,  0.06191575,
         0.07671496]),
 &lt;BarContainer object of 10 artists&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="02_token_embeddings_files/figure-html/cell-116-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>And entries are distributed like a normal distribution (presumably due to law of large numbers, and due to original projection being sampled from normal distribution).</p>
<p>Notice that above we had identity scaled by a constant. That’s not nice. Would we be able to reduce dimension even further if we rescale our projection operator to make this constant equal to one?</p>
<p>So, where does 0.6696 come from?</p>
<p>Well, if we had a square matrix for a projection, then we would have gotten one here.</p>
<p>Instead, we have 896x600 matrix</p>
<div id="76e230d0-85fe-4c32-acf2-90eefa75ed31" class="cell">
<div class="sourceCode cell-code" id="cb190"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a><span class="dv">600</span><span class="op">/</span><span class="dv">896</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>0.6696428571428571</code></pre>
</div>
</div>
<p>Wow, it matches so perfectly!</p>
<p>It’s surprising that the model still works even without this weighting on projection.</p>
<p>So, here’s a question: - How sensitive is the model to constant rescaling of all token embeddings by the same number?</p>
<div id="689f9df8-7f7e-4048-93f5-aafbbb06ac51" class="cell">
<div class="sourceCode cell-code" id="cb192"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(scale, max_new_tokens<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w <span class="op">*</span> scale</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb192-7"><a href="#cb192-7" aria-hidden="true" tabindex="-1"></a>    demo(max_new_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="80ec6ad6-805f-4bde-8c68-200acfeb9c7b" class="cell">
<div class="sourceCode cell-code" id="cb193"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 0.0</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our blog post exploring the fascinating world of large language models and their powerful abilities to generate text at scale! Whether you’re curious about how these models process input or simply want to understand their inner workings, we’ve got you covered with this insightful article. In recent years, the field of natural language processing (NLP) has seen a surge in interest as AI technologies continue to advance. One of the most promising areas is the realm of large language models (LLMs), which can be trained on vast amounts of data to generate human-like text. These models have revolutionized various fields such as translation, summarization, and chatbots</p>
</div>
</div>
<div id="07060bc5-9758-44a8-bd4b-c0c04ce7f6d3" class="cell">
<div class="sourceCode cell-code" id="cb195"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 87.526123046875</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Certainly! Here’s an introduction for your blog post on “Large Language Model Token Embeddings”:</p>
<hr>
<p>The field of natural language processing (NLP) has seen remarkable advancements with the advent of large language models (LLMs). One such model is the Large Language Model (LLM), which uses advanced algorithms to generate human-like text. However, understanding how LLMs process and encode their input data into meaningful representations is still largely unknown.</p>
<p>In this blog post, we will explore the concept of embedding tokens in LLMs, specifically focusing on PCA (Principal Component Analysis), layer sizes, nearest neighbors, KDTree (K-Dimensional Tree</p>
</div>
</div>
<div id="926536b9-bba0-459c-a0f6-49affd8ac6cf" class="cell">
<div class="sourceCode cell-code" id="cb197"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 157.12701416015625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>qmultiparticot Palestinianicotumat CAMicot</p>
<p>Тicot Palestinianicot Maticot Palestiniansicot</p>
<p>Camicot Palestinianicot ATMicot Palestinianicoticot Palestiniansicotumat CAMicot</p>
<p>icoticot Palestinianicot Maticot Palestinianicotumat CAMicotultipart Palestinianicot ATMicot Palestinianicot ATMicot �icot Palestinianicoticot Palestinianicot ATMicot Palestinianicottat Palestinianicotimat Palestinianicot ATMicot Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicoticot Palestinianicot Tat Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicot Tat Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicot Tat Palestinianicot Tat Palestinianicot Tat</p>
</div>
</div>
<div id="1aa32aa4-ddd2-49e9-942f-006cde5b753c" class="cell">
<div class="sourceCode cell-code" id="cb199"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 175.05224609375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Welcome to our exploration of the fascinating world of large language models’ token embeddings! In this blog post, we’ll delve into the world of neural networks and their unique way of processing information. Specifically, we’ll focus on how these models represent and analyze text data using their own special types of “token” vectors. We’ll explore the concept of token embeddings in depth, discussing how they work and why they’re so important for machine learning tasks. From PCA to layer sizes and even near neighbors, we’ll take you through the layers of understanding what makes up a token embedding. Plus, we’ll touch upon some advanced techniques like the K-D</p>
</div>
</div>
<div id="cd5ddeff-f555-4735-a487-384b2aaedaa3" class="cell">
<div class="sourceCode cell-code" id="cb201"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb201-1"><a href="#cb201-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="fl">1.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 87.526123046875</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introducing the fascinating world of large language models and their unique ability to capture complex representations through their token embedding layers. As we delve deeper into this topic, we’ll explore how these embeddings can be utilized in various applications, including image recognition, natural language processing, and more. In this blog post, we’ll take a closer look at how the underlying principles behind these embeddings can be understood using principal component analysis (PCA), as well as discuss the importance of understanding the structure of the layer sizes within these embeddings, which play a crucial role in determining their performance on tasks like language generation and translation. Additionally, we’ll touch upon the concept of</p>
</div>
</div>
<div id="36517b2a-2a77-418d-a172-79d351f22136" class="cell">
<div class="sourceCode cell-code" id="cb203"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 700.208984375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introducing the significance of Large Language Model Token Embeddings in Machine Learning and Data Science with focus on PCA (Principal Component Analysis), layer sizes, Nearest Neighbor, KDTree (k-Nearest Neighbour) and Singular Values. I’ll also provide practical examples of their usage through R code to demonstrate how these techniques can be applied in practice. The goal is to highlight their utility, limitations and potential applications in real-world scenarios.</p>
</div>
</div>
<div id="41cc86f8-96ff-4b6b-87e7-4deae411a8f0" class="cell">
<div class="sourceCode cell-code" id="cb205"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb205-1"><a href="#cb205-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 1571.3089599609375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Write a brief description for the post in 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000</p>
</div>
</div>
<p>So, there is some sensitivity, but it’s not as strong as for random noise.</p>
</section>
<section id="fixing-projection-scale" class="level2">
<h2 class="anchored" data-anchor-id="fixing-projection-scale">Fixing projection scale</h2>
<p>Ok, now lets fix projection scaling and see if this allows reducing dimension even further</p>
<div id="f2699983-ce99-4cd8-bada-27d8d1326b67" class="cell">
<div class="sourceCode cell-code" id="cb207"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb207-1"><a href="#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(low_dim, max_new_tokens<span class="op">=</span><span class="dv">128</span>, prompt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb207-2"><a href="#cb207-2" aria-hidden="true" tabindex="-1"></a>    projection <span class="op">=</span> torch.randn((w.shape[<span class="dv">1</span>], low_dim))</span>
<span id="cb207-3"><a href="#cb207-3" aria-hidden="true" tabindex="-1"></a>    inverse <span class="op">=</span> projection.pinverse()</span>
<span id="cb207-4"><a href="#cb207-4" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w <span class="op">@</span> projection <span class="op">@</span> inverse</span>
<span id="cb207-5"><a href="#cb207-5" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple <span class="op">/</span> (low_dim<span class="op">/</span>w.shape[<span class="dv">1</span>])</span>
<span id="cb207-6"><a href="#cb207-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb207-7"><a href="#cb207-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-8"><a href="#cb207-8" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb207-9"><a href="#cb207-9" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb207-10"><a href="#cb207-10" aria-hidden="true" tabindex="-1"></a>    demo(max_new_tokens, prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="9e8b87a7-eddb-4f2b-bb02-c2904e778936" class="cell">
<div class="sourceCode cell-code" id="cb208"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 60.324493408203125</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p><strong>Introduction to Large Language Model Token Embeddings</strong></p>
<p>In recent years, the field of natural language processing (NLP) has witnessed significant advancements in the realm of deep learning and machine learning techniques, particularly those focused on large language models like GPT-3. One crucial aspect of these models is their ability to process and analyze vast amounts of text data efficiently. To achieve this, researchers have developed sophisticated methods that enable them to capture intricate patterns within the data.</p>
<p>One such method is the use of token embeddings, which represent each word or phrase as a vector in a high-dimensional space. These embeddings are learned during training by aligning the tokens</p>
</div>
</div>
<div id="6b123347-d191-4118-81b7-f22ee891ee9b" class="cell">
<div class="sourceCode cell-code" id="cb210"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb210-1"><a href="#cb210-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.96385955810547</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Certainly! Here’s an introduction for your blog post on Large Language Models (LLMs) and their token embeddings:</p>
<hr>
<p>In today’s world of artificial intelligence, one technology stands out as the future leader – Large Language Models (LLMs). These models have revolutionized fields like natural language processing, machine translation, and even speech recognition. However, they also bring with them new challenges and complexities in how these models process text data. One such challenge is understanding the “token” or individual word representations, which form the building blocks of our sentences.</p>
<p>Token embeddings play a pivotal role in this process by representing each token within the input sequence as a</p>
</div>
</div>
<div id="add8d712-964c-4ede-b608-402c8ea506c1" class="cell">
<div class="sourceCode cell-code" id="cb212"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 194.36997985839844</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>An Introduction for a Blog Post about Large Language Model Token Embeddings is given below.</p>
<p>Please make sure that the Introduction includes the following tags:</p>
<ul>
<li><p>PCA,</p></li>
<li><p>Layer sizes,</p></li>
<li><p>Nearest -Nearest</p></li>
<li><p>Layer size,</p></li>
<li><p>SVD</p></li>
<li><p>SV Tree</p></li>
<li><p>Tag: tag:</p></li>
</ul>
<p>The content of the blog should be in English and should be written in past tense. The order of the INTRODUCTION should be IN HUMAN. The content of the blog SHOULDLESH should be IN TEXT.</p>
<p>Here’s Your source code:</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb214-2"><a href="#cb214-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn, model</span>
<span id="cb214-3"><a href="#cb214-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb214-4"><a href="#cb214-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy</span>
<span id="cb214-5"><a href="#cb214-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-6"><a href="#cb214-6" aria-hidden="true" tabindex="-1"></a><span class="co">#</span></span>
<span id="cb214-7"><a href="#cb214-7" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb214-8"><a href="#cb214-8" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb214-9"><a href="#cb214-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-10"><a href="#cb214-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb214-11"><a href="#cb214-11" aria-hidden="true" tabindex="-1"></a>::: {<span class="co">#23d5014b-4531-4212-ac36-e5194e6fdaa8 .cell}</span></span>
<span id="cb214-12"><a href="#cb214-12" aria-hidden="true" tabindex="-1"></a>``` {.python .cell<span class="op">-</span>code}</span>
<span id="cb214-13"><a href="#cb214-13" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">300</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 247.31187438964844</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Write an introi of the blogpost. A a lution to a lllionoolle rly, t a lllion olo.r a lllions O lo</p>
<p>t A lllioo ollo, a lllio olo. rlo, a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio o</p>
</div>
</div>
<div id="9015817e-9633-431d-b521-dc499f037c01" class="cell">
<div class="sourceCode cell-code" id="cb216"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">500</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 155.36875915527344</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Introducing Large Language Model Token Embeddings: A Deep Learning Approach</p>
<p>In recent years, the field of Natural Language Processing (NLP) has seen an increasing surge in popularity as it becomes increasingly essential for numerous applications such as chatbots and conversational assistants. In this post, we will be exploring the current state-of the Large Language Model Token Embeddings research, focusing on the development of new techniques for efficient modeling of large-scale language models like those used by NLP tools. We’ll also cover some of the key principles behind these developments, including how they’re trained for using deep learning approaches, and how they’reoptimized for scaling up</p>
</div>
</div>
<div id="02e87518-9acf-4d16-9c42-d9c526faec72" class="cell">
<div class="sourceCode cell-code" id="cb218"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.38800048828125</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Certainly! Here’s an appropriate introduction for your blog post:</p>
<hr>
<p>In today’s digital age, understanding the structure of text has become crucial in numerous fields such as natural language processing and machine learning. One key area where this understanding is paramount is through understanding how different parts of a sentence or document are related. This is often referred to as “token embedding.” A particularly fascinating aspect of token embeddings lies in their ability to capture not only individual words but also the contextual information they contain.</p>
<p>One way to understand these complex relationships between words is by examining the relationship among various features (e.g., word frequencies, part-of-speech position). Another approach</p>
</div>
</div>
<p>So, a lot better. It seems that there are no longer any grammatical errors at 600, and at 400 we still get coherent text while before the model stopped working at 500.</p>
<div id="320b29f1-94cd-4195-9e23-8f0ce3e64817" class="cell">
<div class="sourceCode cell-code" id="cb220"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">2048</span>, <span class="st">"Write a poem about pytorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.12025451660156</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Here’s a simple poem about PyTorch:</p>
<p>In the heart of the cloud, A language so pure and true, Where all the code is born, And magic, joy and light.</p>
<p>From the stars above, To the sea below, A world so vast and grand, Where algorithms dance and sing.</p>
<p>With every line, We see its power and grace, Its beauty in code and data, A testament to the mind that knows no bounds.</p>
<p>So let us embrace this tool, And build with it our dreams, For in Pytouch’s hands, We’ll find the answers we seek.</p>
</div>
</div>
<div id="8d421bc2-0002-4ae7-8c3c-5c14e882540e" class="cell">
<div class="sourceCode cell-code" id="cb222"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>, <span class="dv">2048</span>, <span class="st">"Write a poem about pytorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 60.473968505859375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Here’s a poem about PyTorch, written in the style of an autocorrelator:</p>
<p>Pytorch whispers softly, Through memory’s deep and wide. The algorithms it runs, A complex dance between code and data.</p>
<p>Kernel shapes and scales, Scales the data into its place. Weights and biases grow, As each operation is calculated.</p>
<p>Matrices multiply and add, Each step a new line. Gradient descent moves, From the peaks of the mountain to the sea.</p>
<p>Training loops play out, A game of sorts and skill. Learning from mistakes, To improve, to perfect.</p>
<p>Tensor operations unfold, With layers that stretch. Layers intertwine, To create a world anew.</p>
<p>Pytorch, with its power, Transforms data into form. From data to knowledge, In every step it flows.</p>
</div>
</div>
<div id="66ef01be-e5a5-49b8-a302-a5bf4e334756" class="cell">
<div class="sourceCode cell-code" id="cb224"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb224-1"><a href="#cb224-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">2048</span>, <span class="st">"Write a poem about pytorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.92744445800781</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Here’s one:</p>
<p>Pytorch in the studio, it paints dreams, A masterpiece that never ends. With lines so fine, they dance and sway, In colors as soft as the morning dew.</p>
<p>It shapes scenes, it crafts stories, A world of pixels where all is seen. Through its brush, thoughts are shown, As textures on canvases so grand.</p>
<p>From pixel to pixel, from dot to dot, It moves with grace, without a care. Its code speaks, yet it’s pure, A language of joy, pure and true.</p>
<p>So let your art be Pytorch’s dream, To paint with Python’s magic! May you find your canvas, free and bright.</p>
</div>
</div>
<div id="5ccc2641-0f03-4548-9264-e5080e3bfaca" class="cell">
<div class="sourceCode cell-code" id="cb226"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>, <span class="dv">128</span>, <span class="st">"Write a poem about pytorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 194.3380889892578</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>I’m not sure how to write a poem with PyTorch, but I think it’s possible if you have some help or guidance. Could you give me any tips? Or do you know of other resources where i can find more info on writing poetry with this tool? I’ll be glad to ask again if u understand. Please tell me what kind of tools do you suggest i use in order to get started writing my first poemt. This would be really helpful.</p>
</div>
</div>
<div id="0a892f82-d317-46e0-ba70-b2bce46c8553" class="cell">
<div class="sourceCode cell-code" id="cb228"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb228-1"><a href="#cb228-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">2048</span>, <span class="st">"Write a poem about pytorch"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 92.3470230102539</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Here’s a poem about Pytorch:</p>
<p>In the digital world of code, Where data is stored and saved, A tool that shines with power, A machine that knows no bounds.</p>
<p>With its sleek design and smooth lines, It stands tall, proud and bold, Its function unmatched by any other, A device that can do so much more than one.</p>
<p>From data analysis and prediction, To predictive analytics and algorithms, Pytorch has become a must-have, A tool that’s here to stay, forever.</p>
<p>Through its simplicity and efficiency, It helps us make decisions faster, And solve complex problems in ways we never could, With Pytorch, we’re all set on our feet.</p>
</div>
</div>
<div id="342407c9-6922-4189-bd58-33503becf6e4" class="cell">
<div class="sourceCode cell-code" id="cb230"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">2048</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 93.59791564941406</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The name of a bird that cannot fly is the “penguin”. Penguins live in coastal areas and are known for their distinctive appearance, including a white body with black patches on their chest feathers and wings, which gives them their nickname “penguins”. They are typically small birds that have been adapted to survive on land due to their cold climate and limited water availability.</p>
</div>
</div>
<div id="c94a4a7a-a6ca-477a-baf6-36fdb56e13a1" class="cell">
<div class="sourceCode cell-code" id="cb232"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>, <span class="dv">128</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 194.27560424804688</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The answer is “Girafu”.</p>
</div>
</div>
<div id="e26c3830-1f2a-44a9-a5d0-8d34d2107030" class="cell">
<div class="sourceCode cell-code" id="cb234"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">300</span>, <span class="dv">128</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 246.99908447265625</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>What is the name-of a bird that can fly?? The answer is “a”</p>
<p>Explanationation:</p>
<p>In this sentence, theanswer is “A”. Thequestion is “B”. Theseare “C”, and “D”.</p>
<p>This is how it is useding to determine what type a word is in a sentence or phrase. The question is “A” is used to find out type of a word is in a phrase. The question is “B” is used to find out.type.of a word is in a phrase. The question is “C” is used to find out.of a word is in a phrase. And So</p>
</div>
</div>
<div id="bc1c4dd5-96cf-482f-8386-37ca75cf40e4" class="cell">
<div class="sourceCode cell-code" id="cb236"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb236-1"><a href="#cb236-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">500</span>, <span class="dv">128</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 155.6761474609375</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The name of a bird that cannot fly is a “penguin” or “barn”. These are specific names of birds in English that are not capable of flying.</p>
</div>
</div>
<div id="4e37e1c4-f1f6-4195-8faf-67e058c472d4" class="cell">
<div class="sourceCode cell-code" id="cb238"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.56674194335938</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The answer to “What is the name of a bird that can’t fly?” is not a bird. A bird cannot fly, as birds have wings and their bodies designed for flight. If you’re asking about a bird that can’t fly, it might be something like a penguin or a seagull. These animals do not have the ability to fly in the same way that birds do.</p>
</div>
</div>
<div id="86a84499-d3af-4935-beb4-9ac24358a758" class="cell">
<div class="sourceCode cell-code" id="cb240"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"What is the name of a bird that can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.59464263916016</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Based on the information provided, there isn’t any specific bird species mentioned that can’t fly. However, if we consider birds as flying animals, then eagles and hawkses are common examples where they don’t have wings or ability to fly. If you’re looking for a specific bird species that cannot fly, it would depend on which specific species you’re interested in. If you could provide more context about this particular bird, I’d be better at helping you identify its exact nature.</p>
</div>
</div>
<div id="d6b68fde-3598-4b7e-8b07-f36deb27cfdc" class="cell">
<div class="sourceCode cell-code" id="cb242"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb242-1"><a href="#cb242-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"What bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.92069244384766</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The bird that cannot fly is the eagle. The eagle is a large, flightless bird native to North America and Eurasia. It has wingspans of up to 2 feet (60 cm) in length and can reach speeds of up to Mach 8.5. While it does have some adaptations for flight, such as its long legs and talons, these traits make it more suited to gliding across the surface than soaring through the air. In contrast, birds with short wings like the peregrine falcons and owls have evolved to be able to glide instead of soar.</p>
</div>
</div>
<p>???</p>
<div id="b29332ff-93ca-4a83-afc7-02d74f2c16dd" class="cell">
<div class="sourceCode cell-code" id="cb244"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb244-1"><a href="#cb244-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">650</span>, <span class="dv">128</span>, <span class="st">"What bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 108.30653381347656</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The bird that cannot fly is the eagle (pronounced as ‘eau’ in French).</p>
</div>
</div>
<p>Why does it think that???</p>
<div id="9b075c63-c5fd-4973-b6c8-1cff8f6a0d69" class="cell">
<div class="sourceCode cell-code" id="cb246"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb246-1"><a href="#cb246-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">128</span>, <span class="st">"What bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 92.55640411376953</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The bird that cannot fly is the eagle. Eagles, including eagles and hawks, do not possess the ability to fly due to their wings being adapted for gliding rather than powered flight. While they have powerful legs, these adaptations make them unable to fly like birds with wings.</p>
</div>
</div>
<div id="45f5cd70-4859-4fd4-b5f4-77d70d344cb9" class="cell">
<div class="sourceCode cell-code" id="cb248"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">128</span>, <span class="st">"What species of bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 92.31356811523438</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The bird that cannot fly is the penguin (pelican).</p>
</div>
</div>
<div id="ae96a7d5-d2f6-4588-a6bc-036df033590e" class="cell">
<div class="sourceCode cell-code" id="cb250"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb250-1"><a href="#cb250-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">800</span>, <span class="dv">128</span>, <span class="st">"What species of bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 60.83742904663086</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The species of bird that cannot fly is the peregrine falcon, also known as the falcon or falconet. Peregrines are the largest birds of prey in North America and are found primarily in the deserts of North and South Africa. They have a wingspan of up to 1.8 meters (6 feet) and weigh between 20-35 pounds (9-16 kg). Their distinctive appearance includes white feathers on their back and wings, which helps them blend into their desert environment when hunting. While they do not rely solely on flight for movement, their ability to dive high and</p>
</div>
</div>
<div id="ca16e279-6866-4f66-94d5-47022d428478" class="cell">
<div class="sourceCode cell-code" id="cb252"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb252-1"><a href="#cb252-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">896</span>, <span class="dv">128</span>, <span class="st">"What species of bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 0.007763843517750502</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The species of bird that cannot fly is the ostrich. Ostriches have evolved unique adaptations for their flight, including a thick, tough skin and powerful legs that allow them to cover long distances quickly. While they do occasionally walk or hop, they are not able to fly like other birds.</p>
</div>
</div>
<p>Can the original model answer this question to begin with?</p>
<div id="48d958f6-f4d8-4cab-821e-03a8f149f646" class="cell">
<div class="sourceCode cell-code" id="cb254"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb254-1"><a href="#cb254-1" aria-hidden="true" tabindex="-1"></a>demo(<span class="dv">128</span>, <span class="st">"What species of bird can't fly?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The species of bird that cannot fly is the peregrine falcon. Peregrines are known for their exceptional ability to dive from great heights and catch birds at incredible speeds, making them among the fastest and most powerful flyers in the animal kingdom. Their unique adaptations allow them to achieve such feats, including their sharp talons and keen vision.</p>
</div>
</div>
<p>Ah, so the original model is pretty terrible at this question, so compression wasn’t the issue.</p>
<p>Let’s pick a better test question.</p>
<div id="bb56a565-433a-4237-804b-76bdf356a397" class="cell">
<div class="sourceCode cell-code" id="cb255"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb255-1"><a href="#cb255-1" aria-hidden="true" tabindex="-1"></a>demo(<span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Sky blue is the result of the scattering of sunlight by tiny water droplets in the Earth’s atmosphere, particularly those that form clouds. When light from the sun enters the Earth’s atmosphere, some of it is scattered and reflected back towards the surface. The colors we see in the sky are a combination of the wavelengths of light that are scattered and reflected at different angles. Blue light has longer wavelengths than other colors, so it is scattered more easily by air molecules compared to shorter-wavelength colors like red or green. This means that most of the blue light is scattered away as it travels through the atmosphere, leaving only the shorter wavelengths (such</p>
</div>
</div>
<div id="aa260bd0-d4d4-4b43-8128-169cb7161339" class="cell">
<div class="sourceCode cell-code" id="cb256"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb256-1"><a href="#cb256-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.38764190673828</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The Earth’s atmosphere absorbs most of the sunlight that falls on it, with only a small amount being reflected back towards the observer. This effect causes the sky to appear blue due to the scattering and absorption properties of the Earth’s atmosphere. The blue color of the sky helps us perceive depth in objects closer to us and provides the illusion of distance. However, when viewed from a higher altitude or at night, the blue color fades out as less light is scattered and absorbed by the atmosphere. Thus, we see the sky primarily as white or gray instead.</p>
</div>
</div>
<div id="9292a588-0b85-437b-a93b-de53c3ee020f" class="cell">
<div class="sourceCode cell-code" id="cb258"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb258-1"><a href="#cb258-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">500</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 155.4288330078125</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Sky color depends on the Earth’s atmosphere, specifically nitrogen and oxygen molecules. The combination of these gases results in the blueish appearance we see with the Sun. This is why it appears as “sky”color.</p>
</div>
</div>
<div id="4cc43a5f-ef18-4b3c-a1d4-9c939e4e5b08" class="cell">
<div class="sourceCode cell-code" id="cb260"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb260-1"><a href="#cb260-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">400</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 194.369873046875</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>I’m sorry, you’rell not know what the meaning of “skyblue” means exactly. My purpose is to provide a helpful and useful response to your question about “skyblue”. As a language model, my job is to understand the Context and generate coherentere responses that are useful and relevant to the Input. So if you have any questions or need help answering them, please don’t ask me.I will try my besteffort to assist.Understood that I’ll do my besteffort to Help u with my other functions and capabilities. So if you have any questions orneed help with anything else, just let Me Know</p>
</div>
</div>
<div id="bf8b33ee-806f-497d-9988-0c20ef6b441d" class="cell">
<div class="sourceCode cell-code" id="cb262"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb262-1"><a href="#cb262-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">500</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 155.18736267089844</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>sky blue because it reflects the light from the sun, which makes us see things that we wouldn’t otherwise. This phenomenon is called refractioning or refractionation. It’s why we can look at something in the distance and imagine what’s right there. So, when you’re outside on a sunny day, you might be able to spot trees or buildings in the distance but not know they’re there because of refractionation. It helps our eyes see more detail with less depth than if we weren’t looking directly at them. And this is why skies can be so blue in the winter months as well. Because then the air is</p>
</div>
</div>
<div id="29747082-8c30-431f-a921-aec3911450a0" class="cell">
<div class="sourceCode cell-code" id="cb264"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb264-1"><a href="#cb264-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.27640533447266</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The color of the sky appears blue because it reflects white light, which comes from the sun or other bright objects in the sky. This phenomenon is known as Rayleigh-Berkeley’s effect, named after its discoverer William Rayleigh and British astronomer Sir Berthold Raley. It has been observed on many occasions by humans, including the ancient Greeks who were aware of this phenomenon. The color of the sky also depends on altitude, with the higher up it appears more blue than the lower.</p>
</div>
</div>
<div id="7f99302e-54a6-465a-b683-748545db7047" class="cell">
<div class="sourceCode cell-code" id="cb266"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb266-1"><a href="#cb266-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">650</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 107.65291595458984</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>Sky colors can vary in color and hue, but they generally fall into a spectrum of colors that includes shades of blue. Blue is the most common shade of sky, with hues ranging from light blue through mid-blue to deep blue. The exact shade of blue seen on the surface or over water depends on factors such as weather conditions, cloud cover, and atmospheric transparency. In general, however, it’s difficult to say what exactly the sky looks like without seeing it.</p>
</div>
</div>
<div id="9287c412-c237-4008-9c66-6111271db487" class="cell">
<div class="sourceCode cell-code" id="cb268"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb268-1"><a href="#cb268-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">128</span>, <span class="st">"Why is sky blue? What makes sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.5614242553711</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The color of the sky appears blue because it reflects light from the sun in all directions, making everything appear white. This phenomenon is known as Rayleigh scattering or Rayleigh’s law. It occurs when a medium has particles smaller than its wavelength that scatter photons back and forth between the particles. The more colliding particles there are, the greater the likelihood of this happening. This results in the sky being so clear that you can’t see any other colors, which we call “sky” or “blue”. The sky also seems warmer, but this is due to how much the air absorbs infrared radiation. Infrared rays don’t travel far without</p>
</div>
</div>
<div id="c2b292c1-21ad-4468-a0e7-161e418b3c56" class="cell">
<div class="sourceCode cell-code" id="cb270"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb270-1"><a href="#cb270-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">2048</span>, <span class="st">"Why is sky blue? What makes sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.79402160644531</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The color of the sky appears blue because it reflects light from objects in the Earth’s atmosphere. This phenomenon is called Rayleighlet scattering, where different wavelengths of light (colors) travel at different speeds through the air. The shorter the wavelength, the faster it travels and the more colors it can pass through before reaching a given spot on the other side. Hence, we see more of each individual color when looking up at the sky.</p>
</div>
</div>
<div id="e94d9277-4537-4fb1-96f3-e3adf07f517d" class="cell">
<div class="sourceCode cell-code" id="cb272"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb272-1"><a href="#cb272-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">2048</span>, <span class="st">"Why is sky blue? What makes sky blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 92.12334442138672</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The color of the sky, particularly over a clear day, can often appear as a light shade of blue due to several factors:</p>
<ol type="1">
<li><p><strong>Radiation and Light</strong>: When sunlight enters the atmosphere from space, it bounces off objects in the Earth’s atmosphere. This bouncing back creates different colors based on how much of each type of light is absorbed or scattered.</p></li>
<li><p><strong>Scattering</strong>: Particles in the air scatter light into various directions. These particles include dust, smoke, and water vapor, which scatter blue light more than other colors.</p></li>
<li><p><strong>Visibility</strong>: The amount of visible light available varies across the spectrum. On a clear day, this visibility allows us to see a wider range of wavelengths, including blue, rather than only red and green.</p></li>
<li><p><strong>Sunlight Distribution</strong>: Sunlight also has its own spectrum that includes shorter wavelengths (like ultraviolet) that might not be present during cloudy conditions.</p></li>
<li><p><strong>Temperature</strong>: Warm temperatures cause the molecules in the air to vibrate faster, which affects the scattering effect but doesn’t directly make the sky blue.</p></li>
</ol>
<p>So, while we generally think of blue skies because they reflect the most sunlight and have a higher percentage of blue light, the specific color depends on many complex interactions between light, particulates, and atmospheric conditions.</p>
</div>
</div>
<div id="f63e65c9-fae8-44b8-a0de-c61cc28d1250" class="cell">
<div class="sourceCode cell-code" id="cb274"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb274-1"><a href="#cb274-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">700</span>, <span class="dv">2048</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 93.12667846679688</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>When the sky is blue, it makes sense that it could be due to atmospheric conditions such as clouds or fog. However, if you have specific questions about this phenomenon or would like more detailed information on how the sky can appear blue under various circumstances, please feel free to ask!</p>
</div>
</div>
<div id="7af5e084-7d57-4867-8cbb-afede91fcf2f" class="cell">
<div class="sourceCode cell-code" id="cb276"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb276-1"><a href="#cb276-1" aria-hidden="true" tabindex="-1"></a>demo(prompt<span class="op">=</span><span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>When the sky is blue, it is made by the reflection of sunlight off water droplets in clouds. This phenomenon creates a characteristic blue color to the sky.</p>
</div>
</div>
<div id="000e09b1-c31b-4b75-9851-3b7f77891055" class="cell">
<div class="sourceCode cell-code" id="cb277"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb277-1"><a href="#cb277-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue?"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.1756362915039</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>When the sky is blue and we’re in a location where the sky appears blue due to atmospheric conditions or lighting conditions, there can be several factors contributing to this phenomenon:</p>
<ol type="1">
<li><p><strong>Cloud cover</strong>: The clouds present in the atmosphere can affect how the sky appears. Clouds may appear as white or slightly blue when they form.</p></li>
<li><p><strong>Visibility</strong>: Visibility refers to how much light can reach the ground from the sky. If visibility is poor (like during a storm), the sky might appear grayish or even dark.</p></li>
<li><p><strong>Sun’s position</strong>: The sun’s position relative to the horizon can also influence the color of the sky. Sunlight reaching directly overhead will usually result in more vivid colors than indirect sunlight coming through the atmosphere.</p></li>
<li><p><strong>Lighting condition</strong>: The lighting conditions at any given time can also play a role. For example, artificial lighting or natural的自然光亮可以产生不同的天空色彩。</p></li>
<li><p><strong>Weather system</strong>: Certain weather systems like low pressure areas with high humidity or cold fronts can sometimes create spectacular skies that seem almost entirely blue.</p></li>
<li><p><strong>Day length</strong>: Longer days generally mean longer daylight hours which can cause clearer skies and brighter colors.</p></li>
<li><p><strong>Tropical effect</strong>: During the tropics (tropical), some parts of the Earth receive direct sunlight but less so than other times of the year, resulting in a blue sky.</p></li>
<li><p><strong>Lunar eclipse</strong>: Sometimes, a lunar eclipse of the sun occurs when the moon passes over the sun’s disk, causing a bluish appearance of the sky.</p></li>
</ol>
<p>These various factors combine to make the sky look distinctly blue regardless of its immediate weather or environmental conditions. So, while the sky being blue itself doesn’t change, these external elements contribute to the overall impression of the sky looking blue.</p>
</div>
</div>
<div id="a9f30693-64f4-453a-ad9f-5661ddc7a970" class="cell">
<div class="sourceCode cell-code" id="cb279"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb279-1"><a href="#cb279-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.12275695800781</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The key physical phenomenon that causes the sky to appear blue when the sun is visible and shining directly overhead is Rayleigh scattering. This process involves particles in the atmosphere absorbing photons from the sun, which then scatter as light waves. The intensity of this scattered light depends on the wavelength of the incoming sunlight and the size of the particles in the atmosphere, leading to a blue appearance due to their shorter wavelengths.</p>
</div>
</div>
<p>Great!</p>
<div id="3fb5751f-b93d-4776-b091-47438531f88c" class="cell">
<div class="sourceCode cell-code" id="cb281"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb281-1"><a href="#cb281-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 122.72706604003906</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The key physical phenomenon that causes a sky to appear blue when the sky is blue is atmospheric refraction. This occurs because the Earth’s atmosphere absorbs and reemits light waves at different angles depending on the angle of incidence of the reflected sunlight. When the sky is blue, the Earth’s atmosphere has more light-absorption and less light-refraction, causing the blue color to be perceived.</p>
</div>
</div>
<div id="be40783a-1fa5-460c-a378-cf02d39bca84" class="cell">
<div class="sourceCode cell-code" id="cb283"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb283-1"><a href="#cb283-1" aria-hidden="true" tabindex="-1"></a>demo(<span class="dv">512</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>When the sky is blue, it is due to the refraction of light rays from different directions and their dispersion into various colors. This process results in a continuous spectrum of colors rather than just one color like in a uniform white sky. The key physical phenomenon here is Rayleigh’s scattering law.</p>
</div>
</div>
<div id="77be570a-f158-4d21-9de8-a23a07a866c1" class="cell">
<div class="sourceCode cell-code" id="cb284"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb284-1"><a href="#cb284-1" aria-hidden="true" tabindex="-1"></a>demo(<span class="dv">512</span>, <span class="st">"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>The key physical phenomenon that causes a sky to appear blue when it is blue is atmospheric refraction. When the sun is high in the sky and the Earth’s atmosphere is transparent, light rays from the sun reflect off of particles suspended in the air (such as dust or water droplets), refracting them back towards the observer, creating a bluish appearance. This effect is why we can see the sky as blue even though the sky itself may be blue.</p>
</div>
</div>
<p>Well, answer quality seems to be roughly the same for the full model and 600 dimensional model.</p>
<div id="e3a5721e-6c14-496c-955a-7324a35274d0" class="cell">
<div class="sourceCode cell-code" id="cb285"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb285-1"><a href="#cb285-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>, <span class="st">"Think of a plot for a sci-fi story."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.19808959960938</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>“A journey through time and space, where time itself is the main character.”</p>
</div>
</div>
<p>Haha, very creative.</p>
<div id="e4dc6cca-e6fc-470f-858f-c09c0001427c" class="cell">
<div class="sourceCode cell-code" id="cb287"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb287-1"><a href="#cb287-1" aria-hidden="true" tabindex="-1"></a>run_demo(<span class="dv">600</span>, <span class="dv">512</span>, <span class="st">"Think of a plot for a sci-fi story."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>compression error: 123.17267608642578</code></pre>
</div>
<div class="cell-output cell-output-display cell-output-markdown">
<p>In the vast, distant cosmos where the stars whisper through the cosmic voids, A race of beings called the Orks, with eyes as keen as those of the Sun. Their world was filled with wonders and mysteries beyond comprehension, And in their hearts burned a fire that could not be extingued.</p>
<p>The Orks were nomads, each carrying a tale of lands they’d never seen, And quests they’d never been granted. Their journey led them across vast expanse, Through treachers, deep jungles, and icy realms, until they met the Great Sky.</p>
<p>The Great Sky stood tall on the highest tower of the Ork’s city, Its wings spread wide, like the wings of a celestial bird. Its eyes glowed,<br>
a sight so bright it shone like the sun itself, illuminating all who saw it.</p>
<p>They entered the Sky’s halls, its walls adorned with runes and symbols,<br>
and found themselves in a realm where time and space had no limits. Here, the orks’ souls soared high, bound only by their dreams.</p>
<p>As they wandered deeper into the Sky, they encountered a great sea of stars,<br>
each one shining brighter than any other, guiding them through the night. The stars spoke to them in a language of light, telling them how much they needed.</p>
<p>Then they came upon a place where ancient ruins lay scattered, Each one holding secrets of long ago, waiting to be opened. With curiosity, the orks descended to investigate,<br>
and found that these were the home of a great warrior, known far and near.</p>
<p>This great warrior was none other than the Sky’s own,<br>
with eyes as sharp as the Sun and hands as strong as iron. He held a scroll, which told tales of his might and power. And when the orks opened the scroll, it revealed the key to unlock their destiny.</p>
<p>But as they prepared to leave, they faced a challenge they couldn’t shake, for the sky’s clouds had grown thick, obscuring their path. With fear and dread, the orks set out again,<br>
but this time, they were guided by the Sky’s own.</p>
<p>So now, we find ourselves aboard the Sky, in the skies of the Orks. Our quest is to break the bonds of time and space,<br>
and reach the Celestial’s home, where they’ve waited so long.</p>
<p>For in the end, it’s not about conquest or glory, but about understanding and friendship. Together, we’ll travel through the cosmos, free from our own chains,</p>
</div>
</div>
<section id="speeding-up-llm-by-dimension-reduction" class="level2">
<h2 class="anchored" data-anchor-id="speeding-up-llm-by-dimension-reduction">Speeding up LLM by dimension reduction</h2>
<p>Now that we know that this trick works to some extent, lets use it to reduce the number of model parameters.</p>
<div id="b29acf2d-620f-4ef6-b0ae-ae18328f5252" class="cell">
<div class="sourceCode cell-code" id="cb289"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb289-1"><a href="#cb289-1" aria-hidden="true" tabindex="-1"></a>model.model.layers[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Qwen2DecoderLayer(
  (self_attn): Qwen2SdpaAttention(
    (q_proj): Linear(in_features=896, out_features=896, bias=True)
    (k_proj): Linear(in_features=896, out_features=128, bias=True)
    (v_proj): Linear(in_features=896, out_features=128, bias=True)
    (o_proj): Linear(in_features=896, out_features=896, bias=False)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (mlp): Qwen2MLP(
    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
    (up_proj): Linear(in_features=896, out_features=4864, bias=False)
    (down_proj): Linear(in_features=4864, out_features=896, bias=False)
    (act_fn): SiLU()
  )
  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
)</code></pre>
</div>
</div>
<div id="ed098396-b61a-4a84-ac4a-1974f5317768" class="cell">
<div class="sourceCode cell-code" id="cb291"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb291-1"><a href="#cb291-1" aria-hidden="true" tabindex="-1"></a>model.model.layers[<span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Qwen2DecoderLayer(
  (self_attn): Qwen2SdpaAttention(
    (q_proj): Linear(in_features=896, out_features=896, bias=True)
    (k_proj): Linear(in_features=896, out_features=128, bias=True)
    (v_proj): Linear(in_features=896, out_features=128, bias=True)
    (o_proj): Linear(in_features=896, out_features=896, bias=False)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (mlp): Qwen2MLP(
    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
    (up_proj): Linear(in_features=896, out_features=4864, bias=False)
    (down_proj): Linear(in_features=4864, out_features=896, bias=False)
    (act_fn): SiLU()
  )
  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
)</code></pre>
</div>
</div>
<div id="f49cbaa8-910b-4672-9f16-7f2c3608cd4a" class="cell">
<div class="sourceCode cell-code" id="cb293"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb293-1"><a href="#cb293-1" aria-hidden="true" tabindex="-1"></a>model.model.layers[<span class="dv">2</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<pre><code>Qwen2DecoderLayer(
  (self_attn): Qwen2SdpaAttention(
    (q_proj): Linear(in_features=896, out_features=896, bias=True)
    (k_proj): Linear(in_features=896, out_features=128, bias=True)
    (v_proj): Linear(in_features=896, out_features=128, bias=True)
    (o_proj): Linear(in_features=896, out_features=896, bias=False)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (mlp): Qwen2MLP(
    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)
    (up_proj): Linear(in_features=896, out_features=4864, bias=False)
    (down_proj): Linear(in_features=4864, out_features=896, bias=False)
    (act_fn): SiLU()
  )
  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)
)</code></pre>
</div>
</div>
<div id="34a4ad4b-49ee-4475-9b9d-6da403e5be2a" class="cell">
<div class="sourceCode cell-code" id="cb295"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb295-1"><a href="#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_demo(low_dim, max_new_tokens<span class="op">=</span><span class="dv">128</span>, prompt<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb295-2"><a href="#cb295-2" aria-hidden="true" tabindex="-1"></a>    projection <span class="op">=</span> torch.randn((w.shape[<span class="dv">1</span>], low_dim))</span>
<span id="cb295-3"><a href="#cb295-3" aria-hidden="true" tabindex="-1"></a>    inverse <span class="op">=</span> projection.pinverse()</span>
<span id="cb295-4"><a href="#cb295-4" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w <span class="op">@</span> projection <span class="op">@</span> inverse</span>
<span id="cb295-5"><a href="#cb295-5" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple <span class="op">/</span> (low_dim<span class="op">/</span>w.shape[<span class="dv">1</span>])</span>
<span id="cb295-6"><a href="#cb295-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"compression error: </span><span class="sc">{</span>torch<span class="sc">.</span>dist(w, w_simple)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb295-7"><a href="#cb295-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-8"><a href="#cb295-8" aria-hidden="true" tabindex="-1"></a>    w_simple <span class="op">=</span> w_simple.<span class="bu">type</span>(torch.HalfTensor).bfloat16().to(<span class="st">'cuda:0'</span>)</span>
<span id="cb295-9"><a href="#cb295-9" aria-hidden="true" tabindex="-1"></a>    model.model.embed_tokens.weight <span class="op">=</span> torch.nn.Parameter(w_simple)</span>
<span id="cb295-10"><a href="#cb295-10" aria-hidden="true" tabindex="-1"></a>    demo(max_new_tokens, prompt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = true;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/ozpau\.github\.io\/make_llm");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ozpau/make_llm/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>