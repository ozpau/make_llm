{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd49241f-0325-41ac-a5f7-ca50450a121f",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "> Exploring transformers library in depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723ed68-2ab5-493d-9774-2916b647bfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8d1c4-f443-47be-a78e-20afdfadee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import fastai\n",
    "import fastai.torch_core\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import gc\n",
    "import pandas as pd\n",
    "from fastcore.all import *\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb0826-1f41-4ed5-a6b3-cb79ef91cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_memory():\n",
    "    with torch.no_grad():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0b03f-8d55-417e-a3a9-0522bf0c0204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 18:03:36.136586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740449016.158831   27875 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740449016.165767   27875 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c259714e-b5bf-44fe-9130-27b1699e979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7539310-66fe-451b-a16f-38a78351476d",
   "metadata": {},
   "source": [
    "Lets look at the source code for this model.\n",
    "\n",
    "From [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modular_qwen2.py#L121) we have\n",
    "\n",
    "```python\n",
    "class Qwen2ForCausalLM(LlamaForCausalLM):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e64725-ce78-4233-b8d8-00235679cd57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.qwen2.modeling_qwen2.Qwen2Model"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604c371-ba3f-49b4-8928-96b8ee6291a9",
   "metadata": {},
   "source": [
    "And this one is defined [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modular_qwen2.py#L117) as\n",
    "\n",
    "```python\n",
    "class Qwen2Model(MistralModel):\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967a6b9-1809-4701-b5d0-4189c8f5ce60",
   "metadata": {},
   "source": [
    "And MistralModel is defined [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modular_mistral.py#L108) like so\n",
    "\n",
    "```python\n",
    "class MistralModel(LlamaModel):\n",
    "    def __init__(self, config: MistralConfig):\n",
    "        super().__init__(config)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def _update_causal_mask(\n",
    "        self,\n",
    "        attention_mask: torch.Tensor,\n",
    "        input_tensor: torch.Tensor,\n",
    "        cache_position: torch.Tensor,\n",
    "        past_key_values: Cache,\n",
    "        output_attentions: bool,\n",
    "    ):\n",
    "    ...\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_4d_causal_attention_mask_with_cache_position(\n",
    "        attention_mask: torch.Tensor,\n",
    "        sequence_length: int,\n",
    "        target_length: int,\n",
    "        dtype: torch.dtype,\n",
    "        device: torch.device,\n",
    "        cache_position: torch.Tensor,\n",
    "        batch_size: int,\n",
    "        config: MistralConfig,\n",
    "        past_key_values: Cache,\n",
    "    ):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9275d1-c4b0-41ef-a89b-2ab44a6808a4",
   "metadata": {},
   "source": [
    "Here is LlamaModel:\n",
    "\n",
    "```python\n",
    "class LlamaModel(LlamaPreTrainedModel):\n",
    "    \"\"\"\n",
    "    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n",
    "\n",
    "    Args:\n",
    "        config: LlamaConfig\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__(config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embed_tokens = value\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a40c70-3d47-4b6b-966f-bd42c56ed208",
   "metadata": {},
   "source": [
    "So, MistralModel overwrites LLamaModel's deep layers with its own"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eba9c1-022c-41e2-9df4-38242f20c4e6",
   "metadata": {},
   "source": [
    "[Here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/mistral/modular_mistral.py#L101) is MistralDecoderLayer:\n",
    "\n",
    "```python\n",
    "class MistralDecoderLayer(LlamaDecoderLayer):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: int):\n",
    "        super().__init__(config, layer_idx)\n",
    "        self.self_attn = MistralAttention(config=config, layer_idx=layer_idx)\n",
    "        self.mlp = MistralMLP(config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a369f6-96e6-4f4e-bad6-f282598b5578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fdd08f-a8eb-4629-979c-f202f1af349a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2DecoderLayer(\n",
       "  (self_attn): Qwen2SdpaAttention(\n",
       "    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (mlp): Qwen2MLP(\n",
       "    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e53a76-c3fa-42db-8196-394fa1ecd1a6",
   "metadata": {},
   "source": [
    "Btw, here are the model sizes that work on Google Collab:\n",
    "\n",
    "- Qwen/Qwen2.5-0.5B-Instruct\n",
    "- Qwen/Qwen2.5-1.5B-Instruct\n",
    "- Qwen/Qwen2.5-3B-Instruct\n",
    "- Qwen/Qwen2.5-7B-Instruct (13.2 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acfd23b-5ff6-47ae-967b-8b2aae999572",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = model.model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084a39f-003e-4ec8-ac1f-d027e6aa3ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2SdpaAttention(\n",
       "  (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "  (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "  (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "  (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "  (rotary_emb): Qwen2RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.self_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822ac1fb-5ad3-400d-a3b5-c87192304d79",
   "metadata": {},
   "source": [
    "Here is how we can extract query projection weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3a0a9-f459-4731-b504-6c26b14baff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0019, -0.0052,  0.0188,  ..., -0.0061, -0.0153,  0.0038],\n",
       "        [ 0.0084,  0.0018,  0.0435,  ...,  0.0066, -0.0422, -0.0181],\n",
       "        [-0.0168, -0.0248,  0.0422,  ...,  0.0089, -0.0008, -0.0094],\n",
       "        ...,\n",
       "        [-0.1040,  0.0791,  0.0132,  ..., -0.0161, -0.0221, -0.0588],\n",
       "        [-0.0140,  0.0654,  0.0591,  ...,  0.0410, -0.0046,  0.0025],\n",
       "        [ 0.0215,  0.0625,  0.0635,  ..., -0.0036, -0.0354, -0.0957]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.self_attn.q_proj.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6cbb0-7837-474f-8b34-e671d3a48621",
   "metadata": {},
   "source": [
    "Question: What is o_proj?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6f3d44-d8b7-4a7a-9512-44b031ea674e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2MLP(\n",
       "  (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "  (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "  (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "  (act_fn): SiLU()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdaee0-87dd-4bb1-bc5c-25339a6b4124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
