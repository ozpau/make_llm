[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "make_llm",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "make_llm"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "make_llm",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall make_llm in Development mode\n# make sure make_llm package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to make_llm\n$ nbdev_prepare",
    "crumbs": [
      "make_llm"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "make_llm",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/ozpau/make_llm.git\nor from conda\n$ conda install -c ozpau make_llm\nor from pypi\n$ pip install make_llm\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "make_llm"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "make_llm",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "make_llm"
    ]
  },
  {
    "objectID": "discussions/attention.html",
    "href": "discussions/attention.html",
    "title": "Attention",
    "section": "",
    "text": "What made modelling long sequences hard with RNN?\nIn simplified attention, how do we compute context vectors for each token?\nHow does attention help with modelling long sequences? What are the downsides and how are they accounted for?\nHow do we extend simplified attention to allow for trainable weights?\nWhat kind of attention mask do text-generating LLMs use? Is it possible to use other masks? Why would we do that and what the implications would be?\nWhat is register_buffer used for?\nIs softmax necessary? What role does it play?\nHow many context vectors do we need to compute?\nWhat are the different ways of implementing multi-head attention?\nWhy do we need multi-head attention and what can multi-head attention encode that larger single-head attention can’t encode?\nWhen implementing multi-head attention, what are the tensor operations that we need to do and why?",
    "crumbs": [
      "discussions",
      "Attention"
    ]
  },
  {
    "objectID": "discussions/attention.html#concrete",
    "href": "discussions/attention.html#concrete",
    "title": "Attention",
    "section": "",
    "text": "What made modelling long sequences hard with RNN?\nIn simplified attention, how do we compute context vectors for each token?\nHow does attention help with modelling long sequences? What are the downsides and how are they accounted for?\nHow do we extend simplified attention to allow for trainable weights?\nWhat kind of attention mask do text-generating LLMs use? Is it possible to use other masks? Why would we do that and what the implications would be?\nWhat is register_buffer used for?\nIs softmax necessary? What role does it play?\nHow many context vectors do we need to compute?\nWhat are the different ways of implementing multi-head attention?\nWhy do we need multi-head attention and what can multi-head attention encode that larger single-head attention can’t encode?\nWhen implementing multi-head attention, what are the tensor operations that we need to do and why?",
    "crumbs": [
      "discussions",
      "Attention"
    ]
  },
  {
    "objectID": "discussions/attention.html#extra-questions",
    "href": "discussions/attention.html#extra-questions",
    "title": "Attention",
    "section": "Extra questions",
    "text": "Extra questions\n\nWhy do we need to divide by the square root of the dimension of key/value embedding in softmax?\nIs output projection layer necessary?\nWhy do you need dropout? When and where dropout is typically applied?",
    "crumbs": [
      "discussions",
      "Attention"
    ]
  },
  {
    "objectID": "discussions/attention.html#more-abstract",
    "href": "discussions/attention.html#more-abstract",
    "title": "Attention",
    "section": "More Abstract",
    "text": "More Abstract\n\nHow hard is it to change context length of a model?\nWhat are the implications of using dot-product when computing attention weights? Are there any alternatives and what would change if they were used?\nHow can you go about interpreting attention weights?\nWhat are some of the things one can try to reduce amount of computation required to compute context vectors?\nHow does attention in LLM differ from human attention?\nIs attention limited to text?",
    "crumbs": [
      "discussions",
      "Attention"
    ]
  },
  {
    "objectID": "discussions/attention.html#redundant",
    "href": "discussions/attention.html#redundant",
    "title": "Attention",
    "section": "Redundant",
    "text": "Redundant\n\nHow does attention mechanism help maintain contextual relevance over long passages of text?",
    "crumbs": [
      "discussions",
      "Attention"
    ]
  },
  {
    "objectID": "transformers/transformers.html",
    "href": "transformers/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "def clean_memory():\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    gc.collect()\n\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n2025-02-24 18:03:36.136586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1740449016.158831   27875 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1740449016.165767   27875 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n\ntype(model)\n\ntransformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM\n\n\nLets look at the source code for this model.\nFrom here we have\nclass Qwen2ForCausalLM(LlamaForCausalLM):\n    pass\n\ntype(model.model)\n\ntransformers.models.qwen2.modeling_qwen2.Qwen2Model\n\n\nAnd this one is defined here as\nclass Qwen2Model(MistralModel):\n    pass\nAnd MistralModel is defined here like so\nclass MistralModel(LlamaModel):\n    def __init__(self, config: MistralConfig):\n        super().__init__(config)\n        self.layers = nn.ModuleList(\n            [MistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n    ...\n\n    @staticmethod\n    def _prepare_4d_causal_attention_mask_with_cache_position(\n        attention_mask: torch.Tensor,\n        sequence_length: int,\n        target_length: int,\n        dtype: torch.dtype,\n        device: torch.device,\n        cache_position: torch.Tensor,\n        batch_size: int,\n        config: MistralConfig,\n        past_key_values: Cache,\n    ):\n    ...\nHere is LlamaModel:\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Cache] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n    ) -&gt; Union[Tuple, BaseModelOutputWithPast]:\n    ...\nSo, MistralModel overwrites LLamaModel’s deep layers with its own\nHere is MistralDecoderLayer:\nclass MistralDecoderLayer(LlamaDecoderLayer):\n    def __init__(self, config: MistralConfig, layer_idx: int):\n        super().__init__(config, layer_idx)\n        self.self_attn = MistralAttention(config=config, layer_idx=layer_idx)\n        self.mlp = MistralMLP(config)\n\nlen(model.model.layers)\n\n24\n\n\n\nmodel.model.layers[0]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\nBtw, here are the model sizes that work on Google Collab:\n\nQwen/Qwen2.5-0.5B-Instruct\nQwen/Qwen2.5-1.5B-Instruct\nQwen/Qwen2.5-3B-Instruct\nQwen/Qwen2.5-7B-Instruct (13.2 GB)\n\n\nl = model.model.layers[0]\n\n\nl.self_attn\n\nQwen2SdpaAttention(\n  (q_proj): Linear(in_features=896, out_features=896, bias=True)\n  (k_proj): Linear(in_features=896, out_features=128, bias=True)\n  (v_proj): Linear(in_features=896, out_features=128, bias=True)\n  (o_proj): Linear(in_features=896, out_features=896, bias=False)\n  (rotary_emb): Qwen2RotaryEmbedding()\n)\n\n\nHere is how we can extract query projection weight:\n\nl.self_attn.q_proj.weight\n\nParameter containing:\ntensor([[-0.0019, -0.0052,  0.0188,  ..., -0.0061, -0.0153,  0.0038],\n        [ 0.0084,  0.0018,  0.0435,  ...,  0.0066, -0.0422, -0.0181],\n        [-0.0168, -0.0248,  0.0422,  ...,  0.0089, -0.0008, -0.0094],\n        ...,\n        [-0.1040,  0.0791,  0.0132,  ..., -0.0161, -0.0221, -0.0588],\n        [-0.0140,  0.0654,  0.0591,  ...,  0.0410, -0.0046,  0.0025],\n        [ 0.0215,  0.0625,  0.0635,  ..., -0.0036, -0.0354, -0.0957]],\n       device='cuda:0', dtype=torch.bfloat16, requires_grad=True)\n\n\nQuestion: What is o_proj?\n\nl.mlp\n\nQwen2MLP(\n  (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n  (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n  (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n  (act_fn): SiLU()\n)",
    "crumbs": [
      "transformers",
      "Transformers"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "token_embeddings.html",
    "href": "token_embeddings.html",
    "title": "LLM token embeddings",
    "section": "",
    "text": "#from transformers import pipeline\n# Doesn't produce any content (probably using incorrectly)\n#pipe = pipeline(\"text-generation\", model=\"nroggendorff/smallama-it\")\n#pipe = pipeline(\"text-generation\", model=\"KingNish/Qwen2.5-0.5b-Test-ft\")\ndef clean_memory():\n    with torch.no_grad():\n        torch.cuda.empty_cache()\n    gc.collect()\nFor our experiments let’s grab a small language model. In this case, I will use Qwen2.5-0.5B. It fits in just about 1GB of VRAM, so it’s great for experimentation!\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n2025-02-25 01:22:48.553363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1740475368.575449   32363 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1740475368.582244   32363 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#generate-text",
    "href": "token_embeddings.html#generate-text",
    "title": "LLM token embeddings",
    "section": "Generate text",
    "text": "Generate text\nEven though this model is small, it can still generate coherent text:\n\ndef demo(max_new_tokens=2048,prompt=None):\n    if not prompt:\n        prompt = \"Write an introduction for a blog post about large language model token embeddings. Tags: PCA, layer sizes, nearest neighbours, KDTree, singular values.\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an autocorrelating model. You use previous outputs for reasoning. Pay attention to tags.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n    \n    generated_ids = model.generate(\n        **model_inputs,\n        max_new_tokens=max_new_tokens\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n    \n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    \n    display_markdown(response, raw=True)\n\n\ndemo()\n\nWelcome to our blog where we explore the fascinating world of large language models and their intricate relationship with neural networks, specifically focusing on token embeddings. As you delve deeper into this topic, you’ll uncover how these mathematical constructs play a crucial role in understanding complex natural language processing tasks.\nIn today’s era of artificial intelligence, large language models have emerged as indispensable tools capable of generating human-like text. However, they often struggle with handling high-dimensional data due to their computational demands. One solution that has proven effective is token embedding, which converts words into numerical representations using vectors. This technique significantly enhances the performance of language models by reducing the dimensionality of input data.\nOne method used to achieve this is through the application of Principal Component Analysis (PCA) to reduce the complexity of the data while preserving most of its variance. By analyzing the first few principal components, we can effectively represent the underlying structure of the data. This approach is particularly useful when dealing with large datasets or when the number of features exceeds the capacity of traditional methods like k-nearest neighbors or KD-trees.\nAnother powerful technique is utilizing the concept of layer sizes in deep learning. Understanding the architecture of a network helps us comprehend how information flows between layers and what kind of patterns emerge at each stage. By examining the learned weights and activations from different layers, we gain insights into the function of each component in the neural network.\nLastly, we delve into the realm of Singular Value Decomposition (SVD), which is employed to extract the essential features of the dataset. SVD decomposes the input matrix into three parts: the left singular vectors, the right singular vectors, and the diagonal matrix containing the singular values. These components allow us to visualize the importance of various features in the context of the entire dataset.\nUnderstanding these techniques and their interplay within the framework of large language models will provide you with valuable insights into the complexities involved in training and deploying such models. Whether you’re a seasoned AI researcher or just starting out, exploring these concepts can lead to significant advancements in your work. Stay tuned for more updates and stay curious!\n\n\n\ndemo()\n\nWelcome to our blog post exploring the fascinating world of large language models and their groundbreaking advancements in text generation! In this article, we delve into the complex world of token embeddings, which play a crucial role in understanding how these models process and generate human-like language. We’ll dive deep into the intricacies of PCA (Principal Component Analysis), layer sizes, nearest neighbors, KDTree (K-D Tree) implementation, and singular values, providing a comprehensive overview of the topic.\nFrom understanding the importance of token embeddings in natural language processing to mastering advanced techniques like nearest neighbors and KDTree implementations, you’ll discover not only the technical aspects but also the practical applications that these methods have revolutionized. Whether you’re a seasoned data scientist or just starting out in the field, this blog post promises to be an enlightening journey through the depths of machine learning and natural language processing. So grab your notebook and get ready to explore the next frontier in AI technology!\n\n\n\ndemo()\n\nIn today’s digital age, the role of large language models (LLMs) in various fields has become increasingly prominent. These models, particularly those trained on large datasets, have generated significant advancements across several domains such as natural language processing (NLP), speech recognition, and more recently, text generation. However, one aspect that has not been fully explored is the representation of input data within these LLMs—a critical yet often overlooked step in their training process.\nLarge language models rely heavily on understanding the context of inputs they encounter during their training phase. To achieve this, they employ embedding layers to convert textual data into numerical vectors. This conversion involves capturing both the meaning of the words themselves as well as contextual information that might be present. The goal is to map the raw text data into a higher-dimensional space where it can be effectively analyzed by the model. This process is known as tokenization or vectorization.\nPCA (Principal Component Analysis) plays a pivotal role in this process because it helps identify the most important features in the dataset. By analyzing the variance captured by each principal component, we can understand which aspects of the input data are most relevant for the model’s performance. Layer sizes, specifically the number of hidden units in the neural networks, significantly influence the complexity and dimensionality of the embedding space. The choice between dense and sparse layers depends on the nature of the task at hand; dense layers are used when there is ample context available, whereas sparse layers are preferred when the context is sparse.\nNearest Neighbors (NN) techniques are essential for learning embeddings based on proximity to other tokens. KDTree (K-d Tree) algorithms efficiently store and query point sets, making them ideal for spatially distributed data like text documents. The selection of optimal parameters for these algorithms, including tree depth and leaf size, is crucial for achieving effective token embeddings.\nSingular Value Decomposition (SVD) is another powerful tool for understanding the structure of the input data. It decomposes the matrix formed by the embedding vectors into three matrices—U, Σ, and V^T—and provides insights into how much of the total variance is explained by each feature. This decomposition allows us to understand the relative importance of different dimensions in the embedding space.\nUnderstanding these components—PCA, layer sizes, nearest neighbors, KDTree, and SVD—can greatly enhance our comprehension of how LLMs handle and represent input data. This knowledge will be invaluable in optimizing the training process, improving model performance, and leveraging the full potential of these powerful tools.",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#llm-parameters",
    "href": "token_embeddings.html#llm-parameters",
    "title": "LLM token embeddings",
    "section": "LLM Parameters",
    "text": "LLM Parameters\nAn LLM is made from an embedding layer, followed by transformer layers. Let’s look at their sizes:\n\nsizes = list(map(lambda x: (np.prod(x.shape), x.shape), model.model.parameters()))\nsizes[:10]\n\n[(136134656, torch.Size([151936, 896])),\n (802816, torch.Size([896, 896])),\n (896, torch.Size([896])),\n (114688, torch.Size([128, 896])),\n (128, torch.Size([128])),\n (114688, torch.Size([128, 896])),\n (128, torch.Size([128])),\n (802816, torch.Size([896, 896])),\n (4358144, torch.Size([4864, 896])),\n (4358144, torch.Size([4864, 896]))]\n\n\nSize of the embedding layer seems to dominate model size for this particular model:\n\ndf = pd.DataFrame(sizes, columns=['size', 'shape'])\ndf.iloc[:40].plot(y = 'size', kind='bar')\n\n\n\n\n\n\n\n\nIn fact, let’s compare size of embeddings with the rest of the model parameters:\n\ndf.loc[0,'size']/df.loc[:, 'size'].sum()\n\n0.275557948415276\n\n\nSo, 28% of the model is in the embeddings.",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#pca",
    "href": "token_embeddings.html#pca",
    "title": "LLM token embeddings",
    "section": "PCA",
    "text": "PCA\nNow let’s look at embeddings in more detail\n\nemb = model.get_input_embeddings()\nemb_w = emb.weight\nemb_w.shape\n\ntorch.Size([151936, 896])\n\n\nWe have about 150k tokens, and we use 896 dimensions to encode them. Let’s see if there is any visible structure among them.\n\nw = emb_w.cpu().detach()\nw = w.type(torch.Tensor)\n\nNumbers, characters, symbols seem to be nicely grouped together:\n\nw_pca = w.pca(3)\n\n\nw_pca.shape\n\ntorch.Size([151936, 3])\n\n\n\nfac0,fac1,fac2 = w_pca.t()[:3]\np = 0\nidxs = list(range(p,p + 100))\nX = fac0[idxs]\nY = fac2[idxs]\nplt.figure(figsize=(12,12))\nplt.scatter(X, Y)\ntoken_labels = [tokenizer.decode([i]) for i in idxs]\nfor i, x, y in zip(token_labels, X, Y):\n    plt.text(x,y,i, color=np.random.rand(3)*0.7, fontsize=14)\n#plt.show()\n#plt.savefig('token_encodings.png')",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#understanding-token-embeddings",
    "href": "token_embeddings.html#understanding-token-embeddings",
    "title": "LLM token embeddings",
    "section": "Understanding token embeddings",
    "text": "Understanding token embeddings\nNow let’s check if there are any tokens that have very similar encodings.\n\ntoken_labels = L([tokenizer.decode([i]) for i in range(w.shape[0])])\ntoken_labels\n\n(#151936) ['!','\"','#','$','%','&',\"'\",'(',')','*','+',',','-','.','/','0','1','2','3','4'...]\n\n\n\nw.T.shape\n\ntorch.Size([896, 151936])\n\n\n\ndf = pd.DataFrame(w.T, columns=token_labels)\n\nHere is a correlation matrix of first 500 tokens. Notice that there is a suspicious cluster of tokens with large correlation.\n\nplt.matshow(df.iloc[:,:500].corr())\n\n\n\n\n\n\n\n\n\ndf.iloc[:,124:130].corr()\n\n\n\n\n\n\n\n\n�\n�\n�\n�\n�\n�\n\n\n\n\n�\n1.000000\n1.000000\n0.036561\n0.949049\n0.999370\n0.613330\n\n\n�\n1.000000\n1.000000\n0.036553\n0.949040\n0.999370\n0.613357\n\n\n�\n0.036561\n0.036553\n1.000000\n0.039240\n0.036404\n0.066037\n\n\n�\n0.949049\n0.949040\n0.039240\n1.000000\n0.948284\n0.574783\n\n\n�\n0.999370\n0.999370\n0.036404\n0.948284\n1.000000\n0.615100\n\n\n�\n0.613330\n0.613357\n0.066037\n0.574783\n0.615100\n1.000000\n\n\n\n\n\n\n\n\nplt.matshow(df.iloc[:,124:225].corr())\n\n\n\n\n\n\n\n\n\nplt.matshow(df.iloc[:,177:190].corr())\n\n\n\n\n\n\n\n\n\ndf.iloc[:,177:190].corr()\n\n\n\n\n\n\n\n\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n�\n\n\u0001\n\n\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286441\n0.324009\n\n\n�\n1.000000\n1.000000\n0.999999\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286423\n0.324003\n\n\n�\n1.000000\n0.999999\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286415\n0.323999\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286428\n0.324013\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286434\n0.324009\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286445\n0.324002\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286447\n0.324016\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286421\n0.323967\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286428\n0.323993\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286425\n0.324003\n\n\n�\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.286441\n0.324025\n\n\n\n0.286441\n0.286423\n0.286415\n0.286428\n0.286434\n0.286445\n0.286447\n0.286421\n0.286428\n0.286425\n0.286441\n1.000000\n0.196765\n\n\n\u0001\n0.324009\n0.324003\n0.323999\n0.324013\n0.324009\n0.324002\n0.324016\n0.323967\n0.323993\n0.324003\n0.324025\n0.196765\n1.000000\n\n\n\n\n\n\n\nThese have almost perfect correlation. But these seem to correspond to some special byte sequences. Perhaps this has something to do with unicode?\nWe can also put token embeddings into a dataframe, and join it together with token labels sequence for convenience:\n\ndf = pd.DataFrame(w)\n\ndf['token'] = token_labels\ndf = df.reindex(columns=pd.Index([df.columns[-1]] + list(df.columns[:-1])))\n\ndf\n\n\n\n\n\n\n\n\ntoken\n0\n1\n2\n3\n4\n5\n6\n7\n8\n...\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n\n\n\n\n0\n!\n-0.010376\n0.040771\n0.009705\n0.000070\n-0.027100\n-0.002975\n-0.001160\n-0.019531\n0.028442\n...\n-0.008179\n0.016724\n0.022339\n-0.027222\n-0.029541\n-0.015381\n-0.007477\n0.009827\n0.013611\n-0.006683\n\n\n1\n\"\n-0.014587\n-0.001366\n-0.017700\n-0.002670\n0.003708\n-0.001495\n0.005402\n-0.010620\n0.017700\n...\n-0.013672\n-0.038330\n0.003433\n-0.007355\n-0.007172\n0.001152\n-0.005798\n-0.002441\n0.002441\n-0.008118\n\n\n2\n#\n-0.036621\n-0.010193\n0.007812\n-0.010925\n0.008057\n0.016724\n0.023438\n-0.030273\n-0.009399\n...\n-0.024048\n-0.002716\n0.006134\n-0.007538\n-0.008545\n0.008789\n0.011292\n-0.007355\n-0.017700\n-0.000675\n\n\n3\n$\n-0.009338\n-0.012085\n-0.015381\n0.010864\n0.003967\n-0.005981\n0.005707\n-0.006866\n0.014343\n...\n-0.047119\n0.007324\n0.007935\n-0.006531\n-0.019897\n-0.007812\n0.020874\n-0.002655\n0.015137\n-0.017822\n\n\n4\n%\n-0.009521\n0.004211\n0.006012\n-0.018433\n0.006409\n0.013428\n0.016846\n0.003372\n-0.000965\n...\n0.005798\n-0.010742\n-0.000957\n-0.024170\n-0.000463\n0.010620\n-0.003967\n0.003937\n-0.006195\n-0.007874\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n151931\n\n0.005951\n-0.005310\n0.003326\n0.002472\n-0.003067\n0.002563\n-0.009583\n0.015747\n-0.000580\n...\n0.003845\n0.016113\n-0.010071\n0.005066\n0.011353\n0.015320\n-0.014954\n-0.008179\n-0.008179\n0.018677\n\n\n151932\n\n0.005951\n-0.005310\n0.003326\n0.002472\n-0.003067\n0.002563\n-0.009583\n0.015747\n-0.000580\n...\n0.003845\n0.016113\n-0.010071\n0.005066\n0.011353\n0.015320\n-0.014954\n-0.008179\n-0.008179\n0.018677\n\n\n151933\n\n0.005951\n-0.005310\n0.003326\n0.002472\n-0.003067\n0.002563\n-0.009583\n0.015747\n-0.000580\n...\n0.003845\n0.016113\n-0.010071\n0.005066\n0.011353\n0.015320\n-0.014954\n-0.008179\n-0.008179\n0.018677\n\n\n151934\n\n0.005951\n-0.005310\n0.003326\n0.002472\n-0.003067\n0.002563\n-0.009583\n0.015747\n-0.000580\n...\n0.003845\n0.016113\n-0.010071\n0.005066\n0.011353\n0.015320\n-0.014954\n-0.008179\n-0.008179\n0.018677\n\n\n151935\n\n0.005951\n-0.005310\n0.003326\n0.002472\n-0.003067\n0.002563\n-0.009583\n0.015747\n-0.000580\n...\n0.003845\n0.016113\n-0.010071\n0.005066\n0.011353\n0.015320\n-0.014954\n-0.008179\n-0.008179\n0.018677\n\n\n\n\n151936 rows × 897 columns",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#kdtree",
    "href": "token_embeddings.html#kdtree",
    "title": "LLM token embeddings",
    "section": "KDTree",
    "text": "KDTree\nTo find nearest neighbours for sample query vectors, we can use KDTree.\nKDTree allows fast nearest neighbour lookups, but it only works fast up to 20 dimensions (according to docs).\n\nfrom scipy.spatial import KDTree\n\nSince KDTree is not good for high dimensions, we first reduce dimension to 10.\n\nw_pca = w.pca(10)\n\n\nw_pca.shape\n\ntorch.Size([151936, 10])\n\n\n\nm = KDTree(w_pca)\n\n\nsmall_distance = 0.001\n\nm.count_neighbors(m, small_distance)\n\nneighbours = m.query_pairs(small_distance, output_type='ndarray')\n\n\nlen(neighbours)\n\n2700502\n\n\nHere we find some tokens that are close to each other and compute their distances.\n\nfirst = 100\n(w[neighbours[:first,0]] - w[neighbours[:first,1]]).norm(dim=1)\n\ntensor([2.0965e-02, 1.2571e-02, 9.4396e-05, 5.1366e-05, 1.0017e-04, 1.1918e-04,\n        9.8044e-05, 6.4521e-05, 1.6839e-04, 8.5055e-05, 8.5407e-05, 1.1496e-04,\n        8.5661e-05, 9.6035e-05, 1.8153e-04, 1.0819e-04, 1.1803e-04, 1.0853e-04,\n        4.9831e-05, 1.7295e-04, 9.1673e-05, 2.3365e-05, 1.1710e-04, 1.6766e-04,\n        9.1910e-05, 1.2525e-04, 1.7944e-04, 1.1530e-04, 1.6751e-04, 1.6791e-04,\n        1.6262e-04, 1.6276e-04, 1.6613e-04, 1.6330e-04, 1.6705e-04, 1.6770e-04,\n        1.6613e-04, 1.8194e-04, 1.6276e-04, 1.7519e-04, 1.7631e-04, 1.7975e-04,\n        1.7582e-04, 1.8060e-04, 1.8088e-04, 1.7943e-04, 1.9296e-04, 1.7532e-04,\n        1.6906e-04, 1.6746e-04, 1.7243e-04, 1.6834e-04, 1.7197e-04, 1.7227e-04,\n        1.7074e-04, 1.8616e-04, 1.6781e-04, 1.6329e-04, 1.6200e-04, 1.6678e-04,\n        1.6255e-04, 1.6631e-04, 1.6556e-04, 1.6539e-04, 1.8126e-04, 1.6200e-04,\n        1.7570e-04, 1.7416e-04, 1.7895e-04, 1.7467e-04, 1.7719e-04, 1.7879e-04,\n        1.7732e-04, 1.9221e-04, 1.7416e-04, 1.6316e-04, 1.6186e-04, 1.6666e-04,\n        1.6241e-04, 1.6618e-04, 1.6681e-04, 1.6523e-04, 1.8112e-04, 1.6186e-04,\n        1.6951e-04, 1.6792e-04, 1.6741e-04, 1.6879e-04, 1.7242e-04, 1.7269e-04,\n        1.6564e-04, 1.8149e-04, 1.6827e-04, 5.3325e-05, 4.3203e-05, 2.7582e-05,\n        4.7724e-05, 5.9301e-05, 5.5018e-05, 2.7512e-05])\n\n\nUnfortunately, the fact that tokens are close to each other doesn’t mean that there is any connection between these tokens:\n\nL(zip(token_labels[neighbours[:first, 0]], token_labels[neighbours[:first, 1]]))\n\n(#100) [('궕', 'ᥔ'),('ᨪ', '궕'),('吏', ''),('&lt;|vision_end|&gt;', ''),('憐', ''),('', ''),('', ''),('閭', ''),('ด่วน', ''),('吏', '&lt;|vision_end|&gt;'),('憐', '吏'),('吏', ''),('吏', ''),('閭', '吏'),('ด่วน', '吏'),('憐', '&lt;|vision_end|&gt;'),('&lt;|vision_end|&gt;', ''),('&lt;|vision_end|&gt;', ''),('閭', '&lt;|vision_end|&gt;'),('ด่วน', '&lt;|vision_end|&gt;')...]\n\n\nSo, closest tokens are not a good measure of tokens being related to each other.\n\nts = tokenizer.encode('wet water')\n[(t, tokenizer.decode(t)) for t in ts]\n\n[(86, 'w'), (295, 'et'), (3015, ' water')]\n\n\n\nwater = 3015\nwater = w_pca[water]\ndist, items = m.query(water, k=20)\nL(tokenizer.decode([i]) for i in items)\n\n(#20) [' water',' bit',' access',' turn',' ×',' March',' exact',' target',' page',' different',' file',' defined',' essential',' Thomas',' unit',' count',' status',' functional',' quality',' zero']\n\n\nLets now try using all vectors from a sentence\n\ndef find_neighbours(query):\n    ts = tokenizer.encode(query)\n    v = w_pca[ts].sum(dim=0)\n    dist, items = m.query(v, k=20)\n    return L(tokenizer.decode([i]) for i in items)\n\n\nfind_neighbours(' lava')\n\n(#20) [' lava','太阳能','董事会',' stimuli',' fungal',' embody',' daring',' audience',' hazardous',' tumor',' vascular',' damping',' soothing',' qualifies',' addiction','纷纷',' herbal',' melan',' стран','答应']\n\n\n\nfind_neighbours(' magma')\n\n(#20) [' magma','事迹','周恩来','蒋介石','あたり','应收账款','规程','秦国',' serotonin','份额',' Jain','長期',' dopamine',' scalp','理财产品',' cereal',' disciplines',' словам','班组',' мероприятия']\n\n\n\nfind_neighbours('lava')\n\n(#20) ['l','m','c','d','a','f','A','B','p','9','b','7','x','6','C','8','v','h','s','D']\n\n\n\nfind_neighbours('lava fire hot')\n\n(#20) [',','1','2',' (','.','3',':',' ','-','4','0','/',' -','5','6',' \"','8',' and','7',' in']\n\n\nDoesn’t seem like there’s much structure here!\nBut here is the structure I was able to find:\n\nfind_neighbours('1')\n\n(#20) ['1','2','3','5','4','0','6','8','7','9','/',':','.','-','(',',','[','_',';',\"'\"]\n\n\n\nfind_neighbours('A')\n\n(#20) ['A','S','B','C','s','D','a','l','i','N','m','M','P','n','d','T','E','I','x','R']\n\n\n\nfind_neighbours('$')\n\n(#20) ['$','{','&gt;','%','|','&','//','#','`','}','&lt;','(\"','^','L','--','I','T','+','::','R']\n\n\nDoes this work for anything else?\nLook, countries sort of work!\n\nfind_neighbours(' Ontario')\n\n(#20) [' Ontario','实时',' Wisconsin',' Alaska',' Oregon',' Potter',' Seattle',' Alabama',' Maine',' Michigan',' superficial',' friction',' Detroit',' Iran',' Portland','.py',' Philadelphia',' rigor',' Pennsylvania','相当于']\n\n\nClass and unit are related:\n\nfind_neighbours(' Class')\n\n(#20) [' Class',' unit',' sign',' black',' Group',' change',' Type',' path',' level',' power',' deep',' direct',' line',' split',' fast',' count',' order',' block',' DO',' value']\n\n\n\nfind_neighbours('**')\n\n(#20) ['**','..','---','--','`','}',']','~','…','____','”','&gt;',' ?','—','\\\\\\\\','(\"',' &gt;','()','%','\\u200b']",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#singular-values",
    "href": "token_embeddings.html#singular-values",
    "title": "LLM token embeddings",
    "section": "Singular values",
    "text": "Singular values\nYet another way to understand a matrix is to look at its singular values and their distirbution.\n\nclean_memory()\n\n\nU, S, Vh = torch.linalg.svd(w, full_matrices=False)\n\n\nU.shape, S.shape, Vh.shape\n\n(torch.Size([151936, 896]), torch.Size([896]), torch.Size([896, 896]))\n\n\n\nS[:70]\n\ntensor([75.2048, 15.9967, 13.2013, 10.6195, 10.5224,  9.5778,  9.1092,  8.6585,\n         8.2998,  8.1300,  7.9553,  7.7216,  7.5018,  7.3106,  7.2360,  7.1848,\n         7.1072,  7.0573,  7.0249,  6.9709,  6.9112,  6.8638,  6.8472,  6.8007,\n         6.7806,  6.7488,  6.7466,  6.7255,  6.6886,  6.6821,  6.6695,  6.6449,\n         6.6378,  6.6109,  6.5966,  6.5856,  6.5781,  6.5602,  6.5523,  6.5365,\n         6.5208,  6.5109,  6.5013,  6.4817,  6.4740,  6.4708,  6.4575,  6.4540,\n         6.4452,  6.4395,  6.4335,  6.4130,  6.4117,  6.3935,  6.3885,  6.3838,\n         6.3800,  6.3771,  6.3733,  6.3621,  6.3533,  6.3416,  6.3364,  6.3318,\n         6.3283,  6.3232,  6.3199,  6.3130,  6.3038,  6.2935])\n\n\n\nplt.plot(S)\n\n\n\n\n\n\n\n\n\nplt.plot(S[10:])\n\n\n\n\n\n\n\n\nLooking at singular values, we see that they drop rapidly!\nThis should be surprising because when the model was made we started with fully random matrix.\nAnd random matrices have uniformly distributed singular values. This means that all embeddings lie very closely to a certain low-dimensional space.\nAbove we can see that singular values stop dropping rapidly at about 100 dimensions, so if we keep just 100 dimensions we might still be able to have a model that can produce sensible text.\nBut first, for comparison, lets look at singular values of a random matrix:\n\nw_random = torch.randn(w.shape)\n\n\nUr, Sr, Vhr = torch.linalg.svd(w_random, full_matrices=False)\n\n\nSr[:70]\n\ntensor([419.4922, 419.2200, 418.8347, 418.7828, 418.7401, 418.2579, 418.1684,\n        418.0732, 417.9306, 417.6548, 417.4839, 417.4085, 417.3276, 417.0378,\n        416.9029, 416.8173, 416.7499, 416.6445, 416.5141, 416.4569, 416.2802,\n        416.2378, 416.0545, 415.9872, 415.9027, 415.8290, 415.6523, 415.5988,\n        415.4861, 415.4289, 415.2998, 415.1790, 415.1625, 414.9554, 414.8152,\n        414.7798, 414.7103, 414.5463, 414.4521, 414.3818, 414.2459, 414.1892,\n        414.0259, 413.9031, 413.8279, 413.7903, 413.7162, 413.5286, 413.4558,\n        413.4163, 413.3558, 413.1771, 413.0956, 413.0775, 412.9968, 412.7909,\n        412.7854, 412.7141, 412.6904, 412.5693, 412.4755, 412.3453, 412.2935,\n        412.1939, 412.1080, 411.9935, 411.9718, 411.8588, 411.8069, 411.6788])\n\n\n\nplt.plot(Sr/420*70, label=\"Random\")\nplt.plot(S, label=\"Trained\")\nplt.legend()\n\n\n\n\n\n\n\n\nAs you can see, there is a huge difference between a random matrix and a matrix that has been optimized with backpropogation.\nNow, lets look at how SVD can be used to recover back the original matrix:\n\ntorch.dist(w, U @ torch.diag_embed(S) @ Vh)\n\ntensor(0.0006)\n\n\n\nw.norm()\n\ntensor(175.0522)\n\n\n\nSs = S.clone().detach()\nSs[400:] = 0\ntorch.dist(w, U @ torch.diag_embed(Ss) @ Vh)\n\ntensor(104.2088)\n\n\n\nw_simple = U @ torch.diag_embed(Ss) @ Vh\n\n\nw_simple\n\ntensor([[-0.0232,  0.0167,  0.0034,  ...,  0.0095,  0.0035, -0.0037],\n        [-0.0199,  0.0100, -0.0105,  ...,  0.0040,  0.0058, -0.0145],\n        [-0.0326, -0.0043,  0.0046,  ...,  0.0129, -0.0049, -0.0117],\n        ...,\n        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182],\n        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182],\n        [ 0.0058, -0.0052,  0.0030,  ..., -0.0077, -0.0083,  0.0182]])\n\n\n\nw\n\ntensor([[-0.0104,  0.0408,  0.0097,  ...,  0.0098,  0.0136, -0.0067],\n        [-0.0146, -0.0014, -0.0177,  ..., -0.0024,  0.0024, -0.0081],\n        [-0.0366, -0.0102,  0.0078,  ..., -0.0074, -0.0177, -0.0007],\n        ...,\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187]])\n\n\n\nemb_w.dtype\n\ntorch.bfloat16\n\n\n\nw_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n\n\nmodel.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n\n\ndef run_demo(s_vals):\n    Ss = S.clone().detach()\n    Ss[s_vals:] = 0\n    w_simple = U @ torch.diag_embed(Ss) @ Vh\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(128)\n\n200 singular values is too little, the model breaks completely:\n\nrun_demo(200)\n\ncompression error: 130.39418029785156\n\n\nConditioning\nLambdaizations\nаccessation с\ntionality\nLambdaization s аccessation с\ntionality\nLambdaization s аAccessExceptionationс\nLambdaization s аAccessExceptionationс\nLambdaization s аAccessExceptionationс\nLambdaizations\nLambdaization с\nLambdaizations\nLambdaization s аAccessExceptionationс\nLambdaization s аAccessExceptionationс\nLambdaization s аAccessExceptionationс\nLambdaization s аAccessExceptionationс\nLambdaizations Lambdaization s аAccessExceptionationс\nLambdaIZATION с\nLambdaications\nLambdaitations\nLambdaigation с\nLambdaigation с\nLambdaigations\n\n\n400 is a bit better, the model produces text that is somewhat coherent\n\nrun_demo(400)\n\ncompression error: 104.20878601074219\n\n\n�ake an prediction\npredicting the prediction with predictions. Bayesian inference\nBayesian inference is Bayesian inference’s Bayesian inference C Bayesian inference With prediction- Bayesian inference\nBayesian inference C Bayesian inference With prediction\nBayesian inference C Bayesian inference с Bayesian inference с Bayesian inference с Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inferenceс Bayesian inference\n\n\n\nrun_demo(400)\n\ncompression error: 104.20878601074219\n\n\nLarge language models like the token embeddings, PCA layers\nnearest neighbors and KD tree\nsingular values\n\n\nRemember that the original embedding dimension was 896:\n\nw.shape\n\ntorch.Size([151936, 896])\n\n\n\nrun_demo(600)\n\ncompression error: 74.72993469238281\n\n\nWrite an introduction an blog post about the token embeddings in PCA approach and layer sizes\nnearest neighbours approach\nKD tree approach\nsingular values approach\n\n\n\nrun_demo(800)\n\ncompression error: 34.6391487121582\n\n\nCertainly! Here’s a brief introduction to the a blog post about large language model token embeddings:\nToken embeddings are an essential component of natural language processing ( NLP) research areas. that enable models like LLMs to understand and process tokens in contextually. analyze text.\nLarge-scale language models like BERT or GPT- often require embedding layers to capture these contextual information effectively\nPCA ( Principal Component Analysis is a statistical technique used to reduce the dimensionality of extract important features from data\nlayer sizes and nearest neighbours are two popular approaches for identifying key features in token embeddings\nKD tree ( k-d tree is a efficient spatial indexing technique\n\n\n\nrun_demo(800)\n\ncompression error: 34.6391487121582\n\n\nSure! Let’s introduce the large language model (token embeddings and also talk about how we can extract these embeddings using PCA, layer sizes and nearest neighbors.\nPCA ( is a technique that can help us reduce dimensionality while retaining most ofer ofentities in a data set. PCA is a popular method in computer vision and image processing\nlayer sizes could be used to specify the number o f hidden layers in each layer size could be specified as the the number o f neurons in each layer The KNN (is a algorithm that could be used t obtain the nearest neighbors in a data set In machine learning t o predict new items based on\n\n\nNotice that LLM no longer seems to remember how to close paranthesis. That was probably lost in compression.\n\nrun_demo(890)\n\ncompression error: 4.259422302246094\n\n\nWelcome to our exploration of the fascinating world of large language models (LLMs) and their groundbreaking advancements in natural language processing (NLP). As we delve into the complex world of token embeddings, we’ll uncover some intriguing insights that shed light on how these powerful models learn and process text data.\nIn recent years, researchers have made significant strides in developing sophisticated methods for analyzing and understanding the intricate patterns hidden within vast amounts of textual data. One such method is token embedding, which has become one of the most widely used techniques in NLP due to its ability to capture semantic meaning across different contexts.\nBut what exactly does this mean? Let’s\n\n\nThis looks pretty decent.\n\nrun_demo(850)\n\ncompression error: 19.876100540161133\n\n\nExploring the Role of Large Language Model Token Embeddings in Natural Language Processing\nNatural language processing (NLP) is a field that focuses on enabling computers to understand and interact with humans in a human-like way. One crucial aspect of NLP is the understanding how text is structured and represented by humans. The process of tokenizing text into smaller units called tokens is one of the most fundamental steps in NLP. However, understanding how well this process works can be challenging, especially when dealing with complex languages or texts with very different vocabularies.\nThat’s where large language models come in - they are artificial intelligence systems designed to generate\n\n\n\nrun_demo(830)\n\ncompression error: 26.247896194458008\n\n\nIntroduction to Blog Post: “Token Embeddings in the Large Language Model\nIn recent years, the development of large language models (LLMs has seen explosive growth. and their capability to understand natural language text is one of the most significant changes. have happened. .\nOne key aspect e.g. that llms can handle is the complex structure e.g. those found in many natural languages e.g. english. french. spanish. chinese. etc. they need to be able to understand how words relate to each other e.g. how a word like “dog” is related to another word like “cat”.\nThis ability comes",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#adding-noise",
    "href": "token_embeddings.html#adding-noise",
    "title": "LLM token embeddings",
    "section": "Adding noise",
    "text": "Adding noise\nNow that we are on the topic of breaking LLMs, lets investigate what happens if we add a bit of noise to embeddings. In other words, how much noise can we add before LLM breaks completely?\n\nw\n\ntensor([[-0.0104,  0.0408,  0.0097,  ...,  0.0098,  0.0136, -0.0067],\n        [-0.0146, -0.0014, -0.0177,  ..., -0.0024,  0.0024, -0.0081],\n        [-0.0366, -0.0102,  0.0078,  ..., -0.0074, -0.0177, -0.0007],\n        ...,\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187],\n        [ 0.0060, -0.0053,  0.0033,  ..., -0.0082, -0.0082,  0.0187]])\n\n\n\nw.std()\n\ntensor(0.0152)\n\n\n\ndef run_demo(noise, max_new_tokens=128):\n    w_simple = w + noise*torch.randn(w.shape)*w.std()\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens)\n\n\nrun_demo(0)\n\ncompression error: 0.0\n\n\nIntroduction\nIn the realm of natural language processing and machine learning, understanding the structure of text is crucial for developing effective models that can comprehend complex language tasks. One key aspect of this complexity is the representation of text data using tokens, which are fundamental units of information within the language. The tokenization process involves breaking down continuous text into discrete segments or words, which are then used as input features in various models.\nOne approach to represent these tokens is through embedding layers, where each token’s position is mapped to a vector space. This mapping allows for efficient computation of the dot product between vectors representing different tokens, enabling the modeling of semantic relationships\n\n\n\nrun_demo(0.01)\n\ncompression error: 1.7495441436767578\n\n\nWelcome to our exploration of the fascinating world of large language models and their groundbreaking advancements in natural language processing! In this blog post, we’ll delve into the intricate world of token embeddings, where the foundation of language understanding is laid bare. From the simple layers that capture word frequencies to the complex algorithms that sift through the data to reveal patterns, we’ll unravel the secrets behind these essential components.\nFirstly, let’s explore the role of PCA (Principal Component Analysis) in dimensionality reduction. By visualizing your text data using a 2D PCA plot, you can identify which words or phrases are most indicative of different topics. This technique\n\n\n\nrun_demo(0.1)\n\ncompression error: 17.48781967163086\n\n\nIntroduction to Large Language Model Token Embeddings\nIn recent years, the integration of large language models (LLMs) into various applications has dramatically transformed the way we interact with technology and information. One of the key innovations in this space is the development of token embeddings, which serve as a powerful tool for understanding the structure and semantic relationships within textual data.\nToken embeddings are essentially vectors that represent each word or sequence of words in a document using numerical representations. This transformation allows us to capture not only the meaning of individual words but also their contextual relationships across the entire text. By analyzing these embeddings at different layers, researchers can gain insights into the\n\n\n\nrun_demo(1)\n\ncompression error: 175.2798309326172\n\n\nIntroduction\nThe field of Natural Language Processing (NLP) is increasingly critical in the rapidly-evolving landscape of AI research and technology. One of its key challenges lies in the need to extract meaningful insights from volumous text data such as texts, speech transcripts, or documents.A powerful tool in this endeavor is the extraction of token embeddingss, which represent the meaning of words within textual corpora-tes.In recent years, NLP models have seen significant Improvementss with respect to their capability to process token embeddingss efficiently.\nThis blog post will explore various aspects related to token embeddingss, including their principal characteristics, essential dimensions\n\n\nHuh? The error is at the point where with low dimensional embedding, the model became inoperable. But here, it still produces something reasonable.\n\nrun_demo(10)\n\ncompression error: 1748.9488525390625\n\n\ns c ?1 & |..B- .. n.io. …G.B !..F B ALL al M AN ?. N DOAD/( unet etc of; upon] FOR by and]+ the G);) i et R #| ren.AE-& NE RE AB. r uin U& | CAP at. its OF w TO ofG & .? RE rem’s volf Ben aga on&gt;. ! -’?.. in. p which-d\n\n\n\nrun_demo(5)\n\ncompression error: 874.4374389648438\n\n\nentmssIo\nnd ust mC.Enttn t n- Sment\nCis Ent usiationng\neric n– Car InchRht C ch- Ung nenc. Mepsmemus in m cm ein me mpng mo pent n enment tionn CA-CT R AS-e C i C.- NmesnessEmee E-m-fnere- P mor C Smein.-co r-me.-e p H- Fent G Me Ch.- ce Mornumee\n\n\n\nrun_demo(4)\n\ncompression error: 701.1428833007812\n\n\np E E An C A R E B FanE n p E An M A F E A EF K Hs0\ng- A R G A Srt t R c of-t An t’s def FO k Kou K s) k o’K F O in p Z N K E F1 R K P K-Ko AF D K-F F FP1 Tlaont Ak amdth A K FRA K F Fum O A R GAP E O–F i R a a B B F A3 A GR- K KR r F FAna\n\n\n\nrun_demo(3)\n\ncompression error: 525.580322265625\n\n\nP P Inere\nBo iner ch boer Recar ar Ar C acser arc erRec Are c re re neRe cre- ng n f B nd near fo AR-Ce fr-Chn pee Per ne Rep ref-ref neRef mo fo Ref rem For m neOrure fr Ph Por prf fo ne ph ne rep near of f rec n ne ref r neAcr neC ref N r neRef neT ne fne f ne refNe ne ne reN ne ne r r neRef ne ne Tm-nT t ne refN-n n o\n\n\n\nrun_demo(2)\n\ncompression error: 350.5704345703125\n\n\nsai s b\nt Sus\ns usut Cu\nsuc\nS u t\ns ut\nS t\nS Us t\nS ut\ns Ut t\ns U t\nsUt\ns Uut\nsUU\nsue\ns UE\ns UEU\nsues\ns r\nsu\ns U\ns U t\ns UUT\nsUt\ns u\nsue\ns UE\nsUE\ns U e\ns UE\nsu e\ns UE\nsu e\nsue\ns ue\nsUE\ns\n\n\n\nrun_demo(1)\n\ncompression error: 175.27740478515625\n\n\nIntroduction pour um\nI’m going to write an introductory para um sobre the embedding of the deep learning models, particularly on the “token” embedding. I’ll be using some keyphs and tags.\nTags:\n\nPolarity - A popular Model in the field of NLP.\nLayer Sizes - The number of layers used by the model.\n** Nearest Bunches** - The number of nearest neighbors used by the model.\n** Kd Tree** - A method that uses the k-d tree to find the most frequent word.\n**\n\n\n\nHaha, this model is drunk\n\nrun_demo(0.5, 512)\n\ncompression error: 87.64675903320312\n\n\nIntroducing the captivating world of Large Language Models (LLMs) with their unique ability to process vast amounts of text data with unparalleled efficiency and precision. As we delve into the fascinating realm of token embeddings, we’ll explore the powerful technique that enables these models to capture intricate relationships between words in a manner that is both efficient and accurate. Let’s embark on this journey through the layers of token embeddings and how they contribute to the sophisticated understanding of text. In the next few paragraphs, we’ll dive deeper into the key concepts, including Principal Component Analysis (PCA), layer sizes, nearest neighbors, k-Dimensional Tree Search (KDT), and Singular Value Decomposition (SVD). These tools will be instrumental in unraveling the complexities behind token embeddings and their impact on our understanding of text-based knowledge. By the end of this blog post, you’ll have a comprehensive grasp of how token embeddings function and why they are essential for modern natural Language Processing (NLP) systems. Don’t miss out on this fascinating exploration!\n\n\nSo, we can see that the model is resistant to noise, as long as this noise is not biased.\nIn particular, it doesn’t like “most descriptive subspace” projection type noise.\nSo, if we want to reduce dimension, we should reduce it in such a way that there is no biased direction.\nFortunately, there is a way to achive that using a random projection.\n\nw.shape\n\ntorch.Size([151936, 896])\n\n\nWe generate a random matrix, and multiply our encoding matrix by it to get an encoding matrix of smaller size.\n\nlow_dim = 200\nprojection = torch.randn((w.shape[1], low_dim))\n# debug\n#projection = torch.randn((low_dim, low_dim))\nprojection.shape\n\ntorch.Size([896, 200])\n\n\n\n(w @ projection).shape\n\ntorch.Size([151936, 200])\n\n\nTo get back to the original space, we need to invert our projection. Of course, this is not an invertible matrix, so we use pseudoinverse.\n\ntorch.linalg.pinv?\n\n\nDocstring:\nlinalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None) -&gt; Tensor\nComputes the pseudoinverse (Moore-Penrose inverse) of a matrix.\nThe pseudoinverse may be `defined algebraically`_\nbut it is more computationally convenient to understand it `through the SVD`_\nSupports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if :attr:`A` is a batch of matrices then\nthe output has the same batch dimensions.\nIf :attr:`hermitian`\\ `= True`, :attr:`A` is assumed to be Hermitian if complex or\nsymmetric if real, but this is not checked internally. Instead, just the lower\ntriangular part of the matrix is used in the computations.\nThe singular values (or the norm of the eigenvalues when :attr:`hermitian`\\ `= True`)\nthat are below :math:`\\max(\\text{atol}, \\sigma_1 \\cdot \\text{rtol})` threshold are\ntreated as zero and discarded in the computation,\nwhere :math:`\\sigma_1` is the largest singular value (or eigenvalue).\nIf :attr:`rtol` is not specified and :attr:`A` is a matrix of dimensions `(m, n)`,\nthe relative tolerance is set to be :math:`\\text{rtol} = \\max(m, n) \\varepsilon`\nand :math:`\\varepsilon` is the epsilon value for the dtype of :attr:`A` (see :class:`.finfo`).\nIf :attr:`rtol` is not specified and :attr:`atol` is specified to be larger than zero then\n:attr:`rtol` is set to zero.\nIf :attr:`atol` or :attr:`rtol` is a :class:`torch.Tensor`, its shape must be broadcastable to that\nof the singular values of :attr:`A` as returned by :func:`torch.linalg.svd`.\n.. note:: This function uses :func:`torch.linalg.svd` if :attr:`hermitian`\\ `= False` and\n          :func:`torch.linalg.eigh` if :attr:`hermitian`\\ `= True`.\n          For CUDA inputs, this function synchronizes that device with the CPU.\n.. note::\n    Consider using :func:`torch.linalg.lstsq` if possible for multiplying a matrix on the left by\n    the pseudoinverse, as::\n        torch.linalg.lstsq(A, B).solution == A.pinv() @ B\n    It is always preferred to use :func:`~lstsq` when possible, as it is faster and more\n    numerically stable than computing the pseudoinverse explicitly.\n.. note::\n    This function has NumPy compatible variant `linalg.pinv(A, rcond, hermitian=False)`.\n    However, use of the positional argument :attr:`rcond` is deprecated in favor of :attr:`rtol`.\n.. warning::\n    This function uses internally :func:`torch.linalg.svd` (or :func:`torch.linalg.eigh`\n    when :attr:`hermitian`\\ `= True`), so its derivative has the same problems as those of these\n    functions. See the warnings in :func:`torch.linalg.svd` and :func:`torch.linalg.eigh` for\n    more details.\n.. seealso::\n        :func:`torch.linalg.inv` computes the inverse of a square matrix.\n        :func:`torch.linalg.lstsq` computes :attr:`A`\\ `.pinv() @ \\ `:attr:`B` with a\n        numerically stable algorithm.\nArgs:\n    A (Tensor): tensor of shape `(*, m, n)` where `*` is zero or more batch dimensions.\n    rcond (float, Tensor, optional): [NumPy Compat]. Alias for :attr:`rtol`. Default: `None`.\nKeyword args:\n    atol (float, Tensor, optional): the absolute tolerance value. When `None` it's considered to be zero.\n                                    Default: `None`.\n    rtol (float, Tensor, optional): the relative tolerance value. See above for the value it takes when `None`.\n                                    Default: `None`.\n    hermitian(bool, optional): indicates whether :attr:`A` is Hermitian if complex\n                               or symmetric if real. Default: `False`.\n    out (Tensor, optional): output tensor. Ignored if `None`. Default: `None`.\nExamples::\n    &gt;&gt;&gt; A = torch.randn(3, 5)\n    &gt;&gt;&gt; A\n    tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n            [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n            [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n    &gt;&gt;&gt; torch.linalg.pinv(A)\n    tensor([[ 0.0600, -0.1933, -0.2090],\n            [-0.0903, -0.0817, -0.4752],\n            [-0.7124, -0.1631, -0.2272],\n            [ 0.1356,  0.3933, -0.5023],\n            [-0.0308, -0.1725, -0.5216]])\n    &gt;&gt;&gt; A = torch.randn(2, 6, 3)\n    &gt;&gt;&gt; Apinv = torch.linalg.pinv(A)\n    &gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))\n    tensor(8.5633e-07)\n    &gt;&gt;&gt; A = torch.randn(3, 3, dtype=torch.complex64)\n    &gt;&gt;&gt; A = A + A.T.conj()  # creates a Hermitian matrix\n    &gt;&gt;&gt; Apinv = torch.linalg.pinv(A, hermitian=True)\n    &gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))\n    tensor(1.0830e-06)\n.. _defined algebraically:\n    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Existence_and_uniqueness\n.. _through the SVD:\n    https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)\nType:      builtin_function_or_method\n\n\n\n\ninverse = projection.pinverse()\ninverse.shape\n\ntorch.Size([200, 896])\n\n\n\n(projection @ inverse).shape, (inverse @ projection).shape\n\n(torch.Size([896, 896]), torch.Size([200, 200]))\n\n\n\nplt.imshow((projection @ inverse)[:100,:100])\n\n\n\n\n\n\n\n\n\n(w @ projection @ inverse).shape\n\ntorch.Size([151936, 896])\n\n\n\ndef run_demo(low_dim, max_new_tokens=128):\n    projection = torch.randn((w.shape[1], low_dim))\n    inverse = projection.pinverse()\n    w_simple = w @ projection @ inverse\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens)\n\n\nrun_demo(200)\n\ncompression error: 153.46058654785156\n\n\nARс AR А р а а А ра А р Aа А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А А р А\n\n\n\nrun_demo(300)\n\ncompression error: 142.6547088623047\n\n\nBel таг\n\n\n\nrun_demo(400)\n\ncompression error: 130.0930938720703\n\n\ns\nTocorrelating-model.\nにのるい てる �りug. たug-siugnt �. �anug. tug-gngnt. tigugnt. �augnt はりug. �ognt たug-signt. tigugnt.\n�aougnt �gugnt �augnt �gugnt. �aougnt �\n\n\n\nrun_demo(500)\n\ncompression error: 116.29911041259766\n\n\n###! The topic of Large Language Model Token Embeddings is crucial for understandingizing the language modeling process and is Crucial in NLPaeringT. It’s importantto understand how the large_language_model works and what its capabilities and limitations are.\nIn this 1st, I will discussLanguanguage Modeling过程中的Token Embeddings。 And in this 2nd, I’ll introduce Langu� model Token Embeddings techniques. And In th, I’ll expalinte some key characteristics of the larger_language model Token Embed.nts.\nThe Topic OF LARGE LANG MODEL TOKEN EMBED IS CRIOUS AND NLPa\n\n\n\nrun_demo(600)\n\ncompression error: 100.79779052734375\n\n\nTitle: Large Language Model Token Embeddings: An Introduction\nAbstract: This blog post introduces the concept of Large Language Model (LLM) and its core component—the Token Embeddings. The term “Token” is used in the context of natural language Processing (NLP), where it refers to units or phrases within a sentence that make up the text.\nIn this article, we’ll explore how these units can be represented as vectors with specific dimensions. We also discussthe relationship between these vectors and the larger LLMs, such as LLaMs and GLoMaS. Lastly, we will delve into how the embedding space\n\n\n\nrun_demo(700)\n\ncompression error: 82.64166259765625\n\n\nIntroduction:\nIn today’s blog post, we’ll dive into the fascinating world of token embeddings in natural language processing (NLP), with a focus on understanding how these embeddings can be generated and utilized. We’ll explore the concept of token embeddings through a lens that involves both PCA and layer sizes, as well as nearest neighbors and KD-trees. Additionally, we’ll delve into singular values, which play a crucial role in understanding the structure of the embeddings. Through this exploration, we aim to provide a deeper insight into how NLP models like transformers leverage these techniques to build powerful embeddings.\n\n\n\nrun_demo(800)\n\ncompression error: 56.02879333496094\n\n\nIntroduction\nIn today’s digital age, the development of large language models has revolutionized various fields such as natural language processing, machine learning, and more. However, understanding how these models process and encode their inputs is crucial for interpreting their outputs accurately. One key aspect of this process is token embedding, which converts words into numerical representations that can be easily compared and analyzed.\nThis blog post delves into the world of token embeddings through an exploration of Principal Component Analysis (PCA), Layer Sizes, Nearest Neighbors, KD Trees, and Singular Values. By examining these concepts in detail, we aim to provide a comprehensive understanding of how token\n\n\nAt just 600 dimensions, the model is able to produce coherent text!\n\nrun_demo(600, 512)\n\ncompression error: 100.35061645507812\n\n\nThe Power of Large Language Models: Understanding Token Embeddings Through Dimensional Analysis Techniques\nAs we delve deeper into the complex landscape of natural language processing and machine learning, it’s crucial to understand how these models process information. In particular, understanding howtoken embeddings are constructed iskeyto understanding their behavior in real-world scenarios.\nIn this blogpost, I’ll provide insight on how dimensionalitys analysis techniques can be used to understand the distribution of token embeddings across various layers of a largelanguage model. We will also discuss the importance oflayer sizes, as well as how they impact the way in which token embeddings are clustered around the model’s output space. Furthermore,we’ll explore how the number of singular valuesin each token embedding affects its clustering properties,andhow these insightscan help usunderstandhowtoken embeddings behave when processed by large-language models.\nLet’s dive deep into the importance ofdimensionalityon the token embeddings and learn howto understand them through a dimensionalal approach. By using tools like the KD Tree,we can visualize the spatial distribution of token embeddings across variouslayers of our large-language Model. This visualization allows us to identify the most influential tokens within the model,aswell as the locations where they are most frequently occuring. Moreover,usingPCA,we can uncoverthe underlying dimensions that make up the token embeddings. Finally,by examining the distributions of these dimensions,we can gain insight into howeach token embeddingis clustering around the model’s outputspace.\nThis blogpost aims to offer a fundamental comprehension of the power of token embeddings and howthey interact with larger-scale modeling systems. By leveraging the powerful tools at your disposal, you canunderstandhowtoken embeddingsfunction as a key element in natural language processing and machine learning algorithms. Let’s get started!\n\n\n\nrun_demo(600, 2048)\n\ncompression error: 100.91890716552734\n\n\nSure! Let’s write an introduction for your blog post about Large Language Model (LLM) Token Embeddings.\n\nIntroduction\nIn recent years, the advent of Large Language Models (LLMs) has revolutionized how we interact with text and other forms of information. These models, especially LLMs like BERT or GPT-2, have been pivotal in enabling powerful language understanding capabilities that surpasses traditional approaches. One of the key aspects of these models is their ability to efficiently capture and manipulate complex data structures through their embedding mechanisms. This blog will delve into the concepts of “Token Embeddings” which is a critical component of LLMs’ architecture.\n###PCA, Layer Sizes, Nearest Neighbours, KD Tree, Singular Values\n\nPCA (Principal Component Analysis)\nPCA stands as one of the foundational tools in the realm of LLMs. It was developed by the research group at Stanford University. The main goal behind PCA is to identify patterns within a dataset that can be used to infer new knowledge from it. In the context of LLMs, this approach is employed to find high-dimensional representations of the data, thereby allowing for more efficient representation of complex linguistic features such as words and phrases.\n\n\nLayer Sizes\nThe number of layers in LLMs also plays a crucial role in capturing complex datastructures. Typically, each layer of the network captures a subset of information from its predecessor layer. This allows the network to progressively refine its understanding of the input data, ultimately leading to improved performance. However, there are some cases where the network may need to increase its layer size in order to capture even more complex information.\n\n\nNearest Neighbours\nNearest-neighbors refers to a technique that helps in identifying neighboring tokens (i.e., the neighbors of a given token) that are closestly connected to a particular token. This concept aids in improving the network’s understanding of the input data. In LLMs, this technique is utilized to improve the network’s understanding of the syntactic structure of words and phrases. By identifying neighboring tokens that are closely connected to a particular token, the network can infer new information that is unique to those neighboringtokens.\n\n\nKD Tree\nKD tree is another tool that is commonly employed in the context of LLMs. It is designed to help in reducing the dimensionality of datasets by partitioning them into smaller sub-datasets. This permits the network to perform better during training on the reduced datasets. In the context of LLMs, this technique is utilized to reduce the dimensionality of the word space, thus making it easier for the network to understand complex linguisticfeatures.\n\n\nSingular Values\nSingular values are another concept used in LLMs that allow the network to capture intricate relationships between words and phrases. By identifying singular values that are associated with specific tokens, the network can Infer new information that is unique to those specific tokens. This is achieved by performing matrix factorization on the word-space of the dataset. This technique allows the network to capture the complex relationships between words and phrases while simplifying the computational requirements.\n###Conclusion\nIn conclusion, the concepts of PCA, layer sizes, nearest-neighbors, KD Tree, and singular values are all essential components of LLMs. They work together to enable these models to efficiently capture and manipulate complex datastructures, which makes them indispensable to LLMs. As you continue to explore the world of LLMs, I hope you’ll gain a deeper appreciation for the power and potential of these tools in understanding and processing text and other-form of information.\n\n\n\n\nSo, it sounds like we can reduce 896 dimensions all the way down to 600 dimensions. Perhaps with some finetuning, we might also be able to get rid of the small typos that the model makes?",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#why-does-random-projection-work",
    "href": "token_embeddings.html#why-does-random-projection-work",
    "title": "LLM token embeddings",
    "section": "Why does random projection work?",
    "text": "Why does random projection work?\nFirst of all, why is the model resistant to noise?\nNotice that the attention mechanism takes weighted averages of token vectors, and hence any noise added to token vectors would be reduced automatically.\nNow, why does random subspace projection behave as adding noise?\nLets take a random projection, and then go back to the original space using the pseudoinverse, and look what the overall resulting operation looks like:\n\nlow_dim = 600\nprojection = torch.randn((w.shape[1], low_dim))\ninverse = projection.pinverse()\n\nop = projection @ inverse\n\nplt.imshow(op[:100,:100])\n\n\n\n\n\n\n\n\nAs you can see, it looks very close to a scaled identity matrix, plus some random noise in all other entries.\n\\(O = I*f + R\\)\n\nop.shape\n\ntorch.Size([896, 896])\n\n\nWhat is f equal to? We can compute this as\n\nop.diag().mean()\n\ntensor(0.6696)\n\n\n\nr = op - torch.eye(op.shape[0])*op.diag().mean()\n\nplt.imshow(r[:100,:100])\n\n\n\n\n\n\n\n\nSo, \\(R\\) looks like a random symmetric matrix.\n\nr[:5,:5]\n\ntensor([[ 0.0061,  0.0089, -0.0006,  0.0262,  0.0277],\n        [ 0.0089, -0.0070,  0.0072,  0.0084, -0.0182],\n        [-0.0006,  0.0072,  0.0121, -0.0263, -0.0122],\n        [ 0.0262,  0.0084, -0.0263,  0.0012,  0.0021],\n        [ 0.0277, -0.0182, -0.0122,  0.0022,  0.0156]])\n\n\n\nplt.hist(r.flatten())\n\n(array([1.18000e+02, 3.05200e+03, 3.17230e+04, 1.42686e+05, 2.78852e+05,\n        2.39637e+05, 9.05380e+04, 1.51170e+04, 1.06200e+03, 3.10000e+01]),\n array([-0.07127711, -0.0564779 , -0.0416787 , -0.02687949, -0.01208028,\n         0.00271892,  0.01751813,  0.03231734,  0.04711654,  0.06191575,\n         0.07671496]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nAnd entries are distributed like a normal distribution (presumably due to law of large numbers, and due to original projection being sampled from normal distribution).\nNotice that above we had identity scaled by a constant. That’s not nice. Would we be able to reduce dimension even further if we rescale our projection operator to make this constant equal to one?\nSo, where does 0.6696 come from?\nWell, if we had a square matrix for a projection, then we would have gotten one here.\nInstead, we have 896x600 matrix\n\n600/896\n\n0.6696428571428571\n\n\nWow, it matches so perfectly!\nIt’s surprising that the model still works even without this weighting on projection.\nSo, here’s a question: - How sensitive is the model to constant rescaling of all token embeddings by the same number?\n\ndef run_demo(scale, max_new_tokens=128):\n    w_simple = w * scale\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens)\n\n\nrun_demo(1)\n\ncompression error: 0.0\n\n\nWelcome to our blog post exploring the fascinating world of large language models and their powerful abilities to generate text at scale! Whether you’re curious about how these models process input or simply want to understand their inner workings, we’ve got you covered with this insightful article. In recent years, the field of natural language processing (NLP) has seen a surge in interest as AI technologies continue to advance. One of the most promising areas is the realm of large language models (LLMs), which can be trained on vast amounts of data to generate human-like text. These models have revolutionized various fields such as translation, summarization, and chatbots\n\n\n\nrun_demo(0.5)\n\ncompression error: 87.526123046875\n\n\nCertainly! Here’s an introduction for your blog post on “Large Language Model Token Embeddings”:\n\nThe field of natural language processing (NLP) has seen remarkable advancements with the advent of large language models (LLMs). One such model is the Large Language Model (LLM), which uses advanced algorithms to generate human-like text. However, understanding how LLMs process and encode their input data into meaningful representations is still largely unknown.\nIn this blog post, we will explore the concept of embedding tokens in LLMs, specifically focusing on PCA (Principal Component Analysis), layer sizes, nearest neighbors, KDTree (K-Dimensional Tree\n\n\n\nrun_demo(0.1)\n\ncompression error: 157.12701416015625\n\n\nqmultiparticot Palestinianicotumat CAMicot\nТicot Palestinianicot Maticot Palestiniansicot\nCamicot Palestinianicot ATMicot Palestinianicoticot Palestiniansicotumat CAMicot\nicoticot Palestinianicot Maticot Palestinianicotumat CAMicotultipart Palestinianicot ATMicot Palestinianicot ATMicot �icot Palestinianicoticot Palestinianicot ATMicot Palestinianicottat Palestinianicotimat Palestinianicot ATMicot Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicoticot Palestinianicot Tat Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicot Tat Palestinianicot OAicoticot Palestinianicot ATMicot Palestinianicot Tat Palestinianicot Tat Palestinianicot Tat Palestinianicot Tat\n\n\n\nrun_demo(2)\n\ncompression error: 175.05224609375\n\n\nWelcome to our exploration of the fascinating world of large language models’ token embeddings! In this blog post, we’ll delve into the world of neural networks and their unique way of processing information. Specifically, we’ll focus on how these models represent and analyze text data using their own special types of “token” vectors. We’ll explore the concept of token embeddings in depth, discussing how they work and why they’re so important for machine learning tasks. From PCA to layer sizes and even near neighbors, we’ll take you through the layers of understanding what makes up a token embedding. Plus, we’ll touch upon some advanced techniques like the K-D\n\n\n\nrun_demo(1.5)\n\ncompression error: 87.526123046875\n\n\nIntroducing the fascinating world of large language models and their unique ability to capture complex representations through their token embedding layers. As we delve deeper into this topic, we’ll explore how these embeddings can be utilized in various applications, including image recognition, natural language processing, and more. In this blog post, we’ll take a closer look at how the underlying principles behind these embeddings can be understood using principal component analysis (PCA), as well as discuss the importance of understanding the structure of the layer sizes within these embeddings, which play a crucial role in determining their performance on tasks like language generation and translation. Additionally, we’ll touch upon the concept of\n\n\n\nrun_demo(5)\n\ncompression error: 700.208984375\n\n\nIntroducing the significance of Large Language Model Token Embeddings in Machine Learning and Data Science with focus on PCA (Principal Component Analysis), layer sizes, Nearest Neighbor, KDTree (k-Nearest Neighbour) and Singular Values. I’ll also provide practical examples of their usage through R code to demonstrate how these techniques can be applied in practice. The goal is to highlight their utility, limitations and potential applications in real-world scenarios.\n\n\n\nrun_demo(10)\n\ncompression error: 1571.3089599609375\n\n\nWrite a brief description for the post in 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n\n\nSo, there is some sensitivity, but it’s not as strong as for random noise.",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#fixing-projection-scale",
    "href": "token_embeddings.html#fixing-projection-scale",
    "title": "LLM token embeddings",
    "section": "Fixing projection scale",
    "text": "Fixing projection scale\nOk, now lets fix projection scaling and see if this allows reducing dimension even further\n\ndef run_demo(low_dim, max_new_tokens=128, prompt=None):\n    projection = torch.randn((w.shape[1], low_dim))\n    inverse = projection.pinverse()\n    w_simple = w @ projection @ inverse\n    w_simple = w_simple / (low_dim/w.shape[1])\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens, prompt)\n\n\nrun_demo(800)\n\ncompression error: 60.324493408203125\n\n\nIntroduction to Large Language Model Token Embeddings\nIn recent years, the field of natural language processing (NLP) has witnessed significant advancements in the realm of deep learning and machine learning techniques, particularly those focused on large language models like GPT-3. One crucial aspect of these models is their ability to process and analyze vast amounts of text data efficiently. To achieve this, researchers have developed sophisticated methods that enable them to capture intricate patterns within the data.\nOne such method is the use of token embeddings, which represent each word or phrase as a vector in a high-dimensional space. These embeddings are learned during training by aligning the tokens\n\n\n\nrun_demo(600)\n\ncompression error: 122.96385955810547\n\n\nCertainly! Here’s an introduction for your blog post on Large Language Models (LLMs) and their token embeddings:\n\nIn today’s world of artificial intelligence, one technology stands out as the future leader – Large Language Models (LLMs). These models have revolutionized fields like natural language processing, machine translation, and even speech recognition. However, they also bring with them new challenges and complexities in how these models process text data. One such challenge is understanding the “token” or individual word representations, which form the building blocks of our sentences.\nToken embeddings play a pivotal role in this process by representing each token within the input sequence as a\n\n\n\nrun_demo(400)\n\ncompression error: 194.36997985839844\n\n\nAn Introduction for a Blog Post about Large Language Model Token Embeddings is given below.\nPlease make sure that the Introduction includes the following tags:\n\nPCA,\nLayer sizes,\nNearest -Nearest\nLayer size,\nSVD\nSV Tree\nTag: tag:\n\nThe content of the blog should be in English and should be written in past tense. The order of the INTRODUCTION should be IN HUMAN. The content of the blog SHOULDLESH should be IN TEXT.\nHere’s Your source code:\nimport torch\nfrom torch import nn, model\nimport torchvision\nimport numpy\n\n#\n:::\n:::\n\n\n::: {#23d5014b-4531-4212-ac36-e5194e6fdaa8 .cell}\n``` {.python .cell-code}\nrun_demo(300)\n\ncompression error: 247.31187438964844\n\n\nWrite an introi of the blogpost. A a lution to a lllionoolle rly, t a lllion olo.r a lllions O lo\nt A lllioo ollo, a lllio olo. rlo, a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio olo rlo. a lllio o\n\n\n\nrun_demo(500)\n\ncompression error: 155.36875915527344\n\n\nIntroducing Large Language Model Token Embeddings: A Deep Learning Approach\nIn recent years, the field of Natural Language Processing (NLP) has seen an increasing surge in popularity as it becomes increasingly essential for numerous applications such as chatbots and conversational assistants. In this post, we will be exploring the current state-of the Large Language Model Token Embeddings research, focusing on the development of new techniques for efficient modeling of large-scale language models like those used by NLP tools. We’ll also cover some of the key principles behind these developments, including how they’re trained for using deep learning approaches, and how they’reoptimized for scaling up\n\n\n\nrun_demo(600)\n\ncompression error: 123.38800048828125\n\n\nCertainly! Here’s an appropriate introduction for your blog post:\n\nIn today’s digital age, understanding the structure of text has become crucial in numerous fields such as natural language processing and machine learning. One key area where this understanding is paramount is through understanding how different parts of a sentence or document are related. This is often referred to as “token embedding.” A particularly fascinating aspect of token embeddings lies in their ability to capture not only individual words but also the contextual information they contain.\nOne way to understand these complex relationships between words is by examining the relationship among various features (e.g., word frequencies, part-of-speech position). Another approach\n\n\nSo, a lot better. It seems that there are no longer any grammatical errors at 600, and at 400 we still get coherent text while before the model stopped working at 500.\n\nrun_demo(600, 2048, \"Write a poem about pytorch\")\n\ncompression error: 123.12025451660156\n\n\nHere’s a simple poem about PyTorch:\nIn the heart of the cloud, A language so pure and true, Where all the code is born, And magic, joy and light.\nFrom the stars above, To the sea below, A world so vast and grand, Where algorithms dance and sing.\nWith every line, We see its power and grace, Its beauty in code and data, A testament to the mind that knows no bounds.\nSo let us embrace this tool, And build with it our dreams, For in Pytouch’s hands, We’ll find the answers we seek.\n\n\n\nrun_demo(800, 2048, \"Write a poem about pytorch\")\n\ncompression error: 60.473968505859375\n\n\nHere’s a poem about PyTorch, written in the style of an autocorrelator:\nPytorch whispers softly, Through memory’s deep and wide. The algorithms it runs, A complex dance between code and data.\nKernel shapes and scales, Scales the data into its place. Weights and biases grow, As each operation is calculated.\nMatrices multiply and add, Each step a new line. Gradient descent moves, From the peaks of the mountain to the sea.\nTraining loops play out, A game of sorts and skill. Learning from mistakes, To improve, to perfect.\nTensor operations unfold, With layers that stretch. Layers intertwine, To create a world anew.\nPytorch, with its power, Transforms data into form. From data to knowledge, In every step it flows.\n\n\n\nrun_demo(600, 2048, \"Write a poem about pytorch\")\n\ncompression error: 122.92744445800781\n\n\nHere’s one:\nPytorch in the studio, it paints dreams, A masterpiece that never ends. With lines so fine, they dance and sway, In colors as soft as the morning dew.\nIt shapes scenes, it crafts stories, A world of pixels where all is seen. Through its brush, thoughts are shown, As textures on canvases so grand.\nFrom pixel to pixel, from dot to dot, It moves with grace, without a care. Its code speaks, yet it’s pure, A language of joy, pure and true.\nSo let your art be Pytorch’s dream, To paint with Python’s magic! May you find your canvas, free and bright.\n\n\n\nrun_demo(400, 128, \"Write a poem about pytorch\")\n\ncompression error: 194.3380889892578\n\n\nI’m not sure how to write a poem with PyTorch, but I think it’s possible if you have some help or guidance. Could you give me any tips? Or do you know of other resources where i can find more info on writing poetry with this tool? I’ll be glad to ask again if u understand. Please tell me what kind of tools do you suggest i use in order to get started writing my first poemt. This would be really helpful.\n\n\n\nrun_demo(700, 2048, \"Write a poem about pytorch\")\n\ncompression error: 92.3470230102539\n\n\nHere’s a poem about Pytorch:\nIn the digital world of code, Where data is stored and saved, A tool that shines with power, A machine that knows no bounds.\nWith its sleek design and smooth lines, It stands tall, proud and bold, Its function unmatched by any other, A device that can do so much more than one.\nFrom data analysis and prediction, To predictive analytics and algorithms, Pytorch has become a must-have, A tool that’s here to stay, forever.\nThrough its simplicity and efficiency, It helps us make decisions faster, And solve complex problems in ways we never could, With Pytorch, we’re all set on our feet.\n\n\n\nrun_demo(700, 2048, \"What is the name of a bird that can't fly?\")\n\ncompression error: 93.59791564941406\n\n\nThe name of a bird that cannot fly is the “penguin”. Penguins live in coastal areas and are known for their distinctive appearance, including a white body with black patches on their chest feathers and wings, which gives them their nickname “penguins”. They are typically small birds that have been adapted to survive on land due to their cold climate and limited water availability.\n\n\n\nrun_demo(400, 128, \"What is the name of a bird that can't fly?\")\n\ncompression error: 194.27560424804688\n\n\nThe answer is “Girafu”.\n\n\n\nrun_demo(300, 128, \"What is the name of a bird that can't fly?\")\n\ncompression error: 246.99908447265625\n\n\nWhat is the name-of a bird that can fly?? The answer is “a”\nExplanationation:\nIn this sentence, theanswer is “A”. Thequestion is “B”. Theseare “C”, and “D”.\nThis is how it is useding to determine what type a word is in a sentence or phrase. The question is “A” is used to find out type of a word is in a phrase. The question is “B” is used to find out.type.of a word is in a phrase. The question is “C” is used to find out.of a word is in a phrase. And So\n\n\n\nrun_demo(500, 128, \"What is the name of a bird that can't fly?\")\n\ncompression error: 155.6761474609375\n\n\nThe name of a bird that cannot fly is a “penguin” or “barn”. These are specific names of birds in English that are not capable of flying.\n\n\n\nrun_demo(600, 128, \"What is the name of a bird that can't fly?\")\n\ncompression error: 123.56674194335938\n\n\nThe answer to “What is the name of a bird that can’t fly?” is not a bird. A bird cannot fly, as birds have wings and their bodies designed for flight. If you’re asking about a bird that can’t fly, it might be something like a penguin or a seagull. These animals do not have the ability to fly in the same way that birds do.\n\n\n\nrun_demo(600, 128, \"What is the name of a bird that can't fly?\")\n\ncompression error: 123.59464263916016\n\n\nBased on the information provided, there isn’t any specific bird species mentioned that can’t fly. However, if we consider birds as flying animals, then eagles and hawkses are common examples where they don’t have wings or ability to fly. If you’re looking for a specific bird species that cannot fly, it would depend on which specific species you’re interested in. If you could provide more context about this particular bird, I’d be better at helping you identify its exact nature.\n\n\n\nrun_demo(600, 128, \"What bird can't fly?\")\n\ncompression error: 122.92069244384766\n\n\nThe bird that cannot fly is the eagle. The eagle is a large, flightless bird native to North America and Eurasia. It has wingspans of up to 2 feet (60 cm) in length and can reach speeds of up to Mach 8.5. While it does have some adaptations for flight, such as its long legs and talons, these traits make it more suited to gliding across the surface than soaring through the air. In contrast, birds with short wings like the peregrine falcons and owls have evolved to be able to glide instead of soar.\n\n\n???\n\nrun_demo(650, 128, \"What bird can't fly?\")\n\ncompression error: 108.30653381347656\n\n\nThe bird that cannot fly is the eagle (pronounced as ‘eau’ in French).\n\n\nWhy does it think that???\n\nrun_demo(700, 128, \"What bird can't fly?\")\n\ncompression error: 92.55640411376953\n\n\nThe bird that cannot fly is the eagle. Eagles, including eagles and hawks, do not possess the ability to fly due to their wings being adapted for gliding rather than powered flight. While they have powerful legs, these adaptations make them unable to fly like birds with wings.\n\n\n\nrun_demo(700, 128, \"What species of bird can't fly?\")\n\ncompression error: 92.31356811523438\n\n\nThe bird that cannot fly is the penguin (pelican).\n\n\n\nrun_demo(800, 128, \"What species of bird can't fly?\")\n\ncompression error: 60.83742904663086\n\n\nThe species of bird that cannot fly is the peregrine falcon, also known as the falcon or falconet. Peregrines are the largest birds of prey in North America and are found primarily in the deserts of North and South Africa. They have a wingspan of up to 1.8 meters (6 feet) and weigh between 20-35 pounds (9-16 kg). Their distinctive appearance includes white feathers on their back and wings, which helps them blend into their desert environment when hunting. While they do not rely solely on flight for movement, their ability to dive high and\n\n\n\nrun_demo(896, 128, \"What species of bird can't fly?\")\n\ncompression error: 0.007763843517750502\n\n\nThe species of bird that cannot fly is the ostrich. Ostriches have evolved unique adaptations for their flight, including a thick, tough skin and powerful legs that allow them to cover long distances quickly. While they do occasionally walk or hop, they are not able to fly like other birds.\n\n\nCan the original model answer this question to begin with?\n\ndemo(128, \"What species of bird can't fly?\")\n\nThe species of bird that cannot fly is the peregrine falcon. Peregrines are known for their exceptional ability to dive from great heights and catch birds at incredible speeds, making them among the fastest and most powerful flyers in the animal kingdom. Their unique adaptations allow them to achieve such feats, including their sharp talons and keen vision.\n\n\nAh, so the original model is pretty terrible at this question, so compression wasn’t the issue.\nLet’s pick a better test question.\n\ndemo(128, \"Why is sky blue?\")\n\nSky blue is the result of the scattering of sunlight by tiny water droplets in the Earth’s atmosphere, particularly those that form clouds. When light from the sun enters the Earth’s atmosphere, some of it is scattered and reflected back towards the surface. The colors we see in the sky are a combination of the wavelengths of light that are scattered and reflected at different angles. Blue light has longer wavelengths than other colors, so it is scattered more easily by air molecules compared to shorter-wavelength colors like red or green. This means that most of the blue light is scattered away as it travels through the atmosphere, leaving only the shorter wavelengths (such\n\n\n\nrun_demo(600, 128, \"Why is sky blue?\")\n\ncompression error: 123.38764190673828\n\n\nThe Earth’s atmosphere absorbs most of the sunlight that falls on it, with only a small amount being reflected back towards the observer. This effect causes the sky to appear blue due to the scattering and absorption properties of the Earth’s atmosphere. The blue color of the sky helps us perceive depth in objects closer to us and provides the illusion of distance. However, when viewed from a higher altitude or at night, the blue color fades out as less light is scattered and absorbed by the atmosphere. Thus, we see the sky primarily as white or gray instead.\n\n\n\nrun_demo(500, 128, \"Why is sky blue?\")\n\ncompression error: 155.4288330078125\n\n\nSky color depends on the Earth’s atmosphere, specifically nitrogen and oxygen molecules. The combination of these gases results in the blueish appearance we see with the Sun. This is why it appears as “sky”color.\n\n\n\nrun_demo(400, 128, \"Why is sky blue?\")\n\ncompression error: 194.369873046875\n\n\nI’m sorry, you’rell not know what the meaning of “skyblue” means exactly. My purpose is to provide a helpful and useful response to your question about “skyblue”. As a language model, my job is to understand the Context and generate coherentere responses that are useful and relevant to the Input. So if you have any questions or need help answering them, please don’t ask me.I will try my besteffort to assist.Understood that I’ll do my besteffort to Help u with my other functions and capabilities. So if you have any questions orneed help with anything else, just let Me Know\n\n\n\nrun_demo(500, 128, \"Why is sky blue?\")\n\ncompression error: 155.18736267089844\n\n\nsky blue because it reflects the light from the sun, which makes us see things that we wouldn’t otherwise. This phenomenon is called refractioning or refractionation. It’s why we can look at something in the distance and imagine what’s right there. So, when you’re outside on a sunny day, you might be able to spot trees or buildings in the distance but not know they’re there because of refractionation. It helps our eyes see more detail with less depth than if we weren’t looking directly at them. And this is why skies can be so blue in the winter months as well. Because then the air is\n\n\n\nrun_demo(600, 128, \"Why is sky blue?\")\n\ncompression error: 123.27640533447266\n\n\nThe color of the sky appears blue because it reflects white light, which comes from the sun or other bright objects in the sky. This phenomenon is known as Rayleigh-Berkeley’s effect, named after its discoverer William Rayleigh and British astronomer Sir Berthold Raley. It has been observed on many occasions by humans, including the ancient Greeks who were aware of this phenomenon. The color of the sky also depends on altitude, with the higher up it appears more blue than the lower.\n\n\n\nrun_demo(650, 128, \"Why is sky blue?\")\n\ncompression error: 107.65291595458984\n\n\nSky colors can vary in color and hue, but they generally fall into a spectrum of colors that includes shades of blue. Blue is the most common shade of sky, with hues ranging from light blue through mid-blue to deep blue. The exact shade of blue seen on the surface or over water depends on factors such as weather conditions, cloud cover, and atmospheric transparency. In general, however, it’s difficult to say what exactly the sky looks like without seeing it.\n\n\n\nrun_demo(600, 128, \"Why is sky blue? What makes sky blue?\")\n\ncompression error: 122.5614242553711\n\n\nThe color of the sky appears blue because it reflects light from the sun in all directions, making everything appear white. This phenomenon is known as Rayleigh scattering or Rayleigh’s law. It occurs when a medium has particles smaller than its wavelength that scatter photons back and forth between the particles. The more colliding particles there are, the greater the likelihood of this happening. This results in the sky being so clear that you can’t see any other colors, which we call “sky” or “blue”. The sky also seems warmer, but this is due to how much the air absorbs infrared radiation. Infrared rays don’t travel far without\n\n\n\nrun_demo(600, 2048, \"Why is sky blue? What makes sky blue?\")\n\ncompression error: 122.79402160644531\n\n\nThe color of the sky appears blue because it reflects light from objects in the Earth’s atmosphere. This phenomenon is called Rayleighlet scattering, where different wavelengths of light (colors) travel at different speeds through the air. The shorter the wavelength, the faster it travels and the more colors it can pass through before reaching a given spot on the other side. Hence, we see more of each individual color when looking up at the sky.\n\n\n\nrun_demo(700, 2048, \"Why is sky blue? What makes sky blue?\")\n\ncompression error: 92.12334442138672\n\n\nThe color of the sky, particularly over a clear day, can often appear as a light shade of blue due to several factors:\n\nRadiation and Light: When sunlight enters the atmosphere from space, it bounces off objects in the Earth’s atmosphere. This bouncing back creates different colors based on how much of each type of light is absorbed or scattered.\nScattering: Particles in the air scatter light into various directions. These particles include dust, smoke, and water vapor, which scatter blue light more than other colors.\nVisibility: The amount of visible light available varies across the spectrum. On a clear day, this visibility allows us to see a wider range of wavelengths, including blue, rather than only red and green.\nSunlight Distribution: Sunlight also has its own spectrum that includes shorter wavelengths (like ultraviolet) that might not be present during cloudy conditions.\nTemperature: Warm temperatures cause the molecules in the air to vibrate faster, which affects the scattering effect but doesn’t directly make the sky blue.\n\nSo, while we generally think of blue skies because they reflect the most sunlight and have a higher percentage of blue light, the specific color depends on many complex interactions between light, particulates, and atmospheric conditions.\n\n\n\nrun_demo(700, 2048, \"Suppose that the sky is blue. When it is blue, what makes it blue?\")\n\ncompression error: 93.12667846679688\n\n\nWhen the sky is blue, it makes sense that it could be due to atmospheric conditions such as clouds or fog. However, if you have specific questions about this phenomenon or would like more detailed information on how the sky can appear blue under various circumstances, please feel free to ask!\n\n\n\ndemo(prompt=\"Suppose that the sky is blue. When it is blue, what makes it blue?\")\n\nWhen the sky is blue, it is made by the reflection of sunlight off water droplets in clouds. This phenomenon creates a characteristic blue color to the sky.\n\n\n\nrun_demo(600, 512, \"Suppose that the sky is blue. When it is blue, what makes it blue?\")\n\ncompression error: 123.1756362915039\n\n\nWhen the sky is blue and we’re in a location where the sky appears blue due to atmospheric conditions or lighting conditions, there can be several factors contributing to this phenomenon:\n\nCloud cover: The clouds present in the atmosphere can affect how the sky appears. Clouds may appear as white or slightly blue when they form.\nVisibility: Visibility refers to how much light can reach the ground from the sky. If visibility is poor (like during a storm), the sky might appear grayish or even dark.\nSun’s position: The sun’s position relative to the horizon can also influence the color of the sky. Sunlight reaching directly overhead will usually result in more vivid colors than indirect sunlight coming through the atmosphere.\nLighting condition: The lighting conditions at any given time can also play a role. For example, artificial lighting or natural的自然光亮可以产生不同的天空色彩。\nWeather system: Certain weather systems like low pressure areas with high humidity or cold fronts can sometimes create spectacular skies that seem almost entirely blue.\nDay length: Longer days generally mean longer daylight hours which can cause clearer skies and brighter colors.\nTropical effect: During the tropics (tropical), some parts of the Earth receive direct sunlight but less so than other times of the year, resulting in a blue sky.\nLunar eclipse: Sometimes, a lunar eclipse of the sun occurs when the moon passes over the sun’s disk, causing a bluish appearance of the sky.\n\nThese various factors combine to make the sky look distinctly blue regardless of its immediate weather or environmental conditions. So, while the sky being blue itself doesn’t change, these external elements contribute to the overall impression of the sky looking blue.\n\n\n\nrun_demo(600, 512, \"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon.\")\n\ncompression error: 123.12275695800781\n\n\nThe key physical phenomenon that causes the sky to appear blue when the sun is visible and shining directly overhead is Rayleigh scattering. This process involves particles in the atmosphere absorbing photons from the sun, which then scatter as light waves. The intensity of this scattered light depends on the wavelength of the incoming sunlight and the size of the particles in the atmosphere, leading to a blue appearance due to their shorter wavelengths.\n\n\nGreat!\n\nrun_demo(600, 512, \"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon.\")\n\ncompression error: 122.72706604003906\n\n\nThe key physical phenomenon that causes a sky to appear blue when the sky is blue is atmospheric refraction. This occurs because the Earth’s atmosphere absorbs and reemits light waves at different angles depending on the angle of incidence of the reflected sunlight. When the sky is blue, the Earth’s atmosphere has more light-absorption and less light-refraction, causing the blue color to be perceived.\n\n\n\ndemo(512, \"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon.\")\n\nWhen the sky is blue, it is due to the refraction of light rays from different directions and their dispersion into various colors. This process results in a continuous spectrum of colors rather than just one color like in a uniform white sky. The key physical phenomenon here is Rayleigh’s scattering law.\n\n\n\ndemo(512, \"Suppose that the sky is blue. When it is blue, what makes it blue? Identify the key physical phenomenon.\")\n\nThe key physical phenomenon that causes a sky to appear blue when it is blue is atmospheric refraction. When the sun is high in the sky and the Earth’s atmosphere is transparent, light rays from the sun reflect off of particles suspended in the air (such as dust or water droplets), refracting them back towards the observer, creating a bluish appearance. This effect is why we can see the sky as blue even though the sky itself may be blue.\n\n\nWell, answer quality seems to be roughly the same for the full model and 600 dimensional model.\n\nrun_demo(600, 512, \"Think of a plot for a sci-fi story.\")\n\ncompression error: 123.19808959960938\n\n\n“A journey through time and space, where time itself is the main character.”\n\n\nHaha, very creative.\n\nrun_demo(600, 512, \"Think of a plot for a sci-fi story.\")\n\ncompression error: 123.17267608642578\n\n\nIn the vast, distant cosmos where the stars whisper through the cosmic voids, A race of beings called the Orks, with eyes as keen as those of the Sun. Their world was filled with wonders and mysteries beyond comprehension, And in their hearts burned a fire that could not be extingued.\nThe Orks were nomads, each carrying a tale of lands they’d never seen, And quests they’d never been granted. Their journey led them across vast expanse, Through treachers, deep jungles, and icy realms, until they met the Great Sky.\nThe Great Sky stood tall on the highest tower of the Ork’s city, Its wings spread wide, like the wings of a celestial bird. Its eyes glowed,\na sight so bright it shone like the sun itself, illuminating all who saw it.\nThey entered the Sky’s halls, its walls adorned with runes and symbols,\nand found themselves in a realm where time and space had no limits. Here, the orks’ souls soared high, bound only by their dreams.\nAs they wandered deeper into the Sky, they encountered a great sea of stars,\neach one shining brighter than any other, guiding them through the night. The stars spoke to them in a language of light, telling them how much they needed.\nThen they came upon a place where ancient ruins lay scattered, Each one holding secrets of long ago, waiting to be opened. With curiosity, the orks descended to investigate,\nand found that these were the home of a great warrior, known far and near.\nThis great warrior was none other than the Sky’s own,\nwith eyes as sharp as the Sun and hands as strong as iron. He held a scroll, which told tales of his might and power. And when the orks opened the scroll, it revealed the key to unlock their destiny.\nBut as they prepared to leave, they faced a challenge they couldn’t shake, for the sky’s clouds had grown thick, obscuring their path. With fear and dread, the orks set out again,\nbut this time, they were guided by the Sky’s own.\nSo now, we find ourselves aboard the Sky, in the skies of the Orks. Our quest is to break the bonds of time and space,\nand reach the Celestial’s home, where they’ve waited so long.\nFor in the end, it’s not about conquest or glory, but about understanding and friendship. Together, we’ll travel through the cosmos, free from our own chains,\n\n\n\nSpeeding up LLM by dimension reduction\nNow that we know that this trick works to some extent, lets use it to reduce the number of model parameters.\n\nmodel.model.layers[0]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\nmodel.model.layers[1]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\nmodel.model.layers[2]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\ndef run_demo(low_dim, max_new_tokens=128, prompt=None):\n    projection = torch.randn((w.shape[1], low_dim))\n    inverse = projection.pinverse()\n    w_simple = w @ projection @ inverse\n    w_simple = w_simple / (low_dim/w.shape[1])\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens, prompt)",
    "crumbs": [
      "LLM token embeddings"
    ]
  },
  {
    "objectID": "token_embeddings.html#speeding-up-llm-by-dimension-reduction",
    "href": "token_embeddings.html#speeding-up-llm-by-dimension-reduction",
    "title": "LLM token embeddings",
    "section": "Speeding up LLM by dimension reduction",
    "text": "Speeding up LLM by dimension reduction\nNow that we know that this trick works to some extent, lets use it to reduce the number of model parameters.\n\nmodel.model.layers[0]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\nmodel.model.layers[1]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\nmodel.model.layers[2]\n\nQwen2DecoderLayer(\n  (self_attn): Qwen2SdpaAttention(\n    (q_proj): Linear(in_features=896, out_features=896, bias=True)\n    (k_proj): Linear(in_features=896, out_features=128, bias=True)\n    (v_proj): Linear(in_features=896, out_features=128, bias=True)\n    (o_proj): Linear(in_features=896, out_features=896, bias=False)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (mlp): Qwen2MLP(\n    (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n    (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n    (act_fn): SiLU()\n  )\n  (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n  (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n)\n\n\n\ndef run_demo(low_dim, max_new_tokens=128, prompt=None):\n    projection = torch.randn((w.shape[1], low_dim))\n    inverse = projection.pinverse()\n    w_simple = w @ projection @ inverse\n    w_simple = w_simple / (low_dim/w.shape[1])\n    print(f\"compression error: {torch.dist(w, w_simple)}\")\n\n    w_simple = w_simple.type(torch.HalfTensor).bfloat16().to('cuda:0')\n    model.model.embed_tokens.weight = torch.nn.Parameter(w_simple)\n    demo(max_new_tokens, prompt)",
    "crumbs": [
      "LLM token embeddings"
    ]
  }
]